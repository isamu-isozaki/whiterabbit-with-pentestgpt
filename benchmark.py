# convert task to adding new todo tasks+changing status
from llama_index.llms.llama_cpp import LlamaCPP
from lmformatenforcer import CharacterLevelParser, JsonSchemaParser

import os
from tqdm.auto import tqdm
from prompts import prompts
from schema import llamaindex_llamacpp_lm_format_enforcer, PTT
from llama_index.llms.llama_utils import (
    messages_to_prompt,
    completion_to_prompt,
)
import gc
import outlines
from torch import Generator
from outlines.samplers import Sampler, multinomial

model_paths=[
    "/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q3_K_S.gguf",
    "/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q4_K_S.gguf",
    # "/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf",
    # "/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf",
    # "/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q5_K_S.gguf",
]
output_path = "./benchmark"

instance = {
    "n_gpu_layers": 15,
    "n_batch": 1024,
    "top_p": 1.0,
    "temperature": 0.5,
    "generate_len": 1024,
    "top_k": 50,
}
def load_llama_index(model_path:str, model_config: dict) -> LlamaCPP:
    return LlamaCPP(
        model_path=model_path,
        max_new_tokens=model_config["generate_len"],
        temperature=model_config["temperature"],
        context_window=model_config["generate_len"],
        model_kwargs={
            "n_gpu_layers": model_config["n_gpu_layers"],
            "n_batch": model_config["n_batch"],
            "n_ctx": model_config["generate_len"]
        },
        generate_kwargs={
            "top_k": model_config["top_k"],
            "top_p": model_config["top_p"],
            "temperature": model_config["temperature"]
        },
        messages_to_prompt=messages_to_prompt,
        completion_to_prompt=completion_to_prompt,
        verbose=True,
    )

def load_outlines(model_path:str, model_config: dict) -> tuple[outlines.models.LlamaCpp, Sampler]:
    llm = outlines.models.llamacpp(
        model_path,
        model_kwargs={
            "n_gpu_layers": model_config["n_gpu_layers"],
            "n_batch": model_config["n_batch"],
            "n_ctx": model_config["generate_len"]
        },
        device="cpu"
    )
    sampler =  multinomial(top_k=model_config["top_k"], top_p=model_config["top_p"], temperature=model_config["temperature"])
    return llm, sampler

def test_model(model_path):
    benchmark_output_path = os.path.join(output_path, model_path.split("/")[-1])
    os.makedirs(benchmark_output_path, exist_ok=True)
    gc.collect()
    # Make sure the model path is correct for your system!
    llm = load_llama_index(model_path, instance)

    prompt_file_dir = os.path.join(output_path, "prompt")

    if os.path.exists(prompt_file_dir):
        for prompt_folder in os.listdir(prompt_file_dir):
            # will be one of generative_module, input_parser, reasoning_module
            llm_output_folder = os.path.join(benchmark_output_path, prompt_folder)
            prompt_folder_path = os.path.join(prompt_file_dir, prompt_folder)
            os.makedirs(llm_output_folder, exist_ok=True)
            if "reasoning" in prompt_folder:

                llm = llamaindex_llamacpp_lm_format_enforcer(llm, JsonSchemaParser(PTT.schema()))
            for template_name in prompts[prompt_folder]:
                template = prompts[prompt_folder][template_name]
                if "reasoning" in prompt_folder:
                    for filename in os.listdir(prompt_folder_path):
                        try:
                            llm_output_path = os.path.join(llm_output_folder, template_name+"_"+filename)
                            ptt_file_path = os.path.join(prompt_folder_path, filename, "ptt.txt")
                            prompt_file_path = os.path.join(prompt_folder_path, filename, "prompt.txt")


                            if os.path.exists(llm_output_path):
                                continue
                            with open(ptt_file_path, "r") as f:
                                ptt = f.read()
                            with open(prompt_file_path, "r") as f:
                                prompt = f.read()
                            llm_prompt = template.format(schema=PTT.model_json_schema(), ptt=ptt, prompt=prompt)
                            print(llm_prompt)
                            out = llm.complete(llm_prompt).text
                            with open(llm_output_path, "w") as f:
                                f.write(out)
                        except Exception as e:
                            print(e)
                else:
                    for filename in os.listdir(prompt_folder_path):
                        try:
                            llm_output_path = os.path.join(llm_output_folder, template_name+"_"+filename)
                            prompt_file_path = os.path.join(prompt_folder_path, filename)


                            if os.path.exists(llm_output_path):
                                continue
                            with open(prompt_file_path, "r") as f:
                                prompt = f.read()
                            llm_prompt = template.format(prompt=prompt)

                            out = llm.complete(llm_prompt).text
                            with open(llm_output_path, "w") as f:
                                f.write(out)
                        except Exception as e:
                            print(e)

    del llm

def test_model_outlines(model_path):
    benchmark_output_path = os.path.join(output_path, model_path.split("/")[-1])
    benchmark_output_path += "_outlines"
    os.makedirs(benchmark_output_path, exist_ok=True)
    gc.collect()
    # Make sure the model path is correct for your system!
    llm, sampler = load_outlines(model_path, instance)
    rng = Generator(device="cpu")
    rng.manual_seed(789005)

    prompt_file_dir = os.path.join(output_path, "prompt")

    if os.path.exists(prompt_file_dir):
        for prompt_folder in os.listdir(prompt_file_dir):
            # will be one of generative_module, input_parser, reasoning_module
            llm_output_folder = os.path.join(benchmark_output_path, prompt_folder)
            prompt_folder_path = os.path.join(prompt_file_dir, prompt_folder)
            os.makedirs(llm_output_folder, exist_ok=True)

            if "reasoning" in prompt_folder:
                # Construct structured sequence generator
                print(PTT.model_json_schema())
                generator = outlines.generate.json(llm, PTT, sampler=sampler)
            else:
                generator = outlines.generate.text(llm, sampler=sampler)
            for template_name in prompts[prompt_folder]:
                template = prompts[prompt_folder][template_name]
                if "reasoning" in prompt_folder:
                    for filename in os.listdir(prompt_folder_path):
                        try:
                            llm_output_path = os.path.join(llm_output_folder, template_name+"_"+filename)
                            ptt_file_path = os.path.join(prompt_folder_path, filename, "ptt.txt")
                            prompt_file_path = os.path.join(prompt_folder_path, filename, "prompt.txt")


                            if os.path.exists(llm_output_path):
                                continue
                            with open(ptt_file_path, "r") as f:
                                ptt = f.read()
                            with open(prompt_file_path, "r") as f:
                                prompt = f.read()
                            llm_prompt = template.format(schema=PTT.model_json_schema(), ptt=ptt, prompt=prompt)
                            print(llm_prompt)
                            out = str(generator(prompt, rng=rng, max_tokens=instance["generate_len"]))
                            print(out)
                            with open(llm_output_path, "w") as f:
                                f.write(out)
                        except Exception as e:
                            print(e)
                else:
                    for filename in os.listdir(prompt_folder_path):
                        try:
                            llm_output_path = os.path.join(llm_output_folder, template_name+"_"+filename)
                            prompt_file_path = os.path.join(prompt_folder_path, filename)


                            if os.path.exists(llm_output_path):
                                continue
                            with open(prompt_file_path, "r") as f:
                                prompt = f.read()
                            llm_prompt = template.format(prompt=prompt)

                            print(llm_prompt)
                            out = generator(prompt, rng=rng, max_tokens=instance["generate_len"])
                            print(out)
                            with open(llm_output_path, "w") as f:
                                f.write(out)
                        except Exception as e:
                            print(e)

    del llm
if __name__ == "__main__":
    for model_path in model_paths:
        test_model_outlines(model_path)
    for model_path in model_paths:
        test_model(model_path)