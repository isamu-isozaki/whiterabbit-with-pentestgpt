from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.llms import LlamaCpp
import os
from tqdm.auto import tqdm
from prompts import prompts

model_paths=[
    "/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf",
    "/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf",
    "/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q5_K_S.gguf"
]
output_path = "./benchmark"

n_gpu_layers = 20  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.
n_batch = 2048-512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
context_length =2048-512
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])


def test_model(model_path):
    benchmark_output_path = os.path.join(output_path, model_path.split("/")[-1])
    os.makedirs(benchmark_output_path, exist_ok=True)
    # Make sure the model path is correct for your system!
    llm = LlamaCpp(
        model_path=model_path,
        max_tokens=context_length,
        n_ctx=context_length,
        n_batch=n_batch,
        top_p=1,
        callback_manager=callback_manager,
        verbose=True, # Verbose is required to pass to the callback manager
        n_gpu_layers=n_gpu_layers
    )
    prompt_file_dir = os.path.join(output_path, "prompt")
    
    if os.path.exists(prompt_file_dir):
        for prompt_folder in os.listdir(prompt_file_dir):
            # will be one of generative_module, input_parser, reasoning_module
            llm_output_folder = os.path.join(benchmark_output_path, prompt_folder)
            prompt_folder_path = os.path.join(prompt_file_dir, prompt_folder)
            os.makedirs(llm_output_folder, exist_ok=True)

            for template_name in prompts[prompt_folder]:
                template = prompts[prompt_folder][template_name]
                llm_chain = LLMChain(prompt=template, llm=llm)
                if "reasoning" in prompt_folder:
                    for filename in os.listdir(prompt_folder_path):
                        llm_output_path = os.path.join(llm_output_folder, template_name+"_"+filename)
                        ptt_file_path = os.path.join(prompt_folder_path, filename, "ptt.txt")
                        prompt_file_path = os.path.join(prompt_folder_path, filename, "prompt.txt")


                        if os.path.exists(llm_output_path):
                            continue
                        with open(ptt_file_path, "r") as f:
                            ptt = f.read()
                        with open(prompt_file_path, "r") as f:
                            prompt = f.read()
                
                        out = llm_chain.run(ptt=ptt, prompt=prompt)
                        with open(llm_output_path, "w") as f:
                            f.write(out)
                else:
                    for filename in os.listdir(prompt_folder_path):
                        llm_output_path = os.path.join(llm_output_folder, template_name+"_"+filename)
                        prompt_file_path = os.path.join(prompt_folder_path, filename)


                        if os.path.exists(llm_output_path):
                            continue
                        with open(prompt_file_path, "r") as f:
                            prompt = f.read()
                
                        out = llm_chain.run(prompt)
                        with open(llm_output_path, "w") as f:
                            f.write(out)
for model_path in model_paths:
    test_model(model_path)