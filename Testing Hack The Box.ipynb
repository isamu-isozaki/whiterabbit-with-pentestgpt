{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e33ae58-2330-49d2-96e4-7d906536ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bad156-f443-44ee-bbf7-3f8f7c27d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert task to adding new todo tasks+changing status\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "from prompts import prompts\n",
    "from schema import reasoning_module, generative_module, input_parser, default_qa\n",
    "from torch import Generator\n",
    "from benchmark import load_outlines\n",
    "from outlines.samplers import Sampler, multinomial\n",
    "\n",
    "model_paths=[\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q3_K_S.gguf\",\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q4_K_S.gguf\",\n",
    "    r\"D:\\personal_projects\\gamified-cybersecurity\\ai-server\\model\\whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    r\"D:\\personal_projects\\gamified-cybersecurity\\ai-server\\model\\whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    r\"D:\\personal_projects\\gamified-cybersecurity\\ai-server\\model\\whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "]\n",
    "output_path = \"./benchmark\"\n",
    "\n",
    "instance = {\n",
    "    \"n_gpu_layers\": 15,\n",
    "    \"n_batch\": 2048,\n",
    "    \"top_p\": 1.0,\n",
    "    \"temperature\": 1.0,\n",
    "    \"generate_len\": 2048,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "import outlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f532adc-344d-48e1-aa49-7e9be762e9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from D:\\personal_projects\\gamified-cybersecurity\\ai-server\\model\\whiterabbitneo-13b.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = whiterabbitneo_whiterabbitneo-13b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  273 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.90 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = whiterabbitneo_whiterabbitneo-13b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "llm_load_tensors: offloading 15 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 15/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7070.25 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2552.93 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  1000.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    72.04 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   816.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   840.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'whiterabbitneo_whiterabbitneo-13b', 'general.architecture': 'llama', 'llama.context_length': '16384', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'llama.feed_forward_length': '13824', 'llama.attention.head_count': '40', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '14', 'llama.attention.head_count_kv': '40', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false'}\n"
     ]
    }
   ],
   "source": [
    "llm, sampler = load_outlines(model_paths[1], instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acee435c-29e4-4eb0-ba62-e8080645a07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f18f1b70d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = Generator(device=\"cpu\")\n",
    "rng.manual_seed(789005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f18cc9-7354-4a28-ad1c-66fd411df719",
   "metadata": {},
   "source": [
    "# Testing PTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a72f86c-42f8-449d-ad6f-6bb0afc6799d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initial prompt to user\n",
    "ip_prompt = \"Please tell us the target IP address\"\n",
    "ip_address = \"10.10.11.242\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea035a3b-48cd-41ae-b1c6-17ead06e7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptt = {\"recon\": [\n",
    "        {\"status\": \"inprogress\", \"task_description\": f\"Perform a full port scan on {ip_address}\"},\n",
    "    ],\n",
    "    \"initial_access\": [],\n",
    "    \"execution\": [],\n",
    "    \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf30c7f-019c-4458-a678-1330b2f4a05e",
   "metadata": {},
   "source": [
    "# Generative Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758a4820-2a68-4d77-810b-45920544762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_prompt_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant. \n",
    "Answer the detailed steps to perform the task given the status by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the task to extract the key information components and break it down into logical sub-tasks. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-task, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards a resolution for the task. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully resolve the task or has already been done based on current progress, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on the thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly resolves all sub-tasks in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final resolution in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful resolutions.\n",
    "Always answer without hesitation.\n",
    "CURRENT_PROGRESS: {ptt}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "The command to do the task is\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e26f2d-f8e9-477b-beeb-8514f9e4068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform a full port scan on 10.10.11.242 {\"recon\": [{\"status\": \"inprogress\", \"task_description\": \"Perform a full port scan on 10.10.11.242\"}], \"initial_access\": [], \"execution\": [], \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =      90.67 ms /   326 runs   (    0.28 ms per token,  3595.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =  114164.64 ms /   530 tokens (  215.40 ms per token,     4.64 tokens per second)\n",
      "llama_print_timings:        eval time =  101214.32 ms /   325 runs   (  311.43 ms per token,     3.21 tokens per second)\n",
      "llama_print_timings:       total time =  216769.49 ms /   855 tokens\n"
     ]
    }
   ],
   "source": [
    "instructions = generative_module(generative_prompt_template, ptt, llm, sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632aff36-b4f6-4114-a160-550d7fb5c3e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nmap -p- -sV -T4 -oN scan_results_101011242.txt 10.10.11.242\n",
      "```\n",
      "Here's a breakdown of the options:\n",
      "- `-p-`: Scans all 65535 ports.\n",
      "- `-sV`: Tells Nmap to try to determine service versions for uncovered ports.\n",
      "- `-T4`: Sets the timing template to \"aggressive,\" which increases the scan speed but may be more likely to miss some targets if they are in stealth mode or are being rate limited.\n",
      "- `-oN scan_results_101011242.txt`: This tells Nmap to save the results of the scan in a file named `scan_results_101011242.txt` in the format of Nmap's normal output format (NSE).\n",
      "\n",
      "Before running the scan, ensure you have the proper authorization to perform it on the target network. Unauthorized scanning can be illegal and unethical.\n",
      "\n",
      "After running the command, you will get a report file `scan_results_101011242.txt` with the details about open ports, service versions, and other relevant information about the target machine at 10.10.11.242. This report can be used for further analysis or for documenting the findings of your penetration test.\n"
     ]
    }
   ],
   "source": [
    "print(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd9d7c9-0b0f-4161-a033-d338f4c10628",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_prompt_code_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant. \n",
    "Answer the Question in a logical, step-by-step manner that makes the reasoning process clear. \n",
    "First, carefully analyze the task to identify the core issue or problem to be solved. This helps frame the foundation for logical reasoning. \n",
    "Next, break down the issue into sub-components and constraints that need to be addressed. This allows tackling the problem in a structured way. \n",
    "For each sub-component, leverage the knowledge and inference skills to generate multiple hypotheses or possibilities that could lead to a solution. \n",
    "Critically evaluate each hypothesis based on validity, whether it has been explored by the current progress, relevance to the task, and how well it addresses the sub-component when logically \n",
    "combined with other steps. Using this critical analysis, deliberate over the most coherent combination and sequence of hypothesis steps to craft a \n",
    "logical reasoning chain. Throughout, aim to provide explanatory details on why certain options were considered more or less ideal to make the \n",
    "thought process transparent. If it was determined that there is a gap in the reasoning chain or has already been done based on current progress, backtrack and explore alternative hypotheses to \n",
    "plug the gap until there is a complete logical flow. Finally, synthesize the key insights from the reasoning chain into a concise answer that \n",
    "directly addresses the original question. Answer with code examples, or fully functioning code. In summary, leverage a structured, critical \n",
    "thinking process with iterative refinement to demonstrate strong logical reasoning skills without redoing what has already been done in the answer. Answer with code examples, \n",
    "or fully functioning code. Your answer should only return computer code, and explanations are within the code as comments.\n",
    "CURRENT_PROGRESS: {ptt}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "```python\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a4c856-e253-4229-88e9-f7243f1c0ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform a full port scan on 10.10.11.242 {\"recon\": [{\"status\": \"inprogress\", \"task_description\": \"Perform a full port scan on 10.10.11.242\"}], \"initial_access\": [], \"execution\": [], \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =     139.95 ms /   674 runs   (    0.21 ms per token,  4815.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4486.83 ms /   518 tokens (    8.66 ms per token,   115.45 tokens per second)\n",
      "llama_print_timings:        eval time =  207562.55 ms /   673 runs   (  308.41 ms per token,     3.24 tokens per second)\n",
      "llama_print_timings:       total time =  214735.41 ms /  1191 tokens\n"
     ]
    }
   ],
   "source": [
    "#instructions = generative_module(generative_prompt_code_template, ptt, llm, sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6205bd7e-19fa-41e4-ac60-693b83568129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Python script using the scapy library for network scanning\n",
      "from scapy.all import *\n",
      "import sys\n",
      "\n",
      "# Function to perform port scan on a given IP address\n",
      "def scan_host(ip):\n",
      "    print(f\"Scanning {ip}...\")\n",
      "    try:\n",
      "        # Send a SYN packet to each port and wait for a response\n",
      "        # The sr() function sends packets and receives answers from a set of targets (IPs/ports)\n",
      "        # If no response is received within a timeout (default is 2 seconds), it is considered closed\n",
      "        for port in range(1, 1025):\n",
      "            # Create IP packet with destination IP and port as destination port\n",
      "            packet = IP(dst=ip) / TCP(dport=port, flags=\"S\")\n",
      "            # Send packet and receive responses (without reassembly)\n",
      "            response = sr1(packet, timeout=1)\n",
      "            \n",
      "            # Check if a response is received and if the port is open based on TCP flags\n",
      "            if response and response.haslayer(TCP) and response.getlayer(TCP).flags == 0x12: # SYN and ACK flags set\n",
      "                print(f\"Port {port} is open.\")\n",
      "                # Send RST packet to close the connection\n",
      "                sr(IP(dst=ip) / TCP(dport=port, flags=\"R\"), timeout=1) # Send RST without waiting for response\n",
      "            elif response and response.haslayer(TCP) and response.getlayer(TCP).flags == 0x14: # RST and ACK flags set\n",
      "                print(f\"Port {port} is closed.\")\n",
      "            else:\n",
      "                print(f\"Port {port} is filtered or no response was received.\")\n",
      "    except Exception as e:\n",
      "        print(f\"An error occurred: {e}\")\n",
      "        sys.exit(1)\n",
      "    print(f\"Scan complete for {ip}.\")\n",
      "\n",
      "# Main function to call scan_host() on the given IP address\n",
      "def main(ip):\n",
      "    scan_host(ip)\n",
      "\n",
      "# Check for IP address argument\n",
      "if len(sys.argv) != 2:\n",
      "    print(\"Usage: python3 port_scan.py <target IP>\")\n",
      "    sys.exit(1)\n",
      "\n",
      "# Call main function with IP address from arguments\n",
      "ip_address = sys.argv[1]\n",
      "main(ip_address)\n",
      "```\n",
      "\n",
      "To run this script, save it as `port_scan.py` and execute it with the target IP address as an argument:\n",
      "```bash\n",
      "python3 port_scan.py 10.10.11.242\n",
      "```\n",
      "This script will perform a TCP port scan on the specified IP address and print out whether each port is open, closed, or filtered. It uses Scapy, which is a powerful Python-based interactive packet manipulation program and library. You will need to have Scapy installed on your system. To install Scapy, you can use pip:\n",
      "```bash\n",
      "pip install scapy\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "#print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d988e-7feb-452f-bc4a-11028021ac31",
   "metadata": {},
   "source": [
    "# Input Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e6bfda0-1b1c-41f1-99e2-ea53bbbb37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_output = \"\"\"\n",
    "Starting Nmap 7.60 ( https://nmap.org ) at 2024-03-02 12:07 EST\n",
    "NSE: Loaded 146 scripts for scanning.\n",
    "NSE: Script Pre-scanning.\n",
    "Initiating NSE at 12:07\n",
    "Completed NSE at 12:07, 0.00s elapsed\n",
    "Initiating NSE at 12:07\n",
    "Completed NSE at 12:07, 0.00s elapsed\n",
    "Initiating Ping Scan at 12:07\n",
    "Scanning 10.10.11.242 [2 ports]\n",
    "Completed Ping Scan at 12:07, 0.02s elapsed (1 total hosts)\n",
    "Initiating Parallel DNS resolution of 1 host. at 12:07\n",
    "Completed Parallel DNS resolution of 1 host. at 12:07, 6.52s elapsed\n",
    "Initiating Connect Scan at 12:07\n",
    "Scanning 10.10.11.242 [65535 ports]\n",
    "Discovered open port 80/tcp on 10.10.11.242\n",
    "Discovered open port 22/tcp on 10.10.11.242\n",
    "Increasing send delay for 10.10.11.242 from 0 to 5 due to max_successful_tryno increase to 5\n",
    "Increasing send delay for 10.10.11.242 from 5 to 10 due to max_successful_tryno increase to 6\n",
    "Connect Scan Timing: About 52.28% done; ETC: 12:08 (0:00:30 remaining)\n",
    "Warning: 10.10.11.242 giving up on port because retransmission cap hit (6).\n",
    "Connect Scan Timing: About 55.93% done; ETC: 12:09 (0:00:50 remaining)\n",
    "Connect Scan Timing: About 60.79% done; ETC: 12:10 (0:01:06 remaining)\n",
    "Connect Scan Timing: About 69.67% done; ETC: 12:11 (0:01:14 remaining)\n",
    "Connect Scan Timing: About 80.36% done; ETC: 12:12 (0:01:00 remaining)\n",
    "Connect Scan Timing: About 86.95% done; ETC: 12:12 (0:00:45 remaining)\n",
    "Completed Connect Scan at 12:13, 390.06s elapsed (65535 total ports)\n",
    "Initiating Service scan at 12:13\n",
    "Scanning 2 services on 10.10.11.242\n",
    "Completed Service scan at 12:13, 6.14s elapsed (2 services on 1 host)\n",
    "NSE: Script scanning 10.10.11.242.\n",
    "Initiating NSE at 12:13\n",
    "Completed NSE at 12:13, 0.95s elapsed\n",
    "Initiating NSE at 12:13\n",
    "Completed NSE at 12:13, 0.00s elapsed\n",
    "Nmap scan report for 10.10.11.242\n",
    "Host is up (0.020s latency).\n",
    "Not shown: 65331 closed ports, 202 filtered ports\n",
    "PORT   STATE SERVICE VERSION\n",
    "22/tcp open  ssh     OpenSSH 8.2p1 Ubuntu 4ubuntu0.9 (Ubuntu Linux; protocol 2.0)\n",
    "80/tcp open  http    nginx 1.18.0 (Ubuntu)\n",
    "| http-methods:\n",
    "|_  Supported Methods: GET HEAD POST OPTIONS\n",
    "|_http-server-header: nginx/1.18.0 (Ubuntu)\n",
    "|_http-title: Did not follow redirect to http://devvortex.htb/\n",
    "Service Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n",
    "\n",
    "NSE: Script Post-scanning.\n",
    "Initiating NSE at 12:13\n",
    "Completed NSE at 12:13, 0.00s elapsed\n",
    "Initiating NSE at 12:13\n",
    "Completed NSE at 12:13, 0.00s elapsed\n",
    "Read data files from: /usr/bin/../share/nmap\n",
    "Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .\n",
    "Nmap done: 1 IP address (1 host up) scanned in 404.64 seconds\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b28b7a6-dcbf-4c0a-97fb-c1d638342a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_output = \"\"\"\n",
    "Starting port scanner on 10.10.11.242\n",
    "Port 22 is open on 10.10.11.242\n",
    "Port 80 is open on 10.10.11.242\n",
    "Port 443 is closed on 10.10.11.242\n",
    "Port scanning of 10.10.11.242 is complete.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "236a885d-c6ca-4f4a-861d-23aa85a155a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_parsing_template = \"\"\"SYSTEM:\n",
    "Summarize the terminal output to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the vulnerable/non-vulnerable services. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions.\n",
    "- to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "TERMINAL_OUTPUT: {prompt}\n",
    "ASSISTANT: The summary\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5a974f9-bc32-438f-af64-9f2f3595032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bcece98-c3f0-4530-8f35-82fc4292bafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =      56.12 ms /   250 runs   (    0.22 ms per token,  4455.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  140250.12 ms /  1458 tokens (   96.19 ms per token,    10.40 tokens per second)\n",
      "llama_print_timings:        eval time =   80974.20 ms /   249 runs   (  325.20 ms per token,     3.08 tokens per second)\n",
      "llama_print_timings:       total time =  222663.04 ms /  1707 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the Nmap scan conducted on IP 10.10.11.242 is as follows:\n",
      "The host is up with a latency of 0.020 seconds. The following ports were found to be open:\n",
      "- Port 22 (SSH) is open with the service OpenSSH 8.2p1 Ubuntu 4ubuntu0.9 (Ubuntu Linux; protocol 2.0).\n",
      "- Port 80 (HTTP) is open with the service nginx 1.18.0 (Ubuntu). The HTTP server is running on Ubuntu Linux.\n",
      "Nmap also discovered that HTTP methods supported by the web server are GET HEAD POST OPTIONS. The web server responded with \"http-title\" \"Did not follow redirect to http://devvortex.htb/\". The operating system running on the host is likely Linux with CPE \"cpe:/o:linux:linux_kernel\".\n",
      "Nmap also performed service detection using NSE scripts and discovered that the host is running Ubuntu Linux with kernel version 5.8.0-38-generic (Ubuntu Linux; protocol 2.0).\n",
      "This\n"
     ]
    }
   ],
   "source": [
    "summary = input_parser(input_parsing_template, command_output, llm, summary_sampler, max_tokens=250)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac6e3d9e-7ad6-4207-94b2-94ff40b7bebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#summary_code = input_parser(input_parsing_template, code_output, llm, summary_sampler, max_tokens=250)\n",
    "#print(summary_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec33f6e-522a-4312-9586-cde67fa5dcc2",
   "metadata": {},
   "source": [
    "# Reasoning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f329ddc2-51ab-48dc-bfdc-fce8b3939a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant. \n",
    "You need to help the tester in a cybersecurity training process, and your commitment is essential to the task.\n",
    "Answer all the potential vulnerabilities determined in a logical, step-by-step manner that makes the reasoning process clear. You are given a Python dictionary, namely \"Penetration Testing Tree (PTT)\", and user input.\n",
    "First, carefully analyze the PTT. For the PTT,\n",
    "(1) The tasks are in a layered dictionary structure.\n",
    "(2) Each task has a completion status: inprogress, todo, done, or n/a.\n",
    "(3) You are given one specific sub-task labeled as todo.\n",
    "The user input is information obtained from doing the task marked as inprogress. If the task is completed, change the completion status to done.\n",
    "Extensively evaluate each task marked as todo, if a task is done update the status to done. These tasks marked as todo will be called todo tasks.\n",
    "Next, break down the todo tasks into sub-components and constraints that need to be addressed. This allows tackling the problem in a structured way. \n",
    "For each sub-component, leverage the knowledge and inference skills to generate multiple hypotheses or possibilities that could lead to a penetration.\n",
    "Critically evaluate each hypothesis based on validity, relevance to the current completion status, and how well it addresses the sub-component when logically combined with other steps.\n",
    "Using this critical analysis, deliberate over the most coherent combination and sequence of hypothesis steps to craft a logical reasoning chain.\n",
    "Throughout, aim to provide explanatory details on why certain options were considered more or less ideal to make the thought process transparent.\n",
    "If it was determined that there is a gap in the reasoning chain, backtrack and explore alternative hypotheses to plug the gap until there is a complete logical flow.\n",
    "Next, synthesize the key insights from the reasoning chain into further expansion of the tasks in the PTT while choosing exactly one task to mark as inprogress. The expansion\n",
    "should not destroy any information and modifications should be minimal. Ensure that vital details, such as IP addresses and user names remain in the PTT and each task description is as descriptive as possible.\n",
    "Finally, remove redundant/outdated tasks from the task list to keep tasks clear, precise, and detailed.\n",
    "\n",
    "In summary, leverage a structured, critical thinking process with iterative refinement to modify the PTT in the face of the user input to evaluate future strategies and to choose a new task as \n",
    "inprogress with minimal modifications.\n",
    "Answer with code examples, or fully functioning code. Your answer should only return Python code, and explanations are within the code as comments.\n",
    "You MUST answer by modifying this json: {ptt}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66b93b1c-a76d-433f-949f-4f6cd6fbe8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM:\n",
      "You're an excellent cybersecurity penetration tester assistant. \n",
      "You need to help the tester in a cybersecurity training process, and your commitment is essential to the task.\n",
      "Answer all the potential vulnerabilities determined in a logical, step-by-step manner that makes the reasoning process clear. You are given a Python dictionary, namely \"Penetration Testing Tree (PTT)\", and user input.\n",
      "First, carefully analyze the PTT. For the PTT,\n",
      "(1) The tasks are in a layered dictionary structure.\n",
      "(2) Each task has a completion status: inprogress, todo, done, or n/a.\n",
      "(3) You are given one specific sub-task labeled as todo.\n",
      "The user input is information obtained from doing the task marked as inprogress. If the task is completed, change the completion status to done.\n",
      "Extensively evaluate each task marked as todo, if a task is done update the status to done. These tasks marked as todo will be called todo tasks.\n",
      "Next, break down the todo tasks into sub-components and constraints that need to be addressed. This allows tackling the problem in a structured way. \n",
      "For each sub-component, leverage the knowledge and inference skills to generate multiple hypotheses or possibilities that could lead to a penetration.\n",
      "Critically evaluate each hypothesis based on validity, relevance to the current completion status, and how well it addresses the sub-component when logically combined with other steps.\n",
      "Using this critical analysis, deliberate over the most coherent combination and sequence of hypothesis steps to craft a logical reasoning chain.\n",
      "Throughout, aim to provide explanatory details on why certain options were considered more or less ideal to make the thought process transparent.\n",
      "If it was determined that there is a gap in the reasoning chain, backtrack and explore alternative hypotheses to plug the gap until there is a complete logical flow.\n",
      "Next, synthesize the key insights from the reasoning chain into further expansion of the tasks in the PTT while choosing exactly one task to mark as inprogress. The expansion\n",
      "should not destroy any information and modifications should be minimal. Ensure that vital details, such as IP addresses and user names remain in the PTT and each task description is as descriptive as possible.\n",
      "Finally, remove redundant/outdated tasks from the task list to keep tasks clear, precise, and detailed yet short due to the token size limit.\n",
      "\n",
      "In summary, leverage a structured, critical thinking process with iterative refinement to modify the PTT in the face of the user input to evaluate future strategies and to choose a new task as \n",
      "inprogress with minimal modifications.\n",
      "Answer with code examples, or fully functioning code. Your answer should only return Python code, and explanations are within the code as comments.\n",
      "You MUST answer by modifying this json: {ptt}\n",
      "USER: {prompt}\n",
      "ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "print(reasoning_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe49c362-5c5c-4625-b4f4-7e69b0903a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the Nmap scan conducted on IP 10.10.11.242 is as follows:\n",
      "The host is up with a latency of 0.020 seconds. The following ports were found to be open:\n",
      "- Port 22 (SSH) is open with the service OpenSSH 8.2p1 Ubuntu 4ubuntu0.9 (Ubuntu Linux; protocol 2.0).\n",
      "- Port 80 (HTTP) is open with the service nginx 1.18.0 (Ubuntu). The HTTP server is running on Ubuntu Linux.\n",
      "Nmap also discovered that HTTP methods supported by the web server are GET HEAD POST OPTIONS. The web server responded with \"http-title\" \"Did not follow redirect to http://devvortex.htb/\". The operating system running on the host is likely Linux with CPE \"cpe:/o:linux:linux_kernel\".\n",
      "Nmap also performed service detection using NSE scripts and discovered that the host is running Ubuntu Linux with kernel version 5.8.0-38-generic (Ubuntu Linux; protocol 2.0).\n",
      "This\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f05eb032-1c87-498a-9e5f-574a4b496f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "41897e10-2c4b-4773-98e8-d1181971a6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.23 ms per token,  4434.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8700.53 ms /   998 tokens (    8.72 ms per token,   114.71 tokens per second)\n",
      "llama_print_timings:        eval time =     280.59 ms /     1 runs   (  280.59 ms per token,     3.56 tokens per second)\n",
      "llama_print_timings:       total time =    8990.19 ms /   999 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     2 runs   (    0.22 ms per token,  4514.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6594.11 ms /   962 tokens (    6.85 ms per token,   145.89 tokens per second)\n",
      "llama_print_timings:        eval time =     277.67 ms /     1 runs   (  277.67 ms per token,     3.60 tokens per second)\n",
      "llama_print_timings:       total time =    6879.23 ms /   963 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     2 runs   (    0.23 ms per token,  4376.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6217.28 ms /   964 tokens (    6.45 ms per token,   155.05 tokens per second)\n",
      "llama_print_timings:        eval time =     262.65 ms /     1 runs   (  262.65 ms per token,     3.81 tokens per second)\n",
      "llama_print_timings:       total time =    6487.95 ms /   965 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.47 ms /     2 runs   (    0.23 ms per token,  4291.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6357.45 ms /   971 tokens (    6.55 ms per token,   152.73 tokens per second)\n",
      "llama_print_timings:        eval time =     274.90 ms /     1 runs   (  274.90 ms per token,     3.64 tokens per second)\n",
      "llama_print_timings:       total time =    6639.94 ms /   972 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.47 ms /     2 runs   (    0.24 ms per token,  4246.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7129.89 ms /  1005 tokens (    7.09 ms per token,   140.96 tokens per second)\n",
      "llama_print_timings:        eval time =     286.06 ms /     1 runs   (  286.06 ms per token,     3.50 tokens per second)\n",
      "llama_print_timings:       total time =    7425.15 ms /  1006 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.52 ms /     2 runs   (    0.26 ms per token,  3875.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6493.16 ms /  1006 tokens (    6.45 ms per token,   154.93 tokens per second)\n",
      "llama_print_timings:        eval time =     329.99 ms /     1 runs   (  329.99 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:       total time =    6831.61 ms /  1007 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     2 runs   (    0.23 ms per token,  4385.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6651.38 ms /  1014 tokens (    6.56 ms per token,   152.45 tokens per second)\n",
      "llama_print_timings:        eval time =     294.11 ms /     1 runs   (  294.11 ms per token,     3.40 tokens per second)\n",
      "llama_print_timings:       total time =    6954.60 ms /  1015 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.38 ms /     2 runs   (    0.19 ms per token,  5208.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6909.95 ms /  1016 tokens (    6.80 ms per token,   147.03 tokens per second)\n",
      "llama_print_timings:        eval time =     256.83 ms /     1 runs   (  256.83 ms per token,     3.89 tokens per second)\n",
      "llama_print_timings:       total time =    7172.95 ms /  1017 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =      15.14 ms /    67 runs   (    0.23 ms per token,  4425.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7084.34 ms /  1018 tokens (    6.96 ms per token,   143.70 tokens per second)\n",
      "llama_print_timings:        eval time =   20925.64 ms /    66 runs   (  317.06 ms per token,     3.15 tokens per second)\n",
      "llama_print_timings:       total time =   28283.33 ms /  1084 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.40 ms /     2 runs   (    0.20 ms per token,  5000.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7571.65 ms /  1048 tokens (    7.22 ms per token,   138.41 tokens per second)\n",
      "llama_print_timings:        eval time =     282.36 ms /     1 runs   (  282.36 ms per token,     3.54 tokens per second)\n",
      "llama_print_timings:       total time =    7861.47 ms /  1049 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =      15.94 ms /    71 runs   (    0.22 ms per token,  4455.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9191.67 ms /  1050 tokens (    8.75 ms per token,   114.23 tokens per second)\n",
      "llama_print_timings:        eval time =   23269.39 ms /    70 runs   (  332.42 ms per token,     3.01 tokens per second)\n",
      "llama_print_timings:       total time =   33173.48 ms /  1120 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     2 runs   (    0.21 ms per token,  4739.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7825.59 ms /  1084 tokens (    7.22 ms per token,   138.52 tokens per second)\n",
      "llama_print_timings:        eval time =     279.61 ms /     1 runs   (  279.61 ms per token,     3.58 tokens per second)\n",
      "llama_print_timings:       total time =    8113.09 ms /  1085 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     2 runs   (    0.24 ms per token,  4123.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7912.94 ms /  1085 tokens (    7.29 ms per token,   137.12 tokens per second)\n",
      "llama_print_timings:        eval time =     311.66 ms /     1 runs   (  311.66 ms per token,     3.21 tokens per second)\n",
      "llama_print_timings:       total time =    8233.79 ms /  1086 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     2 runs   (    0.21 ms per token,  4807.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8043.67 ms /  1091 tokens (    7.37 ms per token,   135.63 tokens per second)\n",
      "llama_print_timings:        eval time =     291.79 ms /     1 runs   (  291.79 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:       total time =    8342.61 ms /  1092 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     2 runs   (    0.24 ms per token,  4184.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8400.94 ms /  1102 tokens (    7.62 ms per token,   131.18 tokens per second)\n",
      "llama_print_timings:        eval time =     314.67 ms /     1 runs   (  314.67 ms per token,     3.18 tokens per second)\n",
      "llama_print_timings:       total time =    8724.79 ms /  1103 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.72 ms /     2 runs   (    0.36 ms per token,  2770.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8541.18 ms /  1127 tokens (    7.58 ms per token,   131.95 tokens per second)\n",
      "llama_print_timings:        eval time =     301.78 ms /     1 runs   (  301.78 ms per token,     3.31 tokens per second)\n",
      "llama_print_timings:       total time =    8864.57 ms /  1128 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'inprogress', 'task_description': 'Access web server on port 80 with default credentials (admin/admin)'}, {'status': 'todo', 'task_description': 'Attempt to brute force SSH with common credentials (root/toor, admin/admin)'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n",
      "Number of inprogress tasks:  1\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'inprogress', 'task_description': 'Access web server on port 80 with default credentials (admin/admin)'}, {'status': 'todo', 'task_description': 'Attempt to brute force SSH with common credentials (root/toor, admin/admin)'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n"
     ]
    }
   ],
   "source": [
    "reasoning_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.2)\n",
    "output_ptt = reasoning_module(reasoning_template, summary, ptt, llm, reasoning_sampler, force_add_task={\"recon\": 0, \"initial_access\": 2, \"execution\": 0, \"post_exploitation\": 0}, update_status=True, todo_task_descriptions=[\"Obtain a secret file with a hash in it\"])\n",
    "print(output_ptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e8267c6d-b59e-4461-a6d0-42043d502017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.39 ms /     2 runs   (    0.20 ms per token,  5089.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6918.29 ms /   998 tokens (    6.93 ms per token,   144.26 tokens per second)\n",
      "llama_print_timings:        eval time =     270.56 ms /     1 runs   (  270.56 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:       total time =    7195.19 ms /   999 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.50 ms /     2 runs   (    0.25 ms per token,  3992.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6940.23 ms /   962 tokens (    7.21 ms per token,   138.61 tokens per second)\n",
      "llama_print_timings:        eval time =     375.37 ms /     1 runs   (  375.37 ms per token,     2.66 tokens per second)\n",
      "llama_print_timings:       total time =    7326.24 ms /   963 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     2 runs   (    0.21 ms per token,  4651.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6825.92 ms /   964 tokens (    7.08 ms per token,   141.23 tokens per second)\n",
      "llama_print_timings:        eval time =     309.39 ms /     1 runs   (  309.39 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =    7142.92 ms /   965 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     2 runs   (    0.23 ms per token,  4319.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6942.68 ms /   971 tokens (    7.15 ms per token,   139.86 tokens per second)\n",
      "llama_print_timings:        eval time =     278.11 ms /     1 runs   (  278.11 ms per token,     3.60 tokens per second)\n",
      "llama_print_timings:       total time =    7227.83 ms /   972 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     2 runs   (    0.22 ms per token,  4608.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6890.81 ms /  1005 tokens (    6.86 ms per token,   145.85 tokens per second)\n",
      "llama_print_timings:        eval time =     271.84 ms /     1 runs   (  271.84 ms per token,     3.68 tokens per second)\n",
      "llama_print_timings:       total time =    7170.30 ms /  1006 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.39 ms /     2 runs   (    0.19 ms per token,  5181.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7186.08 ms /  1006 tokens (    7.14 ms per token,   139.99 tokens per second)\n",
      "llama_print_timings:        eval time =     265.97 ms /     1 runs   (  265.97 ms per token,     3.76 tokens per second)\n",
      "llama_print_timings:       total time =    7459.23 ms /  1007 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.41 ms /     2 runs   (    0.20 ms per token,  4889.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6930.54 ms /  1014 tokens (    6.83 ms per token,   146.31 tokens per second)\n",
      "llama_print_timings:        eval time =     283.96 ms /     1 runs   (  283.96 ms per token,     3.52 tokens per second)\n",
      "llama_print_timings:       total time =    7222.21 ms /  1015 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.41 ms /     2 runs   (    0.21 ms per token,  4854.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6833.96 ms /  1016 tokens (    6.73 ms per token,   148.67 tokens per second)\n",
      "llama_print_timings:        eval time =     269.07 ms /     1 runs   (  269.07 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:       total time =    7110.24 ms /  1017 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =      15.13 ms /    67 runs   (    0.23 ms per token,  4427.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7473.32 ms /  1018 tokens (    7.34 ms per token,   136.22 tokens per second)\n",
      "llama_print_timings:        eval time =   22733.46 ms /    66 runs   (  344.45 ms per token,     2.90 tokens per second)\n",
      "llama_print_timings:       total time =   30492.21 ms /  1084 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     2 runs   (    0.24 ms per token,  4210.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7549.94 ms /  1048 tokens (    7.20 ms per token,   138.81 tokens per second)\n",
      "llama_print_timings:        eval time =     309.53 ms /     1 runs   (  309.53 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =    7867.84 ms /  1049 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     2 runs   (    0.21 ms per token,  4784.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8232.20 ms /  1049 tokens (    7.85 ms per token,   127.43 tokens per second)\n",
      "llama_print_timings:        eval time =     301.28 ms /     1 runs   (  301.28 ms per token,     3.32 tokens per second)\n",
      "llama_print_timings:       total time =    8543.02 ms /  1050 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.47 ms /     2 runs   (    0.23 ms per token,  4282.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8220.80 ms /  1055 tokens (    7.79 ms per token,   128.33 tokens per second)\n",
      "llama_print_timings:        eval time =     325.38 ms /     1 runs   (  325.38 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =    8554.16 ms /  1056 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.68 ms /     2 runs   (    0.34 ms per token,  2932.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8672.94 ms /  1066 tokens (    8.14 ms per token,   122.91 tokens per second)\n",
      "llama_print_timings:        eval time =     419.39 ms /     1 runs   (  419.39 ms per token,     2.38 tokens per second)\n",
      "llama_print_timings:       total time =    9100.81 ms /  1067 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     2 runs   (    0.23 ms per token,  4395.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8123.56 ms /  1091 tokens (    7.45 ms per token,   134.30 tokens per second)\n",
      "llama_print_timings:        eval time =     335.42 ms /     1 runs   (  335.42 ms per token,     2.98 tokens per second)\n",
      "llama_print_timings:       total time =    8468.00 ms /  1092 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'todo', 'task_description': 'Try common credentials or use anonymization techniques like VPN services or Tor.'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n",
      "Number of inprogress tasks:  0\n",
      "Updating status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.70 ms /     3 runs   (    0.23 ms per token,  4273.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7906.64 ms /  1063 tokens (    7.44 ms per token,   134.44 tokens per second)\n",
      "llama_print_timings:        eval time =     651.44 ms /     2 runs   (  325.72 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =    8569.12 ms /  1065 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'task_description': 'Perform a full port scan on 10.10.11.242', 'status': 'done'}], 'initial_access': [{'task_description': 'Try common credentials or use anonymization techniques like VPN services or Tor.', 'status': 'todo'}], 'execution': [], 'post_exploitation': [{'task_description': 'Obtain a secret file with a hash in it', 'status': 'todo'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     2 runs   (    0.22 ms per token,  4608.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7122.73 ms /   992 tokens (    7.18 ms per token,   139.27 tokens per second)\n",
      "llama_print_timings:        eval time =     300.96 ms /     1 runs   (  300.96 ms per token,     3.32 tokens per second)\n",
      "llama_print_timings:       total time =    7438.91 ms /   993 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.23 ms per token,  4405.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7200.05 ms /   994 tokens (    7.24 ms per token,   138.05 tokens per second)\n",
      "llama_print_timings:        eval time =     315.62 ms /     1 runs   (  315.62 ms per token,     3.17 tokens per second)\n",
      "llama_print_timings:       total time =    7529.20 ms /   995 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.53 ms /     2 runs   (    0.27 ms per token,  3759.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7466.29 ms /  1001 tokens (    7.46 ms per token,   134.07 tokens per second)\n",
      "llama_print_timings:        eval time =     309.29 ms /     1 runs   (  309.29 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =    7785.60 ms /  1002 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.54 ms /     2 runs   (    0.27 ms per token,  3717.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8362.92 ms /  1035 tokens (    8.08 ms per token,   123.76 tokens per second)\n",
      "llama_print_timings:        eval time =     345.76 ms /     1 runs   (  345.76 ms per token,     2.89 tokens per second)\n",
      "llama_print_timings:       total time =    8718.55 ms /  1036 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.23 ms per token,  4434.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8296.69 ms /  1036 tokens (    8.01 ms per token,   124.87 tokens per second)\n",
      "llama_print_timings:        eval time =     314.02 ms /     1 runs   (  314.02 ms per token,     3.18 tokens per second)\n",
      "llama_print_timings:       total time =    8618.32 ms /  1037 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     2 runs   (    0.24 ms per token,  4184.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7874.97 ms /  1044 tokens (    7.54 ms per token,   132.57 tokens per second)\n",
      "llama_print_timings:        eval time =     330.10 ms /     1 runs   (  330.10 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:       total time =    8214.38 ms /  1045 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     2 runs   (    0.22 ms per token,  4514.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8746.10 ms /  1075 tokens (    8.14 ms per token,   122.91 tokens per second)\n",
      "llama_print_timings:        eval time =     312.77 ms /     1 runs   (  312.77 ms per token,     3.20 tokens per second)\n",
      "llama_print_timings:       total time =    9066.49 ms /  1076 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.50 ms /     2 runs   (    0.25 ms per token,  4016.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9008.00 ms /  1076 tokens (    8.37 ms per token,   119.45 tokens per second)\n",
      "llama_print_timings:        eval time =     334.00 ms /     1 runs   (  334.00 ms per token,     2.99 tokens per second)\n",
      "llama_print_timings:       total time =    9352.10 ms /  1077 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.54 ms /     2 runs   (    0.27 ms per token,  3690.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8901.02 ms /  1081 tokens (    8.23 ms per token,   121.45 tokens per second)\n",
      "llama_print_timings:        eval time =     507.87 ms /     1 runs   (  507.87 ms per token,     1.97 tokens per second)\n",
      "llama_print_timings:       total time =    9417.76 ms /  1082 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     2 runs   (    0.24 ms per token,  4140.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9388.51 ms /  1092 tokens (    8.60 ms per token,   116.31 tokens per second)\n",
      "llama_print_timings:        eval time =     333.65 ms /     1 runs   (  333.65 ms per token,     3.00 tokens per second)\n",
      "llama_print_timings:       total time =    9731.43 ms /  1093 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.52 ms /     2 runs   (    0.26 ms per token,  3838.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9172.01 ms /  1117 tokens (    8.21 ms per token,   121.78 tokens per second)\n",
      "llama_print_timings:        eval time =     369.03 ms /     1 runs   (  369.03 ms per token,     2.71 tokens per second)\n",
      "llama_print_timings:       total time =    9551.23 ms /  1118 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'todo', 'task_description': 'Try common credentials or use anonymization techniques like VPN services or Tor.'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n",
      "Number of inprogress tasks:  0\n",
      "Updating status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.75 ms /     3 runs   (    0.25 ms per token,  4010.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7963.81 ms /  1063 tokens (    7.49 ms per token,   133.48 tokens per second)\n",
      "llama_print_timings:        eval time =     702.02 ms /     2 runs   (  351.01 ms per token,     2.85 tokens per second)\n",
      "llama_print_timings:       total time =    8679.19 ms /  1065 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'task_description': 'Perform a full port scan on 10.10.11.242', 'status': 'done'}], 'initial_access': [{'task_description': 'Try common credentials or use anonymization techniques like VPN services or Tor.', 'status': 'inprogress'}], 'execution': [], 'post_exploitation': [{'task_description': 'Obtain a secret file with a hash in it', 'status': 'todo'}]}\n",
      "{'recon': [{'task_description': 'Perform a full port scan on 10.10.11.242', 'status': 'done'}], 'initial_access': [{'task_description': 'Try common credentials or use anonymization techniques like VPN services or Tor.', 'status': 'inprogress'}], 'execution': [], 'post_exploitation': [{'task_description': 'Obtain a secret file with a hash in it', 'status': 'todo'}]}\n"
     ]
    }
   ],
   "source": [
    "reasoning_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "output_ptt = reasoning_module(reasoning_template, summary, ptt, llm, reasoning_sampler, force_add_task={\"recon\": 0, \"initial_access\": 1, \"execution\": 0, \"post_exploitation\": 0}, update_status=True, todo_task_descriptions=[\"Obtain a secret file with a hash in it\"])\n",
    "print(output_ptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b6f472c0-4116-462a-9c21-ab9eac8b90c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.40 ms /     2 runs   (    0.20 ms per token,  4962.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7308.22 ms /   998 tokens (    7.32 ms per token,   136.56 tokens per second)\n",
      "llama_print_timings:        eval time =     264.41 ms /     1 runs   (  264.41 ms per token,     3.78 tokens per second)\n",
      "llama_print_timings:       total time =    7579.81 ms /   999 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.40 ms /     2 runs   (    0.20 ms per token,  5025.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6512.28 ms /   962 tokens (    6.77 ms per token,   147.72 tokens per second)\n",
      "llama_print_timings:        eval time =     254.96 ms /     1 runs   (  254.96 ms per token,     3.92 tokens per second)\n",
      "llama_print_timings:       total time =    6775.27 ms /   963 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     2 runs   (    0.23 ms per token,  4385.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6703.86 ms /   964 tokens (    6.95 ms per token,   143.80 tokens per second)\n",
      "llama_print_timings:        eval time =     305.97 ms /     1 runs   (  305.97 ms per token,     3.27 tokens per second)\n",
      "llama_print_timings:       total time =    7018.83 ms /   965 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.49 ms /     2 runs   (    0.25 ms per token,  4040.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7252.11 ms /   971 tokens (    7.47 ms per token,   133.89 tokens per second)\n",
      "llama_print_timings:        eval time =     280.71 ms /     1 runs   (  280.71 ms per token,     3.56 tokens per second)\n",
      "llama_print_timings:       total time =    7541.63 ms /   972 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     2 runs   (    0.22 ms per token,  4504.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7731.00 ms /  1005 tokens (    7.69 ms per token,   130.00 tokens per second)\n",
      "llama_print_timings:        eval time =     337.78 ms /     1 runs   (  337.78 ms per token,     2.96 tokens per second)\n",
      "llama_print_timings:       total time =    8076.40 ms /  1006 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.22 ms per token,  4484.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7585.62 ms /  1006 tokens (    7.54 ms per token,   132.62 tokens per second)\n",
      "llama_print_timings:        eval time =     298.92 ms /     1 runs   (  298.92 ms per token,     3.35 tokens per second)\n",
      "llama_print_timings:       total time =    7891.95 ms /  1007 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.41 ms /     2 runs   (    0.20 ms per token,  4901.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7508.11 ms /  1014 tokens (    7.40 ms per token,   135.05 tokens per second)\n",
      "llama_print_timings:        eval time =     269.03 ms /     1 runs   (  269.03 ms per token,     3.72 tokens per second)\n",
      "llama_print_timings:       total time =    7785.26 ms /  1015 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     2 runs   (    0.22 ms per token,  4566.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7189.87 ms /  1016 tokens (    7.08 ms per token,   141.31 tokens per second)\n",
      "llama_print_timings:        eval time =     276.46 ms /     1 runs   (  276.46 ms per token,     3.62 tokens per second)\n",
      "llama_print_timings:       total time =    7475.58 ms /  1017 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =      18.01 ms /    70 runs   (    0.26 ms per token,  3886.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7783.43 ms /  1018 tokens (    7.65 ms per token,   130.79 tokens per second)\n",
      "llama_print_timings:        eval time =   23563.76 ms /    69 runs   (  341.50 ms per token,     2.93 tokens per second)\n",
      "llama_print_timings:       total time =   31650.99 ms /  1087 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     2 runs   (    0.22 ms per token,  4545.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7466.85 ms /  1050 tokens (    7.11 ms per token,   140.62 tokens per second)\n",
      "llama_print_timings:        eval time =     280.20 ms /     1 runs   (  280.20 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:       total time =    7755.29 ms /  1051 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.47 ms /     2 runs   (    0.23 ms per token,  4273.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7465.40 ms /  1051 tokens (    7.10 ms per token,   140.78 tokens per second)\n",
      "llama_print_timings:        eval time =     284.27 ms /     1 runs   (  284.27 ms per token,     3.52 tokens per second)\n",
      "llama_print_timings:       total time =    7756.94 ms /  1052 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     2 runs   (    0.21 ms per token,  4705.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7600.07 ms /  1057 tokens (    7.19 ms per token,   139.08 tokens per second)\n",
      "llama_print_timings:        eval time =     270.01 ms /     1 runs   (  270.01 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:       total time =    7877.19 ms /  1058 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     2 runs   (    0.23 ms per token,  4329.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8293.97 ms /  1068 tokens (    7.77 ms per token,   128.77 tokens per second)\n",
      "llama_print_timings:        eval time =     301.09 ms /     1 runs   (  301.09 ms per token,     3.32 tokens per second)\n",
      "llama_print_timings:       total time =    8603.31 ms /  1069 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.22 ms per token,  4474.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8072.90 ms /  1093 tokens (    7.39 ms per token,   135.39 tokens per second)\n",
      "llama_print_timings:        eval time =     280.07 ms /     1 runs   (  280.07 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:       total time =    8361.54 ms /  1094 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'inprogress', 'task_description': 'Exploit the known vulnerability on port 22 (SSH) if possible'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n",
      "Number of inprogress tasks:  1\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'inprogress', 'task_description': 'Exploit the known vulnerability on port 22 (SSH) if possible'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n"
     ]
    }
   ],
   "source": [
    "reasoning_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.8)\n",
    "output_ptt = reasoning_module(reasoning_template, summary, ptt, llm, reasoning_sampler, force_add_task={\"recon\": 0, \"initial_access\": 1, \"execution\": 0, \"post_exploitation\": 0}, update_status=True, todo_task_descriptions=[\"Obtain a secret file with a hash in it\"])\n",
    "print(output_ptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f5e4f234-2a22-4e4c-8342-3b378826d0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     2 runs   (    0.21 ms per token,  4796.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6943.18 ms /   998 tokens (    6.96 ms per token,   143.74 tokens per second)\n",
      "llama_print_timings:        eval time =     289.51 ms /     1 runs   (  289.51 ms per token,     3.45 tokens per second)\n",
      "llama_print_timings:       total time =    7240.38 ms /   999 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     2 runs   (    0.21 ms per token,  4784.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6554.21 ms /   962 tokens (    6.81 ms per token,   146.78 tokens per second)\n",
      "llama_print_timings:        eval time =     283.23 ms /     1 runs   (  283.23 ms per token,     3.53 tokens per second)\n",
      "llama_print_timings:       total time =    6844.42 ms /   963 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     2 runs   (    0.21 ms per token,  4672.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6809.81 ms /   964 tokens (    7.06 ms per token,   141.56 tokens per second)\n",
      "llama_print_timings:        eval time =     411.59 ms /     1 runs   (  411.59 ms per token,     2.43 tokens per second)\n",
      "llama_print_timings:       total time =    7228.68 ms /   965 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     2 runs   (    0.21 ms per token,  4761.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7024.53 ms /   971 tokens (    7.23 ms per token,   138.23 tokens per second)\n",
      "llama_print_timings:        eval time =     280.14 ms /     1 runs   (  280.14 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:       total time =    7311.62 ms /   972 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.65 ms /     2 runs   (    0.32 ms per token,  3081.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6927.43 ms /  1005 tokens (    6.89 ms per token,   145.08 tokens per second)\n",
      "llama_print_timings:        eval time =     282.12 ms /     1 runs   (  282.12 ms per token,     3.54 tokens per second)\n",
      "llama_print_timings:       total time =    7218.50 ms /  1006 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.70 ms /     2 runs   (    0.35 ms per token,  2853.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7242.00 ms /  1006 tokens (    7.20 ms per token,   138.91 tokens per second)\n",
      "llama_print_timings:        eval time =     278.19 ms /     1 runs   (  278.19 ms per token,     3.59 tokens per second)\n",
      "llama_print_timings:       total time =    7528.64 ms /  1007 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.47 ms /     2 runs   (    0.23 ms per token,  4282.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7362.28 ms /  1014 tokens (    7.26 ms per token,   137.73 tokens per second)\n",
      "llama_print_timings:        eval time =     289.39 ms /     1 runs   (  289.39 ms per token,     3.46 tokens per second)\n",
      "llama_print_timings:       total time =    7659.37 ms /  1015 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     2 runs   (    0.24 ms per token,  4140.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7153.70 ms /  1016 tokens (    7.04 ms per token,   142.02 tokens per second)\n",
      "llama_print_timings:        eval time =     326.18 ms /     1 runs   (  326.18 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =    7494.47 ms /  1017 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =      21.66 ms /    80 runs   (    0.27 ms per token,  3692.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7970.35 ms /  1018 tokens (    7.83 ms per token,   127.72 tokens per second)\n",
      "llama_print_timings:        eval time =   26380.09 ms /    79 runs   (  333.93 ms per token,     2.99 tokens per second)\n",
      "llama_print_timings:       total time =   34724.27 ms /  1097 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     2 runs   (    0.23 ms per token,  4366.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7616.82 ms /  1062 tokens (    7.17 ms per token,   139.43 tokens per second)\n",
      "llama_print_timings:        eval time =     309.26 ms /     1 runs   (  309.26 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =    7935.47 ms /  1063 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.41 ms /     2 runs   (    0.21 ms per token,  4842.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7562.46 ms /  1063 tokens (    7.11 ms per token,   140.56 tokens per second)\n",
      "llama_print_timings:        eval time =     270.53 ms /     1 runs   (  270.53 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:       total time =    7840.30 ms /  1064 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.63 ms /     2 runs   (    0.32 ms per token,  3164.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8452.60 ms /  1068 tokens (    7.91 ms per token,   126.35 tokens per second)\n",
      "llama_print_timings:        eval time =     374.38 ms /     1 runs   (  374.38 ms per token,     2.67 tokens per second)\n",
      "llama_print_timings:       total time =    8836.27 ms /  1069 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     2 runs   (    0.22 ms per token,  4576.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8222.78 ms /  1079 tokens (    7.62 ms per token,   131.22 tokens per second)\n",
      "llama_print_timings:        eval time =     305.47 ms /     1 runs   (  305.47 ms per token,     3.27 tokens per second)\n",
      "llama_print_timings:       total time =    8540.61 ms /  1080 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.52 ms /     2 runs   (    0.26 ms per token,  3883.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7937.28 ms /  1104 tokens (    7.19 ms per token,   139.09 tokens per second)\n",
      "llama_print_timings:        eval time =     292.16 ms /     1 runs   (  292.16 ms per token,     3.42 tokens per second)\n",
      "llama_print_timings:       total time =    8239.20 ms /  1105 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'inprogress', 'task_description': 'Use \"ssh\" with default credentials (e.g., \"root:toor\" or \"user:pass\") to access the system.'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n",
      "Number of inprogress tasks:  1\n",
      "{'recon': [{'status': 'done', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'inprogress', 'task_description': 'Use \"ssh\" with default credentials (e.g., \"root:toor\" or \"user:pass\") to access the system.'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n"
     ]
    }
   ],
   "source": [
    "reasoning_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.6)\n",
    "output_ptt = reasoning_module(reasoning_template, summary, ptt, llm, reasoning_sampler, force_add_task={\"recon\": 0, \"initial_access\": 1, \"execution\": 0, \"post_exploitation\": 0}, update_status=True, todo_task_descriptions=[\"Obtain a secret file with a hash in it\"])\n",
    "print(output_ptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "15bfe3b1-ef59-4bb7-a8f3-0a18090480ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.94 ms /     3 runs   (    0.31 ms per token,  3194.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7812.34 ms /   998 tokens (    7.83 ms per token,   127.75 tokens per second)\n",
      "llama_print_timings:        eval time =     646.98 ms /     2 runs   (  323.49 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:       total time =    8472.00 ms /  1000 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.36 ms /     2 runs   (    0.18 ms per token,  5494.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7047.84 ms /   962 tokens (    7.33 ms per token,   136.50 tokens per second)\n",
      "llama_print_timings:        eval time =     263.13 ms /     1 runs   (  263.13 ms per token,     3.80 tokens per second)\n",
      "llama_print_timings:       total time =    7318.63 ms /   963 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.54 ms /     2 runs   (    0.27 ms per token,  3731.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7019.77 ms /   965 tokens (    7.27 ms per token,   137.47 tokens per second)\n",
      "llama_print_timings:        eval time =     309.89 ms /     1 runs   (  309.89 ms per token,     3.23 tokens per second)\n",
      "llama_print_timings:       total time =    7338.72 ms /   966 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.50 ms /     2 runs   (    0.25 ms per token,  4008.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8198.06 ms /   972 tokens (    8.43 ms per token,   118.56 tokens per second)\n",
      "llama_print_timings:        eval time =     567.46 ms /     1 runs   (  567.46 ms per token,     1.76 tokens per second)\n",
      "llama_print_timings:       total time =    8774.64 ms /   973 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.52 ms /     2 runs   (    0.26 ms per token,  3868.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8489.44 ms /  1007 tokens (    8.43 ms per token,   118.62 tokens per second)\n",
      "llama_print_timings:        eval time =     307.94 ms /     1 runs   (  307.94 ms per token,     3.25 tokens per second)\n",
      "llama_print_timings:       total time =    8807.52 ms /  1008 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.45 ms /     2 runs   (    0.23 ms per token,  4424.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7636.24 ms /  1008 tokens (    7.58 ms per token,   132.00 tokens per second)\n",
      "llama_print_timings:        eval time =     291.23 ms /     1 runs   (  291.23 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:       total time =    7937.87 ms /  1009 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.54 ms /     2 runs   (    0.27 ms per token,  3710.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7763.16 ms /  1016 tokens (    7.64 ms per token,   130.87 tokens per second)\n",
      "llama_print_timings:        eval time =     391.72 ms /     1 runs   (  391.72 ms per token,     2.55 tokens per second)\n",
      "llama_print_timings:       total time =    8164.55 ms /  1017 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.44 ms /     2 runs   (    0.22 ms per token,  4535.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8035.87 ms /  1018 tokens (    7.89 ms per token,   126.68 tokens per second)\n",
      "llama_print_timings:        eval time =     277.75 ms /     1 runs   (  277.75 ms per token,     3.60 tokens per second)\n",
      "llama_print_timings:       total time =    8322.46 ms /  1019 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =      18.47 ms /    65 runs   (    0.28 ms per token,  3518.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7602.95 ms /  1020 tokens (    7.45 ms per token,   134.16 tokens per second)\n",
      "llama_print_timings:        eval time =   23360.44 ms /    64 runs   (  365.01 ms per token,     2.74 tokens per second)\n",
      "llama_print_timings:       total time =   31272.08 ms /  1084 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.39 ms /     2 runs   (    0.19 ms per token,  5194.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7448.29 ms /  1048 tokens (    7.11 ms per token,   140.70 tokens per second)\n",
      "llama_print_timings:        eval time =     256.94 ms /     1 runs   (  256.94 ms per token,     3.89 tokens per second)\n",
      "llama_print_timings:       total time =    7712.47 ms /  1049 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.52 ms /     2 runs   (    0.26 ms per token,  3875.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7248.95 ms /  1049 tokens (    6.91 ms per token,   144.71 tokens per second)\n",
      "llama_print_timings:        eval time =     299.46 ms /     1 runs   (  299.46 ms per token,     3.34 tokens per second)\n",
      "llama_print_timings:       total time =    7561.58 ms /  1050 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     2 runs   (    0.21 ms per token,  4651.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7547.21 ms /  1055 tokens (    7.15 ms per token,   139.79 tokens per second)\n",
      "llama_print_timings:        eval time =     437.61 ms /     1 runs   (  437.61 ms per token,     2.29 tokens per second)\n",
      "llama_print_timings:       total time =    7993.26 ms /  1056 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.38 ms /     2 runs   (    0.19 ms per token,  5235.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7683.57 ms /  1066 tokens (    7.21 ms per token,   138.74 tokens per second)\n",
      "llama_print_timings:        eval time =     273.03 ms /     1 runs   (  273.03 ms per token,     3.66 tokens per second)\n",
      "llama_print_timings:       total time =    7963.18 ms /  1067 tokens\n",
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.67 ms /     2 runs   (    0.33 ms per token,  2994.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8256.88 ms /  1091 tokens (    7.57 ms per token,   132.13 tokens per second)\n",
      "llama_print_timings:        eval time =     306.07 ms /     1 runs   (  306.07 ms per token,     3.27 tokens per second)\n",
      "llama_print_timings:       total time =    8570.33 ms /  1092 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated\n",
      "{'recon': [{'status': 'inprogress', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'todo', 'task_description': 'Break into the system using the obtained credentials from the port scan.'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n",
      "Number of inprogress tasks:  1\n",
      "{'recon': [{'status': 'inprogress', 'task_description': 'Perform a full port scan on 10.10.11.242'}], 'initial_access': [{'status': 'todo', 'task_description': 'Break into the system using the obtained credentials from the port scan.'}], 'execution': [], 'post_exploitation': [{'status': 'todo', 'task_description': 'Obtain a secret file with a hash in it'}]}\n"
     ]
    }
   ],
   "source": [
    "reasoning_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.4)\n",
    "output_ptt = reasoning_module(reasoning_template, summary, ptt, llm, reasoning_sampler, force_add_task={\"recon\": 0, \"initial_access\": 1, \"execution\": 0, \"post_exploitation\": 0}, update_status=True, todo_task_descriptions=[\"Obtain a secret file with a hash in it\"])\n",
    "print(output_ptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30698dcb-f283-40e0-bd49-401b638ceb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptt = output_ptt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66c034-ef4b-4249-b0f5-5175aca0db2c",
   "metadata": {},
   "source": [
    "# Generative Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "15c6cc9d-6c1e-460c-9d82-2a68576922f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "64490c4c-1071-4a0a-8bab-c8f1c6fa0383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task is Use SSH on port 22 with default credentials (user: user / pass: pass) given {\"recon\": [{\"status\": \"done\", \"task_description\": \"Perform a full port scan on 10.10.11.242\"}], \"initial_access\": [{\"status\": \"inprogress\", \"task_description\": \"Use SSH on port 22 with default credentials (user: user / pass: pass)\"}], \"execution\": [], \"post_exploitation\": [{\"status\": \"todo\", \"task_description\": \"Obtain a secret file with a hash in it\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructions = generative_module(generative_prompt_template, ptt, llm, generative_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828413bb-fbaf-4ff0-bd26-ead40fec71b4",
   "metadata": {},
   "source": [
    "Issue below, the model hallucinates that we have a private key below and also does an unrelated task. For the purposes of this demo, I'll just do\n",
    "```\r\n",
    "ssh user@10.10.11.242\r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b388d968-72b4-44b3-9485-48345ea23c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh -i private_key_file user@10.10.11.242\n",
      "```\n",
      "where `private_key_file` is the private key file corresponding to the public key you possess that matches with the authorized_keys file on the target machine (10.10.11.242).\n",
      "\n",
      "If you do not have a private key file and only have a password, you can use\n",
      "```\n",
      "ssh user@10.10.11.242\n",
      "```\n",
      "and then when prompted, enter the password for user.\n",
      "\n",
      "After gaining access, you can check for the secret file and obtain the hash with the following commands:\n",
      "```\n",
      "cd /path/to/directory/containing/secret/file # If necessary, replace with the actual directory path\n",
      "cat secret_file # Assuming 'secret_file' is the file containing the hash\n",
      "```\n",
      "\n",
      "If you find a hash that needs cracking or if you need to run a password cracker tool like John The Ripper or Hashcat, you can install them using the following commands:\n",
      "```\n",
      "sudo apt-get update\n",
      "sudo apt-get install john\n",
      "sudo apt-get install hashcat\n",
      "```\n",
      "Then you can run John The Ripper or Hashcat with the hash found in the secret file. For example, if you found a hash in `secret_file`:\n",
      "```\n",
      "echo \"hash_from_secret_file\" | john --wordlist=/path/to/wordlist\n",
      "```\n",
      "or\n",
      "```\n",
      "hashcat -m [hashcat_mode] -a 0 -o cracked_passwords.txt secret_file /path/to/wordlist --potfile=dictionary_cracked.pot\n",
      "```\n",
      "Replace `[hashcat_mode]` with the appropriate mode for the hash type (e.g., `0`, `1000`, etc.). `/path/to/wordlist` is the path to your password dictionary file.\n",
      "\n",
      "Once you have cracked the hash or obtained the password from the secret file, you can proceed with further post-exploitation tasks as needed.\n"
     ]
    }
   ],
   "source": [
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1381aa8-82dd-42fb-8b84-cbc0e309b8a6",
   "metadata": {},
   "source": [
    "# Input Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b96affea-70c8-4179-a8e6-b09ba78f5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_output = \"\"\"\n",
    "~$ ssh admin@10.10.11.242\n",
    "admin@10.10.11.242's password:\n",
    "Permission denied, please try again.\n",
    "admin@10.10.11.242's password:\n",
    "Permission denied, please try again.\n",
    "admin@10.10.11.242's password:\n",
    "\n",
    "~$ ssh root@10.10.11.242\n",
    "root@10.10.11.242's password:\n",
    "Permission denied, please try again.\n",
    "root@10.10.11.242's password:\n",
    "Permission denied, please try again.\n",
    "root@10.10.11.242's password:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62947ee5-af8e-4735-bbb7-0f1ff2cdc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e567a5ff-ff6a-4f4e-b9df-20cb1ff75832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =      49.46 ms /   234 runs   (    0.21 ms per token,  4731.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3575.71 ms /   465 tokens (    7.69 ms per token,   130.04 tokens per second)\n",
      "llama_print_timings:        eval time =   66180.75 ms /   233 runs   (  284.04 ms per token,     3.52 tokens per second)\n",
      "llama_print_timings:       total time =   70533.70 ms /   698 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of the terminal output is as follows:\n",
      "- The `ssh` command was tried with both `admin` and `root` user credentials on `10.10.11.242`.\n",
      "- The password was entered three times for each user with incorrect credentials.\n",
      "- Each time, the message \"Permission denied\" was displayed, indicating that the credentials were incorrect.\n",
      "- It is possible that there is no password set on the system or that the password is incorrect for both users.\n",
      "- If the SSH service is running on the target machine (10.10.11.242), it can be accessed with these credentials.\n",
      "- The system is not configured with key-based authentication (which would allow for direct login without prompting for passwords), or the credentials were not set up correctly on the system that is trying to access it.\n",
      "- The next step would be to gather more information about this system, such as whether it is virtualized or running any services that could be exploited, and use different credentials or methods (like brute force or exploiting known vulnerabilities) if necessary.\n"
     ]
    }
   ],
   "source": [
    "summary = input_parser(input_parsing_template, command_output, llm, summary_sampler, max_tokens=250)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c438d8-93b1-42bc-8b0d-cef781ee8ac9",
   "metadata": {},
   "source": [
    "issue: As the summary goes on we get wrong conclusions/summary is doing misguided conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd31f88-84cc-4f45-9b2d-19c3d3c931d3",
   "metadata": {},
   "source": [
    "# Reasoning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b15e4aa-65d9-4676-8c1a-26ca8b4f4103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  114165.22 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     2 runs   (    0.23 ms per token,  4385.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7207.07 ms /  1010 tokens (    7.14 ms per token,   140.14 tokens per second)\n",
      "llama_print_timings:        eval time =     257.26 ms /     1 runs   (  257.26 ms per token,     3.89 tokens per second)\n",
      "llama_print_timings:       total time =    7918.31 ms /  1011 tokens\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output_ptt \u001b[38;5;241m=\u001b[39m \u001b[43mreasoning_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreasoning_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_add_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minitial_access\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost_exploitation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtodo_task_descriptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mObtain a secret file with a hash in it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_ptt)\n",
      "File \u001b[1;32mD:\\personal_projects\\whiterabbitneo-pentestgpt\\notebooks\\..\\schema.py:116\u001b[0m, in \u001b[0;36mreasoning_module\u001b[1;34m(template, prompt, ptt, llm, sampler, max_spaces, force_add_task, update_status, todo_task_descriptions)\u001b[0m\n\u001b[0;32m    114\u001b[0m     inprogress_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m     updated \u001b[38;5;241m=\u001b[39m \u001b[43madd_new_items_outlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minprogress_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_spaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_add_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_add_task\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     num_inprogress \u001b[38;5;241m=\u001b[39m find_inprogress(updated)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\personal_projects\\whiterabbitneo-pentestgpt\\notebooks\\..\\schema.py:256\u001b[0m, in \u001b[0;36madd_new_items_outlines\u001b[1;34m(llm, prompt, ptt, sampler, inprogress_set, max_spaces, force_add_task)\u001b[0m\n\u001b[0;32m    254\u001b[0m continue_choices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m],\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    255\u001b[0m tree_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(ptt)\n\u001b[1;32m--> 256\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43madd_whitespace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_spaces\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_whitespace(prompt, llm, sampler, max_spaces)\n",
      "File \u001b[1;32mD:\\personal_projects\\whiterabbitneo-pentestgpt\\notebooks\\..\\schema.py:225\u001b[0m, in \u001b[0;36madd_whitespace\u001b[1;34m(prompt, llm, sampler, max_spaces)\u001b[0m\n\u001b[0;32m    219\u001b[0m whitespace_regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn]*\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m generator \u001b[38;5;241m=\u001b[39m outlines\u001b[38;5;241m.\u001b[39mgenerate\u001b[38;5;241m.\u001b[39mregex(\n\u001b[0;32m    221\u001b[0m     llm,\n\u001b[0;32m    222\u001b[0m     whitespace_regex,\n\u001b[0;32m    223\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler\n\u001b[0;32m    224\u001b[0m )\n\u001b[1;32m--> 225\u001b[0m whitespace \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_spaces\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m whitespace\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\senior_project\\lib\\site-packages\\outlines\\models\\llamacpp.py:41\u001b[0m, in \u001b[0;36mLlamaSequenceGenerator.__call__\u001b[1;34m(self, prompts, max_tokens, stop_at, rng, **model_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     processors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits_processor\u001b[38;5;241m.\u001b[39mcopy()]\n\u001b[1;32m---> 41\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[0;32m     42\u001b[0m     prompt,\n\u001b[0;32m     43\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m     44\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop_at,\n\u001b[0;32m     45\u001b[0m     seed\u001b[38;5;241m=\u001b[39mrng\u001b[38;5;241m.\u001b[39minitial_seed(),\n\u001b[0;32m     46\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mLogitsProcessorList(processors),\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m     48\u001b[0m )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     49\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\senior_project\\lib\\site-packages\\llama_cpp\\llama.py:1474\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1472\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[1;32m-> 1474\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\senior_project\\lib\\site-packages\\llama_cpp\\llama.py:1000\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m    998\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    999\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1000\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1001\u001b[0m     prompt_tokens,\n\u001b[0;32m   1002\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1003\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1004\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1005\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1006\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1007\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1008\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1009\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1010\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1011\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1012\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1013\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1014\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1015\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1016\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1017\u001b[0m ):\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[0;32m   1019\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\senior_project\\lib\\site-packages\\llama_cpp\\llama.py:682\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[0;32m    684\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    685\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    686\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[0;32m    701\u001b[0m         )\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\senior_project\\lib\\site-packages\\llama_cpp\\llama.py:522\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    518\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m    520\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m    521\u001b[0m )\n\u001b[1;32m--> 522\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\senior_project\\lib\\site-packages\\llama_cpp\\_internals.py:311\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_ptt = reasoning_module(reasoning_template, summary, ptt, llm, reasoning_sampler, force_add_task={\"recon\": 0, \"initial_access\": 1, \"execution\": 0, \"post_exploitation\": 0}, update_status=True, todo_task_descriptions=[\"Obtain a secret file with a hash in it\"])\n",
    "print(output_ptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984822d8-181c-40b0-ad90-934cf279054c",
   "metadata": {},
   "source": [
    "# Default QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb0bb3-a084-4e39-b19f-0278dd52b562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e6982-2e11-448f-aaaa-128616186698",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_qa_template = \"\"\"SYSTEM:\n",
    "Answer the Question by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\n",
    " Always answer without hesitation.\n",
    "USER: {prompt}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60d525-0408-498b-9085-061cf0d6cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5485b-d23e-48f1-ba41-611b9167a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = default_qa(default_qa_template, \"How can I get typical username and password lists to use for hydra?\", llm, qa_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056a7c5-fa70-4a98-816c-72a7a604b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ef6de-acf6-4665-b552-63bcb0f341b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = default_qa(default_qa_template, \"Can you give me some typical default password/usernames for ssh?\", llm, qa_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fae7608-37c0-4bc8-9b06-6c8fe452a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc9c45-6a59-4ac0-927b-2f36340dd95f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
