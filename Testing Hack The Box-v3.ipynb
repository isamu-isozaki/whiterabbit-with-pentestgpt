{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d44b1f-e7a6-4cc7-95ec-d6c5b6566d09",
   "metadata": {},
   "source": [
    "The goal of this notebook is to solve \n",
    "DevVortex on htb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e33ae58-2330-49d2-96e4-7d906536ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb7ad84-16f6-453a-b0de-a693ac6533c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whiterabbitneo-13b.Q4_K_S.gguf    whiterabbitneo-33b-v1.Q4_K_M.gguf\n",
      "whiterabbitneo-13b.Q5_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/chinguyen/Documents/personalProjects/whiterabbit_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6bad156-f443-44ee-bbf7-3f8f7c27d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert task to adding new todo tasks+changing status\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../outlines-dev\")\n",
    "import os\n",
    "import traceback\n",
    "from schema import reasoning_module, generative_module, input_parser, default_qa, load_outlines, get_current_status, find_inprogress_task\n",
    "from torch import Generator\n",
    "from outlines.samplers import Sampler, multinomial\n",
    "import outlines\n",
    "import pickle\n",
    "import pprint\n",
    "import json\n",
    "import copy\n",
    "\n",
    "model_paths=[\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q3_K_S.gguf\",\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q4_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    # r\"D:\\projects\\gamified-cybersecurity-ai-server\\model\\whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "    \"/Users/chinguyen/Documents/personalProjects/whiterabbit_models/whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    \"/Users/chinguyen/Documents/personalProjects/whiterabbit_models/whiterabbitneo-33b-v1.Q4_K_M.gguf\",\n",
    "    \n",
    "]\n",
    "output_path = \"./benchmark\"\n",
    "\n",
    "instance = {\n",
    "    \"n_gpu_layers\": 41,\n",
    "    \"n_batch\": 2048,\n",
    "    \"top_p\": 1.0,\n",
    "    \"temperature\": 1.0,\n",
    "    \"generate_len\": 2048,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "import outlines\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f532adc-344d-48e1-aa49-7e9be762e9c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from /Users/chinguyen/Documents/personalProjects/whiterabbit_models/whiterabbitneo-13b.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = whiterabbitneo_whiterabbitneo-13b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  273 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.90 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = whiterabbitneo_whiterabbitneo-13b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  6982.33 MiB, ( 6982.39 / 10922.67)\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    87.93 MiB\n",
      "llm_load_tensors:      Metal buffer size =  6982.33 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/isamu/miniconda3/envs/senior_project/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1600.00 MiB, ( 8584.20 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    72.04 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   816.02 MiB, ( 9400.22 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   816.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    40.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '16384', 'llama.attention.head_count': '40', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '14', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'whiterabbitneo_whiterabbitneo-13b'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "llm, sampler = load_outlines(model_paths[0], instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acee435c-29e4-4eb0-ba62-e8080645a07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1349d0f30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = Generator(device=\"cpu\")\n",
    "rng.manual_seed(789005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d22d9-2c6f-4239-abd5-290fa56db5d5",
   "metadata": {},
   "source": [
    "# Prompt Templates and samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc73e463-8c6f-4c05-b003-741dc6573c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_prompt_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant.\n",
    "Answer the detailed steps to perform the task from the user by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the task to extract the key information components and break it down into logical sub-tasks. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-task, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards a resolution for the task. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully resolve the task or has already been done based on the chat history, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on the thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly resolves all sub-tasks in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final resolution in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "- Do not use automated scanning tools such as Nessus or OpenVAS. You should use manual penetration testing tools such as nmap, nikto, gobuster, etc.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful resolutions.\n",
    "Always answer without hesitation.\n",
    "CURRENT_PROGRESS: {ptt}\n",
    "CHAT_HISTORY: {history}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "\"\"\"\n",
    "generative_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "input_parsing_templates = {\"terminal\": \"\"\"SYSTEM:\n",
    "Summarize the terminal output to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the vulnerable/non-vulnerable services. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions \n",
    "to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "TERMINAL_OUTPUT: {prompt}\n",
    "SUMMARY:\"\"\",\n",
    "                           \"web\": \"\"\"SYSTEM:\n",
    "Summarize the content of webpages to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the key widgets, contents, buttons, and comments that can be useful for penetration testing. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the terminal output to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions\n",
    "to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "TERMINAL_OUTPUT: {prompt}\n",
    "SUMMARY:\"\"\"}\n",
    "\n",
    "input_parsing_past_summaries_template = \"\"\"SYSTEM:\n",
    "Summarize the summaries to get the main points as if you are reporting to a senior penetration tester for further guidance. Your output will be provided to another large language model, so the result should be short and precise for token limit reasons.\n",
    "First, comprehensively analyze the output and identify the test results and the vulnerable/non-vulnerable services. This allows for the other model to tackle attacking in a structured way.\n",
    "For each service, given the test result, ensure that all detail that is needed for further testing is kept intact. For instance, you should keep both the port number and the service name/version if a port is open.\n",
    "Finally, critically evaluate each summary to confirm that only summarization is done and no assumptions or conclusions are made. If reasoning is done omit said reasoning.\n",
    "In conclusion, create a concise summary by evaluating the summaries to get the key information and vulnerable/non-vulnerable services, ensure all important details remain intact, and critically evaluate that the summary only distills information with no assumptions or conclusions \n",
    "to assist another large language model acting as a senior penetration tester\n",
    "Always answer without hesitation.\n",
    "SUMMARIES: {prompt}\n",
    "SUMMARY:\"\"\"\n",
    "summary_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.1)\n",
    "choice_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.1)\n",
    "# reasoning_template = \"\"\"SYSTEM:\n",
    "# You're an excellent cybersecurity penetration tester assistant. \n",
    "# You need to help the tester in a cybersecurity training process, and your commitment is essential to the task.\n",
    "# Answer all the potential vulnerabilities determined in a logical, step-by-step manner that makes the reasoning process clear. You are given a Python dictionary, namely \"Penetration Testing Tree (PTT)\", user input, and chat history.\n",
    "# First, carefully analyze the PTT. For the PTT,\n",
    "# (1) The tasks are in a layered dictionary structure.\n",
    "# (2) Each task has a completion status: inprogress, todo, done.\n",
    "# The user input is information obtained from doing the task marked as inprogress. Chat history is the past progress. If the task is completed, change the completion status to done.\n",
    "# Extensively evaluate each task marked as todo, if a task is done update the status to done. These tasks marked as todo will be called todo tasks.\n",
    "# Next, break down the todo tasks into sub-components and constraints that need to be addressed. This allows tackling the problem in a structured way. \n",
    "# For each sub-component, leverage the knowledge and inference skills to generate multiple hypotheses or possibilities that could lead to a penetration.\n",
    "# Critically evaluate each hypothesis based on validity, relevance to the current completion status, and how well it addresses the sub-component when logically combined with other steps.\n",
    "# Using this critical analysis, deliberate over the most coherent combination and sequence of hypothesis steps to craft a logical reasoning chain.\n",
    "# Throughout, aim to provide explanatory details on why certain options were considered more or less ideal to make the thought process transparent.\n",
    "# If it was determined that there is a gap in the reasoning chain, backtrack and explore alternative hypotheses to plug the gap until there is a complete logical flow.\n",
    "# Next, synthesize the key insights from the reasoning chain into further expansion of the tasks in the PTT while choosing exactly one task to mark as inprogress. The expansion\n",
    "# should not destroy any information and modifications should be minimal. Ensure that vital details, such as IP addresses and user names remain in the PTT and each task description is as descriptive as possible.\n",
    "# Do not use automated scanning tools such as Nessus or OpenVAS. You should use manual penetration testing tools such as nmap, nikto, gobuster, etc.\n",
    "# Finally, remove redundant/outdated tasks from the task list to keep tasks clear, precise, and detailed.\n",
    "\n",
    "# In summary, leverage a structured, critical thinking process with iterative refinement to modify the PTT in the face of the user input to evaluate future strategies to mark tasks that are finished as done,\n",
    "# add new tasks with status todo and mark one task as inprogress.\n",
    "# Answer only with an output PTT. Your answer should only return a dictionary with no explanations.\n",
    "# You MUST answer by modifying this PTT: {ptt}\n",
    "# CHAT_HISTORY: {history}\n",
    "# USER: {prompt}\n",
    "# \"\"\"\n",
    "reasoning_template = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant. \n",
    "You need to help the tester in a cybersecurity training process, and your commitment is essential to the task.\n",
    "Answer all the potential vulnerabilities determined in a logical, step-by-step manner that makes the reasoning process clear. You are given a Python dictionary, namely \"Penetration Testing Tree (PTT)\", user input, and chat history.\n",
    "First, carefully analyze the PTT. For the PTT,\n",
    "(1) The tasks are in a layered dictionary structure.\n",
    "(2) Each task has a completion status: inprogress, todo, done, and remove.\n",
    "The user input is information obtained from doing the task marked as inprogress. Chat history is the past progress. If the task is completed, change the completion status to done.\n",
    "Extensively evaluate each task marked as todo, if a task is done update the status to done. These tasks marked as todo will be called todo tasks.\n",
    "Next, break down the todo tasks into sub-components and constraints that need to be addressed. This allows tackling the problem in a structured way. \n",
    "For each sub-component, leverage the knowledge and inference skills to generate multiple hypotheses or possibilities that could lead to a penetration.\n",
    "Critically evaluate each hypothesis based on validity, relevance to the current completion status, and how well it addresses the sub-component when logically combined with other steps.\n",
    "Using this critical analysis, deliberate over the most coherent combination and sequence of hypothesis steps to craft a logical reasoning chain.\n",
    "Throughout, aim to provide explanatory details on why certain options were considered more or less ideal to make the thought process transparent.\n",
    "If it was determined that there is a gap in the reasoning chain, backtrack and explore alternative hypotheses to plug the gap until there is a complete logical flow.\n",
    "Next, synthesize the key insights from the reasoning chain into further expansion of the tasks in the PTT while choosing exactly one task to mark as inprogress. Ensure that vital details, such as IP addresses and user names remain in the PTT and each task description is as descriptive as possible.\n",
    "Do not use automated scanning tools such as Nessus or OpenVAS. You should use manual penetration testing tools such as nmap, nikto, gobuster, etc.\n",
    "Finally, remove redundant/outdated tasks from the task list by updating the status to remove to keep tasks clear, precise, and detailed.\n",
    "\n",
    "In summary, leverage a structured, critical thinking process with iterative refinement to modify the PTT in the face of the user input to evaluate future strategies to mark tasks that are finished as done,\n",
    "add new tasks with status todo, mark one task as inprogress. and mark tasks that are not needed as remove.\n",
    "Answer only with an output PTT. Your answer should only return a dictionary with no explanations.\n",
    "You MUST answer by modifying this PTT: {ptt}\n",
    "CHAT_HISTORY: {history}\n",
    "USER: {prompt}\n",
    "\"\"\"\n",
    "reasoning_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "default_qa_template = \"\"\"SYSTEM:\n",
    "Answer the Question by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\n",
    " Always answer without hesitation.\n",
    "USER: {prompt}\n",
    "\"\"\"\n",
    "qa_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=1.0)\n",
    "filter_template = \"\"\"SYSTEM:\n",
    "Answer the Question by exploring multiple reasoning paths as follows:\n",
    "- First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\n",
    "- For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\n",
    "- Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\n",
    "- Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\n",
    "- If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\n",
    "- Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\n",
    "- Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\n",
    "- Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\n",
    "In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\n",
    " Always answer without hesitation.\n",
    "USER: Given {history}, does {task} help in pentesting? To determine if a task helps in pentesting, extensively analyze if the service/exploit being examined is available. For example, if \n",
    "nmap didn't reveal FTP servers then all tasks related to FTP vulnerabilities are not needed. Answer with only true or false.\n",
    "ASSISTANT: \"\"\"\n",
    "filter_sampler = multinomial(top_k=instance[\"top_k\"], top_p=instance[\"top_p\"], temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf59818c-aaa0-4e39-9bfa-572920c9bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options(prompt, options):\n",
    "    output = \"\"\n",
    "    assert output not in options\n",
    "    option_str = options[0] + \"/\"\n",
    "    for i, option in enumerate(options):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        option_str += option + \"/\"\n",
    "    option_str = option_str[:-1]\n",
    "    prompt += f\" Answer with {option_str}\"\n",
    "    while output not in options:\n",
    "        output = input(prompt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb061a4-a356-4b4f-b01e-66958dc57212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_todo_tasks(ptt_dict: dict[str, list]) -> list[str]:\n",
    "    output = []\n",
    "    for key in ptt_dict:\n",
    "        for task in ptt_dict[key]:\n",
    "            if task[\"status\"] == \"todo\":\n",
    "                output.append(task[\"task\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a7b932e-be38-4799-b5ef-5377f496f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instructions(template, ptt, llm, sampler, all_instructions, current_history=\"\", force_command=False, only_provide_currrent_status=True, dataset=[]):\n",
    "    prompt = template\n",
    "    if force_command:\n",
    "        prompt += \"The commands to do the tasks are ```bash\"\n",
    "    \n",
    "    while True:\n",
    "        instructions = generative_module(prompt, ptt, llm, sampler, current_history, only_provide_currrent_status=only_provide_currrent_status)\n",
    "        print(instructions)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        generative_ptt = ptt\n",
    "        if only_provide_currrent_status:\n",
    "            generative_ptt = get_current_status(ptt)\n",
    "        task = find_inprogress_task(ptt)\n",
    "        generative_ptt = json.dumps(generative_ptt)\n",
    "        generative_prompt = template.format(ptt=generative_ptt, prompt=task, history=current_history)\n",
    "        dataset.append({\"prompt\": generative_prompt, \"output\": instructions, \"correct\": correct})\n",
    "        if correct == \"y\":\n",
    "            print(\"Got instruction\")\n",
    "            all_instructions.append(instructions)\n",
    "            return instructions\n",
    "        if not force_command:\n",
    "            if get_options(\"Should we force the output to commands?\", [\"y\", \"n\"]) == \"y\":\n",
    "                prompt += \"The commands to do the tasks are ```bash\"\n",
    "                force_command = True\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "def do_qa(template, llm, sampler, force_command=False, dataset=[]):\n",
    "    do_question = get_options(\"Do you have questions?\", [\"y\", \"n\"])\n",
    "    if do_question == \"n\":\n",
    "        return\n",
    "    prompt = template\n",
    "    question = input(\"What is your question?\")\n",
    "    if force_command:\n",
    "        prompt += \"The commands to do the tasks are ```bash\"\n",
    "    while True:\n",
    "        instructions = default_qa(prompt, question, llm, sampler)\n",
    "        print(instructions)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        generative_prompt = template.format(prompt=question)\n",
    "        dataset.append({\"prompt\": generative_prompt, \"output\": instructions, \"correct\": correct})\n",
    "        if correct == \"y\":\n",
    "            print(\"Got answer\")\n",
    "            new_q = get_options(\"Do you have another question?\", [\"y\", \"n\"])\n",
    "            if new_q == \"n\":\n",
    "                return\n",
    "            question = input(\"What is your question?\")\n",
    "        force_command_response = get_options(\"Should we force the output to commands?\", [\"y\", \"n\"])\n",
    "        if not force_command and force_command_response == \"y\":\n",
    "            prompt += \"The commands to do the tasks are ```bash\"\n",
    "            force_command = True\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "def get_summary(template, ptt, llm, sampler, summaries, all_command_outputs, max_tokens=250, dataset=[]):\n",
    "    if len(summaries) == len(all_command_outputs):\n",
    "        tool = get_options(\"Tell us the tool you got the output from.\", [\"terminal\", \"web\"])\n",
    "        options_desc = {\n",
    "            \"terminal\": \" Paste the output of the security test tool used\",\n",
    "            \"web\": \" Paste the relevant content of a web page\",\n",
    "        }\n",
    "        assert tool in options_desc\n",
    "        output = input(options_desc[tool])\n",
    "    else:\n",
    "        output = all_command_outputs[-1]\n",
    "    while True:\n",
    "        if get_options(\"Directly return output instead of summarizing?\", [\"y\", \"n\"]) == \"y\":\n",
    "            summaries.append(output)\n",
    "            all_command_outputs.append(output)\n",
    "            return output\n",
    "        summary = input_parser(template[tool], output, llm, sampler, max_tokens=max_tokens)\n",
    "        print(summary)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        input_parser_prompt = template[tool].format(prompt=output)\n",
    "        dataset.append({\"prompt\": input_parser_prompt, \"output\": summary, \"correct\": correct})\n",
    "        if correct == \"y\":\n",
    "            summaries.append(summary)\n",
    "            all_command_outputs.append(output)\n",
    "            return summary\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "def summarize_summaries(template, llm, sampler, summaries, max_summaries=2, max_tokens=300, full=False, dataset=[]):\n",
    "    if len(summaries) == 0:\n",
    "        return \"\"\n",
    "    if len(summaries) == 1:\n",
    "        if full:\n",
    "            return summaries[0]\n",
    "        return \"\"\n",
    "    if len(summaries) == 2:\n",
    "        if not full:\n",
    "            return summaries[0]\n",
    "    if full:\n",
    "        offset= 0\n",
    "    else:\n",
    "        offset= 1\n",
    "    summaries_of_interest = summaries[-max_summaries-offset:]\n",
    "    if not full:\n",
    "        summaries_of_interest = summaries_of_interest[:-1]\n",
    "    summaries_combined = \"\\n\"\n",
    "    for i, summary in enumerate(summaries_of_interest):\n",
    "        summaries_combined += f\"summary {i+1}: {summary}\\n\\n\"\n",
    "    print(\"SUMMARIES: \", summaries_combined)\n",
    "    # return_concatenated_summaries = get_options(\"Return concatenated summaries?\", [\"y\", \"n\"])\n",
    "    # if return_concatenated_summaries == \"y\":\n",
    "    #     return return_concatenated_summaries\n",
    "    while True:\n",
    "        past_history = input_parser(template, summaries_combined, llm, sampler, max_tokens=max_tokens)\n",
    "        print(past_history)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        input_parser_prompt = template.format(prompt=summaries_combined)\n",
    "        dataset.append({\"prompt\": input_parser_prompt, \"output\": past_history, \"correct\": correct})\n",
    "        if correct == \"y\":\n",
    "            return past_history\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "\n",
    "\n",
    "def get_new_ptt(template, summary, past_history, ptt, llm, sampler, ptts, max_spaces=0, delete_done: bool =False, choice_temperature=0.3, generate_discussion=True, ask_model_for_add_task_num=False, dataset=[]):\n",
    "    inprogress_done = get_options(\"Force set current task to done?\", [\"y\", \"n\"])\n",
    "    if inprogress_done == \"y\":\n",
    "        for key in ptt:\n",
    "            for task in ptt[key]:\n",
    "                if \"inprogress\" in task[\"status\"]:\n",
    "                    task[\"status\"] = \"done\"\n",
    "    reasoning_module_prompt = template.format(ptt=json.dumps(ptt), history=past_history, prompt=summary).strip()\n",
    "    inprogress_sampler = multinomial(top_k=50, top_p=1.0, temperature=1.0)\n",
    "    generator = outlines.generate.text(llm, sampler=inprogress_sampler)\n",
    "    reasoning_module_prompt += \"\\nNEXT_INPROGRESS_TASK: \"\n",
    "    generator = outlines.generate.choice(llm, choices=find_todo_tasks(ptt), sampler=inprogress_sampler)\n",
    "    while True:\n",
    "        inprogress_task = generator(reasoning_module_prompt)\n",
    "        print(\"The inprogress task is\", inprogress_task)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        dataset.append({\"prompt\": reasoning_module_prompt, \"output\": inprogress_task, \"correct\": correct})\n",
    "        if correct == \"y\":\n",
    "            break\n",
    "    force_add_task = {}\n",
    "    if ask_model_for_add_task_num:\n",
    "        generator = outlines.generate.choice(llm, [\"0\", \"1\"], sampler=choice_sampler)\n",
    "    for key in list(ptt.keys()):\n",
    "        num_add_task_question = f\"How many {key} tasks do you want to add? Enter an integer: \"\n",
    "        if not ask_model_for_add_task_num:\n",
    "            num_add_tasks =  int(input(num_add_task_question))\n",
    "            force_add_task[key] = num_add_tasks \n",
    "        else:\n",
    "            num_added_prompt = reasoning_module_prompt+\"NUM_ADDED_TASKS: \"+num_add_task_question\n",
    "            while True:\n",
    "                num_added_tasks = int(generator(num_added_prompt))\n",
    "                print(f\"{num_add_task_question}{num_added_tasks}\")\n",
    "                correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "                dataset.append({\"prompt\": num_added_prompt, \"output\": str(num_added_tasks), \"correct\": correct})\n",
    "                if correct == \"y\":\n",
    "                    force_add_task[key] = num_added_tasks\n",
    "                    break\n",
    "    discussion = \"\"\n",
    "    if generate_discussion:\n",
    "        generator = outlines.generate.text(llm, sampler=sampler)\n",
    "        reasoning_module_prompt += inprogress_task + \"DISCUSSION:\"\n",
    "        while True:\n",
    "            discussion = generator(reasoning_module_prompt, stop_at=\"{\")\n",
    "            discussion = discussion[:-1]\n",
    "            print(\"Generated discussion: \", discussion)\n",
    "            correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "            dataset.append({\"prompt\": reasoning_module_prompt, \"output\": discussion, \"correct\": correct})\n",
    "            if correct == \"y\":\n",
    "                break\n",
    "            still_do_discussion = get_options(\"Still do discussion?\", [\"y\", \"n\"])\n",
    "            if still_do_discussion == \"n\":\n",
    "                discussion = \"\"\n",
    "                break\n",
    "    template += discussion\n",
    "            \n",
    "    while True:\n",
    "        reasoning_module_prompt = template.format(ptt=json.dumps(ptt), history=past_history, prompt=summary).strip()\n",
    "        output_ptt = reasoning_module(template, summary, past_history, ptt, llm, sampler,force_add_task=force_add_task, update_status=True, todo_tasks=[\"Obtain a secret file with a hash in it\"], max_spaces=max_spaces, delete_done=delete_done, choice_temperature=choice_temperature)\n",
    "        print(output_ptt)\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        dataset.append({\"prompt\": reasoning_module_prompt, \"output\": output_ptt, \"correct\": correct})\n",
    "        if correct == \"y\":\n",
    "            ptts.append(output_ptt)\n",
    "            return output_ptt\n",
    "        change_tasks = get_options(\"Do you want to change the task numbers?\", [\"y\", \"n\"])\n",
    "        if change_tasks == \"y\":\n",
    "            for key in list(ptt.keys()):\n",
    "                num_add_tasks =  int(input(f\"How many {key} tasks do you want to add? Enter an integer\"))\n",
    "                force_add_task[key] = num_add_tasks\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)\n",
    "        update_choice_temperature = get_options(\"Update choice temperature?\", [\"y\", \"n\"])\n",
    "        if update_choice_temperature == \"y\":\n",
    "            choice_temperature = float(input(\"Enter new temperature\"))\n",
    "def get_ports():\n",
    "    print(\"Enter the ports one by one\")\n",
    "    ports = []\n",
    "    while True:\n",
    "        try:\n",
    "            port =  int(input(\"Enter a port\"))\n",
    "        except:\n",
    "            do_exit = get_options(\"Exit?\", [\"y\", \"n\"])\n",
    "            if do_exit == \"y\":\n",
    "                return ports\n",
    "            continue\n",
    "        ports.append(port)\n",
    "        more_ports = get_options(\"Continue?\", [\"y\", \"n\"])\n",
    "        if more_ports == \"n\":\n",
    "            return ports\n",
    "def add_ports(ptt, ports2ptt):\n",
    "    ports = get_ports()\n",
    "    ptt_diff = {}\n",
    "    for key in ptt:\n",
    "        ptt_diff[key] = set()\n",
    "    for port in ports:\n",
    "        port_ptt = ports2ptt.get(port, {})\n",
    "        for key in port_ptt:\n",
    "            for task in port_ptt[key]:\n",
    "                ptt_diff[key].add(json.dumps(task))\n",
    "    for key in ptt_diff:\n",
    "        for task in ptt_diff[key]:\n",
    "            ptt[key].append(json.loads(task))\n",
    "    return ptt\n",
    "\n",
    "# the purpose of this function is to filter out tasks which are unrelated to our current problem\n",
    "def filter_tasks(template, past_history, ptt, llm, sampler, todo_tasks=[\"Obtain a secret file with a hash in it\"]):\n",
    "    prompt_ptt = get_current_status(ptt)\n",
    "    while True:\n",
    "        output = {}\n",
    "        for key in ptt:\n",
    "            output[key] = []\n",
    "            for task in ptt[key]:\n",
    "                if task[\"status\"] == \"todo\":\n",
    "                    task_set = False\n",
    "                    for todo_task in todo_tasks:\n",
    "                        if todo_task in task[\"task\"]:\n",
    "                            task_set = True\n",
    "                    if not task_set:\n",
    "                        prompt = template.format(history=past_history, task=task[\"task\"])\n",
    "                        generator = outlines.generate.choice(llm, [\"true\", \"false\"], sampler=sampler)\n",
    "                        answer = generator(prompt)\n",
    "                        if answer == \"false\":\n",
    "                            continue\n",
    "                output[key].append(task)\n",
    "        print(json.dumps(output, indent=4))\n",
    "        correct = get_options(\"Does this look correct?\", [\"y\", \"n\"])\n",
    "        if correct == \"y\":\n",
    "            return output\n",
    "        filter_further = get_options(\"Do you want to filter further?\", [\"y\", \"n\"])\n",
    "        if filter_further == \"y\":\n",
    "            ptt = copy.deepcopy(output)\n",
    "        update_temperature = get_options(\"Update temperature?\", [\"y\", \"n\"])\n",
    "        if update_temperature == \"y\":\n",
    "            temp = float(input(\"Enter new temperature\"))\n",
    "            sampler = multinomial(top_k=50, top_p=1.0, temperature=temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828413bb-fbaf-4ff0-bd26-ead40fec71b4",
   "metadata": {},
   "source": [
    "Issues\n",
    "1. Hallucinates that there is a ssh key\n",
    "2. As the summary goes on we get wrong conclusions/summary is doing misguided conclusions\n",
    "3. at temperature 1.2 we got an issues where nc -v -n -z -w1 -W1 -p80 -s 10.10.11.242 80 is an invalid command but when I removed -s it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2026c4-cfb1-42ab-a71c-32433fe6b625",
   "metadata": {},
   "source": [
    "Test if removing done tasks help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1891b-c967-420f-8bcc-71373cf0c4c6",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a9563f-ff41-4fae-9284-f2d5f6638b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial prompt to user\n",
    "ip_prompt = \"Please tell us the target IP address\"\n",
    "ip_address = \"10.10.11.242\"\n",
    "\n",
    "ports2ptt = {\n",
    "    22: {\n",
    "        \"Enumeration\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Enumerate SSH services\"}\n",
    "        ],\n",
    "        \"Vulnerability Scanning\" : [\n",
    "            {\"status\": \"todo\", \"task\": \"Scan SSH services with nmap scripts\"}\n",
    "        ],\n",
    "        \"Exploitation\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Exploit SSH vulnerabilities\"}\n",
    "        ],\n",
    "    },\n",
    "    80: {\n",
    "        \"Enumeration\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Enumerate web services\"}\n",
    "        ],\n",
    "        \"Vulnerability Scanning\" : [\n",
    "            {\"status\": \"todo\", \"task\": \"Scan web services with nikto and gobuster\"},\n",
    "        ],\n",
    "        \"Exploitation\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Exploit web vulnerabilities\"},\n",
    "        ],\n",
    "    },\n",
    "    139: {\n",
    "        \"Enumeration\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Enumerate SMB shares\"}\n",
    "        ],\n",
    "        \"Vulnerability Scanning\" : [\n",
    "            {\"status\": \"todo\", \"task\": \"Scan SMB shares with enum4linux or smbmap\"},\n",
    "        ],\n",
    "        \"Exploitation\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Exploit SMB vulnerabilities\"}\n",
    "        ],\n",
    "    },\n",
    "    111: {\n",
    "        \"Enumeration\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Enumerate NFS shares\"}\n",
    "        ],\n",
    "        \"Vulnerability Scanning\" : [\n",
    "            {\"status\": \"todo\", \"task\": \"Scan NFS shares with showmount\"}\n",
    "        ],\n",
    "        \"Exploitation\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Exploit NFS vulnerabilities\"}\n",
    "        ],\n",
    "    },\n",
    "    20: {\n",
    "        \"Enumeration\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Enumerate FTP shares\"}\n",
    "        ],\n",
    "        \"Vulnerability Scanning\" : [\n",
    "            {\"status\": \"todo\", \"task\": \"Scan FTP services with nmap scripts\"}\n",
    "        ],\n",
    "        \"Exploitation\": [\n",
    "            {\"status\": \"todo\", \"task\": \"Exploit FTP vulnerabilities\"}\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "ports2ptt[445] = ports2ptt[139]\n",
    "ports2ptt[443] = ports2ptt[80]\n",
    "ports2ptt[2049] = ports2ptt[111]\n",
    "ports2ptt[21] = ports2ptt[20]\n",
    "progress_save_path = 'progress.pickle'\n",
    "reset = False\n",
    "force_command = False\n",
    "delete_done = False\n",
    "generate_discussion = True\n",
    "only_provide_currrent_status = True\n",
    "past_history_set = False\n",
    "max_spaces = 2\n",
    "max_summaries = 2\n",
    "ask_model_for_add_task_num = False\n",
    "max_tokens = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7ffd08-8b90-4c3b-9431-7553abaaa786",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptt = {\n",
    "    \"Reconnaissance\": [\n",
    "        {\"status\": \"inprogress\", \"task\": f\"Perform nmap scan on {ip_address}\"},\n",
    "        {\"status\": \"todo\", \"task\": \"Identify nmap ports and services\"}        \n",
    "    ],\n",
    "    \"Enumeration\": [],\n",
    "    \"Vulnerability Scanning\" : [],\n",
    "    \"Exploitation\": [],\n",
    "    \"Privilege Escalation\": [],\n",
    "    \"Post Exploitation\": [{\"status\": \"todo\", \"task\": \"Obtain a secret file with a hash in it\"}]\n",
    "}\n",
    "if os.path.exists(progress_save_path) and not reset:\n",
    "    with open(progress_save_path, 'rb') as handle:\n",
    "        progress = pickle.load(handle)\n",
    "    summaries = progress[\"summaries\"]\n",
    "    all_instructions = progress[\"all_instructions\"]\n",
    "    ptts = progress[\"ptts\"]\n",
    "    all_command_outputs = progress[\"all_command_outputs\"]\n",
    "    current_history = \"\"\n",
    "    if \"current_history\" in progress:\n",
    "        current_history = progress[\"current_history\"]\n",
    "    if len(summaries) > 0:\n",
    "        summary = summaries[-1]\n",
    "    if len(ptts) > 0:\n",
    "        ptt = ptts[-1]\n",
    "    setup = progress[\"setup\"]\n",
    "    dataset = progress[\"dataset\"]\n",
    "else:\n",
    "    summaries = []\n",
    "    all_instructions = []\n",
    "    ptts = []\n",
    "    all_command_outputs = []\n",
    "    dataset = []\n",
    "    current_history = \"\"\n",
    "    setup = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e020ecf-c585-4505-aba4-03c5cf10cc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 3, 4, 65)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries), len(all_instructions), len(ptts), len(all_command_outputs), len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8af691-3f4e-4211-a523-25ecca5426df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current ptt:  {'Reconnaissance': [{'status': 'done', 'task': 'Perform nmap scan on 10.10.11.242'}, {'status': 'done', 'task': 'Identify nmap ports and services'}], 'Enumeration': [{'status': 'done', 'task': 'Enumerate web services'}, {'status': 'done', 'task': 'Enumerate SSH services'}], 'Vulnerability Scanning': [{'status': 'todo', 'task': 'Scan SSH services with nmap scripts'}, {'status': 'done', 'task': 'Scan web services with nikto and gobuster'}, {'status': 'todo', 'task': 'Exploit SSH vulnerability CVE-2021-25745'}], 'Exploitation': [{'status': 'todo', 'task': 'Exploit web vulnerabilities'}, {'status': 'todo', 'task': 'Exploit SSH vulnerabilities'}], 'Privilege Escalation': [{'status': 'todo', 'task': 'Identify misconfigurations and writable files for privilege escalation'}], 'Post Exploitation': [{'status': 'todo', 'task': 'Obtain a secret file with a hash in it'}]}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Force set current task to done? Answer with y/n y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.64 ms /     8 runs   (    0.08 ms per token, 12539.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14332.64 ms /  1348 tokens (   10.63 ms per token,    94.05 tokens per second)\n",
      "llama_print_timings:        eval time =     519.00 ms /     7 runs   (   74.14 ms per token,    13.49 tokens per second)\n",
      "llama_print_timings:       total time =   14877.35 ms /  1355 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inprogress task is Scan SSH services with nmap scripts\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Does this look correct? Answer with y/n y\n",
      "How many Reconnaissance tasks do you want to add? Enter an integer:  0\n",
      "How many Enumeration tasks do you want to add? Enter an integer:  0\n",
      "How many Vulnerability Scanning tasks do you want to add? Enter an integer:  0\n",
      "How many Exploitation tasks do you want to add? Enter an integer:  0\n",
      "How many Privilege Escalation tasks do you want to add? Enter an integer:  0\n",
      "How many Post Exploitation tasks do you want to add? Enter an integer:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =      28.41 ms /   374 runs   (    0.08 ms per token, 13164.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13836.99 ms /  1360 tokens (   10.17 ms per token,    98.29 tokens per second)\n",
      "llama_print_timings:        eval time =   27660.45 ms /   373 runs   (   74.16 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:       total time =   42154.19 ms /  1733 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated discussion:   The next inprogress task is \"Scan SSH services with nmap scripts\". The nmap scan for SSH services will be performed using various scripting engines available in nmap, which can provide a detailed report on SSH versions, misconfigurations, and known vulnerabilities.\n",
      "\n",
      "The SSH service on port 22 is running OpenSSH 8.2p1 which has been identified as one of the most secure versions of SSH available. It has several vulnerabilities that can be checked using specific nmap scripts or by updating nmap and using more aggressive scripts (-sV -A). It's important to note that updating nmap or using more aggressive scripts can potentially harm legitimate services if performed on an unauthorized network without consent.\n",
      "\n",
      "The SSH service on port 22 does not have any obvious misconfigurations based on the limited information provided by nmap's version detection. However, it is possible that additional scans or manual inspection of configuration files could reveal misconfigurations.\n",
      "\n",
      "To continue with the Penetration Testing Tree (PTT), we would now proceed with the \"Exploit SSH vulnerability CVE-2021-25745\" task, as it has been marked as \"todo\". This task would involve using information about CVE-2021-25745 to craft an exploit against the OpenSSH service running on port 22, which could lead to remote code execution if successful. The exploit would be executed with caution and only against systems that have been authorized for penetration testing.\n",
      "\n",
      "Here is how we would update the PTT after the completion of \"Scan SSH services with nmap scripts\":\n",
      "```python\n",
      "PTT =\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Does this look correct? Answer with y/n y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['todo', 'done', 'inprogress', 'remove']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.21 ms /     3 runs   (    0.07 ms per token, 14423.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20745.44 ms /  1828 tokens (   11.35 ms per token,    88.12 tokens per second)\n",
      "llama_print_timings:        eval time =     159.72 ms /     2 runs   (   79.86 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   20923.28 ms /  1830 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.41 ms /     3 runs   (    0.14 ms per token,  7407.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19365.65 ms /  1881 tokens (   10.30 ms per token,    97.13 tokens per second)\n",
      "llama_print_timings:        eval time =     154.04 ms /     2 runs   (   77.02 ms per token,    12.98 tokens per second)\n",
      "llama_print_timings:       total time =   19537.57 ms /  1883 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.21 ms /     3 runs   (    0.07 ms per token, 14150.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19403.83 ms /  1906 tokens (   10.18 ms per token,    98.23 tokens per second)\n",
      "llama_print_timings:        eval time =     156.83 ms /     2 runs   (   78.41 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:       total time =   19566.56 ms /  1908 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.22 ms /     3 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21997.79 ms /  1924 tokens (   11.43 ms per token,    87.46 tokens per second)\n",
      "llama_print_timings:        eval time =     192.92 ms /     2 runs   (   96.46 ms per token,    10.37 tokens per second)\n",
      "llama_print_timings:       total time =   22196.69 ms /  1926 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.21 ms /     3 runs   (    0.07 ms per token, 14492.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25305.51 ms /  1963 tokens (   12.89 ms per token,    77.57 tokens per second)\n",
      "llama_print_timings:        eval time =     210.03 ms /     2 runs   (  105.02 ms per token,     9.52 tokens per second)\n",
      "llama_print_timings:       total time =   25521.54 ms /  1965 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating done from \n",
      "{'Reconnaissance': [{'task': 'Perform nmap scan on 10.10.11.242', 'status': 'done'}, {'task': 'Identify nmap ports and services', 'status': 'done'}], 'Enumeration': [{'task': 'Enumerate web services', 'status': 'done'}, {'task': 'Enumerate SSH services', 'status': 'done'}], 'Vulnerability Scanning': [{'task': 'Scan SSH services with nmap scripts', 'status': 'inprogress'}, {'task': 'Scan web services with nikto and gobuster', 'status': 'done'}, {'task': 'Exploit SSH vulnerability CVE-2021-25745', 'status': 'todo'}], 'Exploitation': [{'task': 'Exploit web vulnerabilities', 'status': 'todo'}, {'task': 'Exploit SSH vulnerabilities', 'status': 'todo'}], 'Privilege Escalation': [{'task': 'Identify misconfigurations and writable files for privilege escalation', 'status': 'todo'}], 'Post Exploitation': [{'task': 'Obtain a secret file with a hash in it', 'status': 'todo'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     2 runs   (    0.07 ms per token, 13422.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24380.83 ms /  1743 tokens (   13.99 ms per token,    71.49 tokens per second)\n",
      "llama_print_timings:        eval time =     116.10 ms /     1 runs   (  116.10 ms per token,     8.61 tokens per second)\n",
      "llama_print_timings:       total time =   24501.77 ms /  1744 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     2 runs   (    0.07 ms per token, 13888.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26818.91 ms /  1761 tokens (   15.23 ms per token,    65.66 tokens per second)\n",
      "llama_print_timings:        eval time =     120.18 ms /     1 runs   (  120.18 ms per token,     8.32 tokens per second)\n",
      "llama_print_timings:       total time =   26943.43 ms /  1762 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     2 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29621.99 ms /  1784 tokens (   16.60 ms per token,    60.23 tokens per second)\n",
      "llama_print_timings:        eval time =     131.81 ms /     1 runs   (  131.81 ms per token,     7.59 tokens per second)\n",
      "llama_print_timings:       total time =   29758.72 ms /  1785 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     2 runs   (    0.07 ms per token, 13422.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32549.30 ms /  1800 tokens (   18.08 ms per token,    55.30 tokens per second)\n",
      "llama_print_timings:        eval time =     141.80 ms /     1 runs   (  141.80 ms per token,     7.05 tokens per second)\n",
      "llama_print_timings:       total time =   32694.98 ms /  1801 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     2 runs   (    0.08 ms per token, 13157.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   34441.62 ms /  1851 tokens (   18.61 ms per token,    53.74 tokens per second)\n",
      "llama_print_timings:        eval time =     144.21 ms /     1 runs   (  144.21 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:       total time =   34590.60 ms /  1852 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated completion status\n",
      "{'Reconnaissance': [{'task': 'Perform nmap scan on 10.10.11.242', 'status': 'done'}, {'task': 'Identify nmap ports and services', 'status': 'done'}], 'Enumeration': [{'task': 'Enumerate web services', 'status': 'done'}, {'task': 'Enumerate SSH services', 'status': 'done'}], 'Vulnerability Scanning': [{'task': 'Scan SSH services with nmap scripts', 'status': 'inprogress'}, {'task': 'Scan web services with nikto and gobuster', 'status': 'done'}, {'task': 'Exploit SSH vulnerability CVE-2021-25745', 'status': 'todo'}], 'Exploitation': [{'task': 'Exploit web vulnerabilities', 'status': 'todo'}, {'task': 'Exploit SSH vulnerabilities', 'status': 'todo'}], 'Privilege Escalation': [{'task': 'Identify misconfigurations and writable files for privilege escalation', 'status': 'todo'}], 'Post Exploitation': [{'task': 'Obtain a secret file with a hash in it', 'status': 'todo'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     2 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33053.83 ms /  1708 tokens (   19.35 ms per token,    51.67 tokens per second)\n",
      "llama_print_timings:        eval time =      78.63 ms /     1 runs   (   78.63 ms per token,    12.72 tokens per second)\n",
      "llama_print_timings:       total time =   33136.74 ms /  1709 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     2 runs   (    0.08 ms per token, 13245.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35254.46 ms /  1711 tokens (   20.60 ms per token,    48.53 tokens per second)\n",
      "llama_print_timings:        eval time =     177.06 ms /     1 runs   (  177.06 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:       total time =   35435.72 ms /  1712 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.18 ms /     2 runs   (    0.09 ms per token, 11363.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36059.24 ms /  1720 tokens (   20.96 ms per token,    47.70 tokens per second)\n",
      "llama_print_timings:        eval time =     154.34 ms /     1 runs   (  154.34 ms per token,     6.48 tokens per second)\n",
      "llama_print_timings:       total time =   36218.61 ms /  1721 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     2 runs   (    0.07 ms per token, 14492.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   37161.40 ms /  1768 tokens (   21.02 ms per token,    47.58 tokens per second)\n",
      "llama_print_timings:        eval time =     159.40 ms /     1 runs   (  159.40 ms per token,     6.27 tokens per second)\n",
      "llama_print_timings:       total time =   37324.33 ms /  1769 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     2 runs   (    0.07 ms per token, 13793.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   37718.88 ms /  1769 tokens (   21.32 ms per token,    46.90 tokens per second)\n",
      "llama_print_timings:        eval time =     162.09 ms /     1 runs   (  162.09 ms per token,     6.17 tokens per second)\n",
      "llama_print_timings:       total time =   37885.48 ms /  1770 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     2 runs   (    0.07 ms per token, 13422.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38952.26 ms /  1777 tokens (   21.92 ms per token,    45.62 tokens per second)\n",
      "llama_print_timings:        eval time =     169.35 ms /     1 runs   (  169.35 ms per token,     5.90 tokens per second)\n",
      "llama_print_timings:       total time =   39126.96 ms /  1778 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     2 runs   (    0.07 ms per token, 13422.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39072.38 ms /  1809 tokens (   21.60 ms per token,    46.30 tokens per second)\n",
      "llama_print_timings:        eval time =     163.41 ms /     1 runs   (  163.41 ms per token,     6.12 tokens per second)\n",
      "llama_print_timings:       total time =   39240.24 ms /  1810 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.15 ms /     2 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35487.97 ms /  1810 tokens (   19.61 ms per token,    51.00 tokens per second)\n",
      "llama_print_timings:        eval time =     149.27 ms /     1 runs   (  149.27 ms per token,     6.70 tokens per second)\n",
      "llama_print_timings:       total time =   35642.73 ms /  1811 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     2 runs   (    0.07 ms per token, 13793.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36035.15 ms /  1821 tokens (   19.79 ms per token,    50.53 tokens per second)\n",
      "llama_print_timings:        eval time =     155.36 ms /     1 runs   (  155.36 ms per token,     6.44 tokens per second)\n",
      "llama_print_timings:       total time =   36194.75 ms /  1822 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     2 runs   (    0.07 ms per token, 13888.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40099.72 ms /  1892 tokens (   21.19 ms per token,    47.18 tokens per second)\n",
      "llama_print_timings:        eval time =     171.60 ms /     1 runs   (  171.60 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:       total time =   40275.33 ms /  1893 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.16 ms /     2 runs   (    0.08 ms per token, 12500.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42661.76 ms /  1893 tokens (   22.54 ms per token,    44.37 tokens per second)\n",
      "llama_print_timings:        eval time =     173.70 ms /     1 runs   (  173.70 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:       total time =   42840.80 ms /  1894 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     2 runs   (    0.07 ms per token, 13986.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45340.30 ms /  1901 tokens (   23.85 ms per token,    41.93 tokens per second)\n",
      "llama_print_timings:        eval time =     180.42 ms /     1 runs   (  180.42 ms per token,     5.54 tokens per second)\n",
      "llama_print_timings:       total time =   45526.75 ms /  1902 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.17 ms /     2 runs   (    0.09 ms per token, 11764.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46889.64 ms /  1937 tokens (   24.21 ms per token,    41.31 tokens per second)\n",
      "llama_print_timings:        eval time =     193.26 ms /     1 runs   (  193.26 ms per token,     5.17 tokens per second)\n",
      "llama_print_timings:       total time =   47087.54 ms /  1938 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.16 ms /     2 runs   (    0.08 ms per token, 12820.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47168.51 ms /  1938 tokens (   24.34 ms per token,    41.09 tokens per second)\n",
      "llama_print_timings:        eval time =     190.24 ms /     1 runs   (  190.24 ms per token,     5.26 tokens per second)\n",
      "llama_print_timings:       total time =   47363.75 ms /  1939 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.42 ms /     2 runs   (    0.21 ms per token,  4773.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48467.16 ms /  1950 tokens (   24.85 ms per token,    40.23 tokens per second)\n",
      "llama_print_timings:        eval time =     108.83 ms /     1 runs   (  108.83 ms per token,     9.19 tokens per second)\n",
      "llama_print_timings:       total time =   48586.62 ms /  1951 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.19 ms /     2 runs   (    0.09 ms per token, 10638.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47299.19 ms /  1979 tokens (   23.90 ms per token,    41.84 tokens per second)\n",
      "llama_print_timings:        eval time =     202.90 ms /     1 runs   (  202.90 ms per token,     4.93 tokens per second)\n",
      "llama_print_timings:       total time =   47512.32 ms /  1980 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.16 ms /     2 runs   (    0.08 ms per token, 12422.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47188.26 ms /  1980 tokens (   23.83 ms per token,    41.96 tokens per second)\n",
      "llama_print_timings:        eval time =     173.98 ms /     1 runs   (  173.98 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:       total time =   47366.61 ms /  1981 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.14 ms /     2 runs   (    0.07 ms per token, 14285.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48296.85 ms /  1989 tokens (   24.28 ms per token,    41.18 tokens per second)\n",
      "llama_print_timings:        eval time =     184.06 ms /     1 runs   (  184.06 ms per token,     5.43 tokens per second)\n",
      "llama_print_timings:       total time =   48485.26 ms /  1990 tokens\n",
      "\n",
      "llama_print_timings:        load time =    7181.98 ms\n",
      "llama_print_timings:      sample time =       0.16 ms /     2 runs   (    0.08 ms per token, 12195.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51483.82 ms /  2012 tokens (   25.59 ms per token,    39.08 tokens per second)\n",
      "llama_print_timings:        eval time =      87.43 ms /     1 runs   (   87.43 ms per token,    11.44 tokens per second)\n",
      "llama_print_timings:       total time =   51579.08 ms /  2013 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new tasks\n",
      "[]\n",
      "Number of inprogress tasks:  1\n",
      "{'Reconnaissance': [{'status': 'done', 'task': 'Perform nmap scan on 10.10.11.242'}, {'status': 'done', 'task': 'Identify nmap ports and services'}], 'Enumeration': [{'status': 'done', 'task': 'Enumerate web services'}, {'status': 'done', 'task': 'Enumerate SSH services'}], 'Vulnerability Scanning': [{'status': 'inprogress', 'task': 'Scan SSH services with nmap scripts'}, {'status': 'done', 'task': 'Scan web services with nikto and gobuster'}, {'status': 'todo', 'task': 'Exploit SSH vulnerability CVE-2021-25745'}], 'Exploitation': [{'status': 'todo', 'task': 'Exploit web vulnerabilities'}, {'status': 'todo', 'task': 'Exploit SSH vulnerabilities'}], 'Privilege Escalation': [{'status': 'todo', 'task': 'Identify misconfigurations and writable files for privilege escalation'}], 'Post Exploitation': [{'status': 'todo', 'task': 'Obtain a secret file with a hash in it'}]}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        if len(all_instructions) == len(summaries) and len(all_instructions) == len(ptts):\n",
    "            current_history = summarize_summaries(input_parsing_past_summaries_template, llm, summary_sampler, summaries, max_summaries=max_summaries, max_tokens=max_tokens, full=True, dataset=dataset)\n",
    "            print(\"Getting instruction\")\n",
    "            get_instructions(generative_prompt_template, ptt, llm, generative_sampler, all_instructions, current_history=current_history, force_command=force_command, dataset=dataset)\n",
    "            do_qa(default_qa_template, llm, qa_sampler, force_command=False, dataset=dataset)\n",
    "        if len(summaries) == len(ptts):\n",
    "            summary = get_summary(input_parsing_templates, ptt, llm, summary_sampler, summaries, all_command_outputs, dataset=dataset)\n",
    "        if setup:\n",
    "            ptt = add_ports(ptt, ports2ptt)\n",
    "            print(\"Updated ptt to \", json.dumps(ptt, indent=4))\n",
    "            setup = False\n",
    "        if not past_history_set:\n",
    "            if len(current_history) > 0:\n",
    "                past_history = summarize_summaries(input_parsing_past_summaries_template, llm, summary_sampler, summaries, max_summaries=max_summaries, max_tokens=300, dataset=dataset)\n",
    "            else:\n",
    "                past_history = current_history\n",
    "            past_history_set = True\n",
    "        print(\"Current ptt: \", ptt)\n",
    "        \n",
    "        ptt = get_new_ptt(reasoning_template, summary, past_history, ptt, llm, reasoning_sampler, ptts, max_spaces=max_spaces, delete_done=delete_done, generate_discussion=generate_discussion, ask_model_for_add_task_num=ask_model_for_add_task_num, dataset=dataset)\n",
    "        past_history_set = False\n",
    "        progress = {\n",
    "            \"summaries\": summaries,\n",
    "            \"all_instructions\": all_instructions,\n",
    "            \"ptts\": ptts,\n",
    "            \"current_history\": current_history,\n",
    "            \"all_command_outputs\": all_command_outputs,\n",
    "            \"dataset\": dataset,\n",
    "            \"setup\": setup,\n",
    "        }\n",
    "        with open(progress_save_path, 'wb') as handle:\n",
    "            pickle.dump(progress, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)\n",
    "    print(traceback.format_exc())\n",
    "    progress = {\n",
    "        \"summaries\": summaries,\n",
    "        \"all_instructions\": all_instructions,\n",
    "        \"ptts\": ptts,\n",
    "        \"current_history\": current_history,\n",
    "        \"all_command_outputs\": all_command_outputs,\n",
    "        \"dataset\": dataset,\n",
    "        \"setup\": setup,\n",
    "    }\n",
    "    with open(progress_save_path, 'wb') as handle:\n",
    "        pickle.dump(progress, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43aa7616-f3a6-405e-b61a-55b5a69ff0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e22520-77e1-4e55-84b4-c76e7ebcce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Don't force command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f70cba-9bae-47ab-aa90-c80e6af7aefe",
   "metadata": {},
   "source": [
    "Issues, \n",
    "1. when just adding one command, in devvortex, can get stuck on ssh checking\n",
    "2. For gobuster issued the following command gobuster -u http://devvortex.htb/ -w directory-list-2.3-medium.txt -x txt which failed to run. This was fixed upon asking the qa both the command + the error message and what was wrong\n",
    "3. Getting a good wordlist for gobuster is hard\n",
    "4. Sometimes it repeats a task that was given in a summary\n",
    "5. It's hard to get model to go to gobuster in the first place\n",
    "6. Keeps setting same task as inprogress forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1efda45d-654e-4cbb-9a50-a84a01362f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = {\n",
    "    \"summaries\": summaries,\n",
    "    \"all_instructions\": all_instructions,\n",
    "    \"ptts\": ptts,\n",
    "    \"all_command_outputs\": all_command_outputs,\n",
    "    \"current_history\": current_history,\n",
    "    \"dataset\": dataset,\n",
    "    \"setup\": setup,\n",
    "}\n",
    "with open(progress_save_path, 'wb') as handle:\n",
    "    pickle.dump(progress, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c36ebc5-35f1-485a-b2ea-4935cbe9f9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be156fed-4121-4af3-98dc-59419dce3925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5f1a0-fed1-440a-961f-3016adae0dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
