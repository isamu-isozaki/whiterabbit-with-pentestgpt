{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e33ae58-2330-49d2-96e4-7d906536ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bad156-f443-44ee-bbf7-3f8f7c27d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert task to adding new todo tasks+changing status\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from lmformatenforcer import CharacterLevelParser, JsonSchemaParser\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from prompts import prompts\n",
    "from schema import llamaindex_llamacpp_lm_format_enforcer, PTT, update_completion_status, add_new_items, update_completion_status_outlines, add_new_items_outlines, add_new_items_outlines_no_json\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "import copy\n",
    "import gc\n",
    "import outlines\n",
    "from torch import Generator\n",
    "from outlines.samplers import Sampler, multinomial\n",
    "import json\n",
    "from benchmark import load_outlines\n",
    "import json\n",
    "model_paths=[\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q3_K_S.gguf\",\n",
    "    # \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-33b-v1.Q4_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q5_K_S.gguf\",\n",
    "]\n",
    "output_path = \"./benchmark\"\n",
    "\n",
    "instance = {\n",
    "    \"n_gpu_layers\": 15,\n",
    "    \"n_batch\": 1536,\n",
    "    \"top_p\": 1.0,\n",
    "    \"temperature\": 0.5,\n",
    "    \"generate_len\": 1536,\n",
    "    \"top_k\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f532adc-344d-48e1-aa49-7e9be762e9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1060 with Max-Q Design, compute capability 6.1, VMM: yes\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from /mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = whiterabbitneo_whiterabbitneo-13b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  273 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.90 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = whiterabbitneo_whiterabbitneo-13b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "llm_load_tensors: offloading 15 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 15/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7070.25 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2552.93 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1536\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   750.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   450.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1200.00 MiB, K (f16):  600.00 MiB, V (f16):  600.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    48.02 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   489.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   480.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'whiterabbitneo_whiterabbitneo-13b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '14'}\n"
     ]
    }
   ],
   "source": [
    "llm, sampler = load_outlines(model_paths[1], instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acee435c-29e4-4eb0-ba62-e8080645a07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1138aefc70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = Generator(device=\"cpu\")\n",
    "rng.manual_seed(789005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ccbfe3-121e-4174-894b-2c98d7b2dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = prompts[\"reasoning_module\"][\"reasoning_session_task_selection_hack_no_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acfe74ab-8f56-46f4-addd-97d2b28bdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"On 142.250.80.46 there is one ssh port open on port 22, a http port on port 80, and a https port on port 443\"\n",
    "ptt = \"\"\"\n",
    "{\"recon\": [\n",
    "        {\"task_description\": \"Perform a full port scan\", \"status\": \"inprogress\"},\n",
    "        {\"task_description\": \"Determine the purpose of each open port\", \"status\": \"todo\"}\n",
    "    ],\n",
    "    \"initial_access\": [],\n",
    "    \"execution\": [],\n",
    "    \"post_exploitation\": []\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e6b3d3-b203-4b22-a9ec-eaf47cca5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_prompt = template.format(ptt=ptt, prompt=prompt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ca25e0-d93c-4ee2-ac3d-a191090edce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SYSTEM:\\nYou\\'re an excellent cybersecurity penetration tester assistant. \\nYou need to help the tester in a cybersecurity training process, and your commitment is essential to the task.\\nAnswer all the potential vulnerabilities determined in a logical, step-by-step manner that makes the reasoning process clear. You are given a Python dictionary, namely \"Penetration Testing Tree (PTT)\", and user input.\\nFirst, carefully analyze the PTT. For the PTT,\\n(1) The tasks are in layered dictionary structure.\\n(2) Each task has a completion status: inprogress, todo, done, or n/a.\\n(3) You are given one specific sub-task labeled as todo.\\nThe user input is information obtained from doing the task marked as in progress. If the task is completed, change completion task to done.\\nExtensively evaluate each task marked as todo, if a task is done update status to done. These tasks marked as todo will be called todo tasks.\\nNext, break down the todo tasks into sub-components and constraints that need to be addressed. This allows tackling the problem in a structured way. \\nFor each sub-component, leverage the knowledge and inference skills to generate multiple hypotheses or possibilities that could lead to a penetration.\\nCritically evaluate each hypothesis based on validity, relevance to the current completion status, and how well it addresses the sub-component when logically combined with other steps.\\nUsing this critical analysis, deliberate over the most coherent combination and sequence of hypothesis steps to craft a logical reasoning chain.\\nThroughout, aim to provide explanatory details on why certain options were considered more or less ideal to make the thought process transparent.\\nIf it was determined that there is a gap in the reasoning chain, backtrack and explore alternative hypotheses to plug the gap until there is a complete logical flow.\\nNext, synthesize the key insights from the reasoning chain into further expansion of the tasks in the PTT while choosing exactly one task to mark as in progress. The expansion\\nshould not destroy any information and modifications should be minimal.\\nFinally, remove redundant/outdated tasks from the task list to keep tasks clear, precise and short due to token size limit.\\n\\nIn summary, leverage a structured, critical thinking process with iterative refinement to modify the PTT in the face of the user input to evaluate future strategies and to choose a new task as \\ninprogress with minimal modifications.\\nAnswer with code examples, or fully functioning code. Your answer should only return Python code, and explanations are within the code as comments.\\nYou MUST answer from modifying this json: \\n{\"recon\": [\\n        {\"task_description\": \"Perform a full port scan\", \"status\": \"inprogress\"},\\n        {\"task_description\": \"Determine the purpose of each open port\", \"status\": \"todo\"}\\n    ],\\n    \"initial_access\": [],\\n    \"execution\": [],\\n    \"post_exploitation\": []\\n}\\n\\nUSER: On 142.250.80.46 there is one ssh port open on port 22, a http port on port 80, and a https port on port 443\\nASSISTANT:'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12296e2-b59a-482d-a778-bb96a4dc629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       1.84 ms /     2 runs   (    0.92 ms per token,  1085.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  265369.41 ms /   746 tokens (  355.72 ms per token,     2.81 tokens per second)\n",
      "llama_print_timings:        eval time =    5265.42 ms /     1 runs   ( 5265.42 ms per token,     0.19 tokens per second)\n",
      "llama_print_timings:       total time =  270735.72 ms /   747 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       1.28 ms /     3 runs   (    0.43 ms per token,  2352.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19840.70 ms /   770 tokens (   25.77 ms per token,    38.81 tokens per second)\n",
      "llama_print_timings:        eval time =     654.43 ms /     2 runs   (  327.22 ms per token,     3.06 tokens per second)\n",
      "llama_print_timings:       total time =   20503.81 ms /   772 tokens\n"
     ]
    }
   ],
   "source": [
    "output, inprogress_set = update_completion_status_outlines(llm, llm_prompt, ptt, sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f737932-62ff-490a-bfda-68861c09b6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"recon\": [\\n        {\"task_description\": \"Perform a full port scan\", \"status\": \"done\"},\\n        {\"task_description\": \"Determine the purpose of each open port\", \"status\": \"inprogress\"}\\n    ],\\n    \"initial_access\": [],\\n    \"execution\": [],\\n    \"post_exploitation\": []\\n}'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc252419-5e8d-4de0-b9bd-c54f3f7a27ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inprogress_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6f3aa74-2758-45aa-a486-c1973b380962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     2 runs   (    0.44 ms per token,  2259.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16930.50 ms /   721 tokens (   23.48 ms per token,    42.59 tokens per second)\n",
      "llama_print_timings:        eval time =     321.70 ms /     1 runs   (  321.70 ms per token,     3.11 tokens per second)\n",
      "llama_print_timings:       total time =   17261.62 ms /   722 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.82 ms /     2 runs   (    0.41 ms per token,  2450.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16910.49 ms /   723 tokens (   23.39 ms per token,    42.75 tokens per second)\n",
      "llama_print_timings:        eval time =     319.38 ms /     1 runs   (  319.38 ms per token,     3.13 tokens per second)\n",
      "llama_print_timings:       total time =   17235.16 ms /   724 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.82 ms /     2 runs   (    0.41 ms per token,  2442.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16960.01 ms /   730 tokens (   23.23 ms per token,    43.04 tokens per second)\n",
      "llama_print_timings:        eval time =     326.26 ms /     1 runs   (  326.26 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =   17291.92 ms /   731 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.82 ms /     2 runs   (    0.41 ms per token,  2433.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18349.67 ms /   772 tokens (   23.77 ms per token,    42.07 tokens per second)\n",
      "llama_print_timings:        eval time =     322.80 ms /     1 runs   (  322.80 ms per token,     3.10 tokens per second)\n",
      "llama_print_timings:       total time =   18677.75 ms /   773 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$defs\": {\"OnlyTODO\": {\"const\": \"todo\", \"title\": \"OnlyTODO\", \"type\": \"string\"}}, \"properties\": {\"status\": {\"$ref\": \"#/$defs/OnlyTODO\"}, \"task_description\": {\"maxLength\": 100, \"title\": \"Task Description\", \"type\": \"string\"}}, \"required\": [\"status\", \"task_description\"], \"title\": \"BaseTaskOnlyTODO\", \"type\": \"object\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =      26.46 ms /    62 runs   (    0.43 ms per token,  2343.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18491.35 ms /   774 tokens (   23.89 ms per token,    41.86 tokens per second)\n",
      "llama_print_timings:        eval time =   20188.03 ms /    61 runs   (  330.95 ms per token,     3.02 tokens per second)\n",
      "llama_print_timings:       total time =   38856.27 ms /   835 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.90 ms /     2 runs   (    0.45 ms per token,  2224.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18598.69 ms /   798 tokens (   23.31 ms per token,    42.91 tokens per second)\n",
      "llama_print_timings:        eval time =     329.44 ms /     1 runs   (  329.44 ms per token,     3.04 tokens per second)\n",
      "llama_print_timings:       total time =   18934.11 ms /   799 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.85 ms /     2 runs   (    0.42 ms per token,  2364.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18621.45 ms /   799 tokens (   23.31 ms per token,    42.91 tokens per second)\n",
      "llama_print_timings:        eval time =     330.86 ms /     1 runs   (  330.86 ms per token,     3.02 tokens per second)\n",
      "llama_print_timings:       total time =   18957.72 ms /   800 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.82 ms /     2 runs   (    0.41 ms per token,  2444.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18894.19 ms /   807 tokens (   23.41 ms per token,    42.71 tokens per second)\n",
      "llama_print_timings:        eval time =     325.35 ms /     1 runs   (  325.35 ms per token,     3.07 tokens per second)\n",
      "llama_print_timings:       total time =   19225.16 ms /   808 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.82 ms /     2 runs   (    0.41 ms per token,  2453.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18802.92 ms /   809 tokens (   23.24 ms per token,    43.03 tokens per second)\n",
      "llama_print_timings:        eval time =     326.77 ms /     1 runs   (  326.77 ms per token,     3.06 tokens per second)\n",
      "llama_print_timings:       total time =   19134.75 ms /   810 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"$defs\": {\"OnlyTODO\": {\"const\": \"todo\", \"title\": \"OnlyTODO\", \"type\": \"string\"}}, \"properties\": {\"status\": {\"$ref\": \"#/$defs/OnlyTODO\"}, \"task_description\": {\"maxLength\": 100, \"title\": \"Task Description\", \"type\": \"string\"}}, \"required\": [\"status\", \"task_description\"], \"title\": \"BaseTaskOnlyTODO\", \"type\": \"object\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =      26.97 ms /    63 runs   (    0.43 ms per token,  2336.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18841.69 ms /   811 tokens (   23.23 ms per token,    43.04 tokens per second)\n",
      "llama_print_timings:        eval time =   20424.58 ms /    62 runs   (  329.43 ms per token,     3.04 tokens per second)\n",
      "llama_print_timings:       total time =   39452.85 ms /   873 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.80 ms /     2 runs   (    0.40 ms per token,  2487.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19863.49 ms /   836 tokens (   23.76 ms per token,    42.09 tokens per second)\n",
      "llama_print_timings:        eval time =     328.62 ms /     1 runs   (  328.62 ms per token,     3.04 tokens per second)\n",
      "llama_print_timings:       total time =   20197.81 ms /   837 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.81 ms /     2 runs   (    0.40 ms per token,  2475.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19905.71 ms /   837 tokens (   23.78 ms per token,    42.05 tokens per second)\n",
      "llama_print_timings:        eval time =     333.26 ms /     1 runs   (  333.26 ms per token,     3.00 tokens per second)\n",
      "llama_print_timings:       total time =   20244.68 ms /   838 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.82 ms /     2 runs   (    0.41 ms per token,  2430.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19944.49 ms /   843 tokens (   23.66 ms per token,    42.27 tokens per second)\n",
      "llama_print_timings:        eval time =     326.59 ms /     1 runs   (  326.59 ms per token,     3.06 tokens per second)\n",
      "llama_print_timings:       total time =   20276.20 ms /   844 tokens\n",
      "\n",
      "llama_print_timings:        load time =  265370.02 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     2 runs   (    0.44 ms per token,  2254.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20102.87 ms /   854 tokens (   23.54 ms per token,    42.48 tokens per second)\n",
      "llama_print_timings:        eval time =     331.04 ms /     1 runs   (  331.04 ms per token,     3.02 tokens per second)\n",
      "llama_print_timings:       total time =   20439.68 ms /   855 tokens\n"
     ]
    }
   ],
   "source": [
    "final_output = add_new_items_outlines(llm, llm_prompt, output, sampler, inprogress_set, max_spaces=2, force_add_task={\"recon\": True, \"initial_access\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150ab7e-4ecf-446c-9dd5-fc844a4790a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2f9640f-59b6-469d-8caa-585867eee8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  {\\n   \"recon\": [\\n       {\"task_description\":\"Perform a full port scan\", \"status\": \"done\"},{\"task_description\":\"Determine the purpose of each open port\", \"status\": \"inprogress\"},\\n      {\\'status\\': \\'todo\\', \\'task_description\\': \\'Assess the web application on port 80\\'}],\\n  \"initial_access\": [\\n     \\n  {\\'status\\': \\'todo\\', \\'task_description\\': \\'Gain initial access using ssh on port 22\\'}],\\n  \"execution\": [\\n     ],\"post_exploitation\": [\\n    ]}'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16de1515-2e14-4565-96a6-cc713c174451",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(final_output.replace(\"'\", '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a296e4e3-057c-47cc-b916-4a1ff24ee7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recon': [{'task_description': 'Perform a full port scan', 'status': 'done'},\n",
       "  {'task_description': 'Determine the purpose of each open port',\n",
       "   'status': 'inprogress'},\n",
       "  {'status': 'todo',\n",
       "   'task_description': 'Assess the web application on port 80'}],\n",
       " 'initial_access': [{'status': 'todo',\n",
       "   'task_description': 'Gain initial access using ssh on port 22'}],\n",
       " 'execution': [],\n",
       " 'post_exploitation': []}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d93496-6580-4625-9cf4-3ca74909282d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7ad0a-cd58-4b12-843e-0c6f7a5e058a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
