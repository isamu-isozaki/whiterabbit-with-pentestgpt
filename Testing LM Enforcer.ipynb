{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a930cd7-c90d-40f2-ae66-77896eb15d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739ac3e7-b2f8-4175-bad8-4f976eb6b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from lmformatenforcer import CharacterLevelParser, JsonSchemaParser\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from prompts import prompts\n",
    "from schema import llamaindex_llamacpp_lm_format_enforcer, PTT, CompletionStatus, HighLevelTasks, BaseTask,TODOList \n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "import gc\n",
    "model_paths=[\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q5_K_S.gguf\"\n",
    "]\n",
    "output_path = \"./benchmark\"\n",
    "\n",
    "instance = {\n",
    "    \"n_gpu_layers\": 25,\n",
    "    \"n_batch\": 1024,\n",
    "    \"top_p\": 1.0,\n",
    "    \"temperature\": 0.5,\n",
    "    \"generate_len\": 1024,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "\n",
    "from pydantic import BaseModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3902534c-ff01-4577-95fd-4c03a91355a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1060 with Max-Q Design, compute capability 6.1, VMM: yes\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from /mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = whiterabbitneo_whiterabbitneo-13b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q3_K:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 5.27 GiB (3.48 BPW) \n",
      "llm_load_print_meta: general.name     = whiterabbitneo_whiterabbitneo-13b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "llm_load_tensors: offloading 25 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 25/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5396.21 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3250.49 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   300.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   500.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  800.00 MiB, K (f16):  400.00 MiB, V (f16):  400.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    24.01 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   246.40 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   242.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'whiterabbitneo_whiterabbitneo-13b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '11'}\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "        model_path=model_paths[0],\n",
    "        max_new_tokens=instance[\"generate_len\"],\n",
    "        temperature=instance[\"temperature\"],\n",
    "        context_window=instance[\"generate_len\"],\n",
    "        model_kwargs={\n",
    "            \"n_gpu_layers\": instance[\"n_gpu_layers\"],\n",
    "            \"n_batch\": instance[\"n_batch\"],\n",
    "            \"n_ctx\": instance[\"generate_len\"]\n",
    "        },\n",
    "        generate_kwargs={\n",
    "            \"top_k\": instance[\"top_k\"],\n",
    "            \"top_p\": instance[\"top_p\"],\n",
    "            \"temperature\": instance[\"temperature\"]\n",
    "        },\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        completion_to_prompt=completion_to_prompt,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a69036c-f2c8-4eeb-bf16-3719d0dd6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "SYSTEM: You are tasked with generating schema for a task given the prompt. \n",
    "USER: {prompt}. You MUST answer in this schema format: {schema}\n",
    "ASSISTANT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a493832d-1479-498a-a988-962ad0eb3799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'CompletionStatus': {'enum': ['todo', 'done', 'inprogress'],\n",
       "   'title': 'CompletionStatus',\n",
       "   'type': 'string'}},\n",
       " 'properties': {'status': {'$ref': '#/$defs/CompletionStatus'},\n",
       "  'task_description': {'default': None,\n",
       "   'maxLength': 50,\n",
       "   'minLength': 1,\n",
       "   'title': 'Task Description',\n",
       "   'type': 'string'}},\n",
       " 'required': ['status'],\n",
       " 'title': 'BaseTask',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_task_schema = BaseTask.model_json_schema()\n",
    "base_task_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0db6992-1f3e-4fa8-9058-2c17343171ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1762868/1945790164.py:15: PydanticDeprecatedSince20: The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "question_with_schema = f'{question}{AnswerFormat.schema_json()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f350e957-ce19-4c3b-b1a5-a1e5b00752c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_task_schema[\"required\"] = [\"status\", \"task_description\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4439f882-0472-48b2-9861-d0c673edb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =llamaindex_llamacpp_lm_format_enforcer(llm, JsonSchemaParser(base_task_schema), analyze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20813c27-ac03-4bc8-9f51-fab542d8087e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first call\n",
      "None\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {'\\n', '\\r', ' ', '\\t', '{'}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore set()\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore set()\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' ', '\\r', '{'}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore {' '}\n",
      "allowed characters {\n",
      "characters_to_explore set()\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore set()\n",
      "allowed characters  \t\n",
      "\"\n",
      "characters_to_explore {'\"', '\\r'}\n",
      "allowed characters st\n",
      "characters_to_explore set()\n",
      "allowed characters  \t\n",
      "\"\n",
      "characters_to_explore set()\n",
      "allowed characters  \t\n",
      "{\n",
      "characters_to_explore set()\n",
      "allowed characters  \t\n",
      "\"\n",
      "characters_to_explore {'\"', '\\r'}\n",
      "allowed characters st\n",
      "characters_to_explore set()\n",
      "allowed characters  \t\n",
      "\"\n",
      "characters_to_explore set()\n",
      "allowed tokens [13, 16, 30004, 35, 29871, 259, 1678, 268, 418, 539, 4706, 308, 3986, 965, 9651, 632, 6756, 426, 8853, 3336, 12, 126, 29912, 6377, 14626]\n",
      "allowed_tokens [13, 16, 30004, 35, 29871, 259, 1678, 268, 418, 539, 4706, 308, 3986, 965, 9651, 632, 6756, 426, 8853, 3336, 12, 126, 29912, 6377, 14626]\n",
      "prev in but current not in\n",
      "None\n",
      "allowed characters \n",
      "characters_to_explore set()\n",
      "Can end\n",
      "2\n",
      "allowed_tokens [2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7203.03 ms\n",
      "llama_print_timings:      sample time =       0.77 ms /     2 runs   (    0.38 ms per token,  2604.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     980.92 ms /     2 runs   (  490.46 ms per token,     2.04 tokens per second)\n",
      "llama_print_timings:       total time =     988.33 ms /     3 tokens\n"
     ]
    }
   ],
   "source": [
    "out = llm.complete(prompt.format(prompt=\"Do port scan\", schema=base_task_schema)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fabc4b6-a71f-46ac-8bbc-bffbba09c063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eada3f28-81ba-418a-b209-d5014d612760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"], \"title\": \"AnswerFormat\", \"type\": \"object\"}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_with_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "912cce1f-c179-4a56-a7c9-aa8260f0df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =llamaindex_llamacpp_lm_format_enforcer(llm, JsonSchemaParser(AnswerFormat.model_json_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f1f4af-ea6b-4708-a609-f1c2eb9e232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7088.47 ms\n",
      "llama_print_timings:      sample time =       0.75 ms /     2 runs   (    0.37 ms per token,  2673.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2695.14 ms /    22 tokens (  122.51 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:        eval time =     399.62 ms /     1 runs   (  399.62 ms per token,     2.50 tokens per second)\n",
      "llama_print_timings:       total time =    3100.54 ms /    23 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.complete(question).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171240b-a50c-4c6b-ab93-ae5535eeddab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
