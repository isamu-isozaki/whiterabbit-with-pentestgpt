{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a930cd7-c90d-40f2-ae66-77896eb15d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739ac3e7-b2f8-4175-bad8-4f976eb6b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from lmformatenforcer import CharacterLevelParser, JsonSchemaParser\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from prompts import prompts\n",
    "from schema import llamaindex_llamacpp_lm_format_enforcer, PTT\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "import gc\n",
    "import outlines\n",
    "from torch import Generator\n",
    "from outlines.samplers import Sampler, multinomial\n",
    "model_paths=[\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q4_K_S.gguf\",\n",
    "    \"/mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q5_K_S.gguf\"\n",
    "]\n",
    "output_path = \"./benchmark\"\n",
    "\n",
    "instance = {\n",
    "    \"n_gpu_layers\": 25,\n",
    "    \"n_batch\": 1024,\n",
    "    \"top_p\": 1.0,\n",
    "    \"temperature\": 0.5,\n",
    "    \"generate_len\": 1024,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from benchmark import load_outlines\n",
    "from schema import BaseTask, PTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3902534c-ff01-4577-95fd-4c03a91355a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce GTX 1060 with Max-Q Design, compute capability 6.1, VMM: yes\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from /mnt/d/projects/gamified-cybersecurity-ai-server/model/whiterabbitneo-13b.Q3_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = whiterabbitneo_whiterabbitneo-13b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q3_K:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 264/32016 vs 259/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 5.27 GiB (3.48 BPW) \n",
      "llm_load_print_meta: general.name     = whiterabbitneo_whiterabbitneo-13b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.14 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  = 2145.86 MiB\n",
      "llm_load_tensors: VRAM used           = 3250.49 MiB\n",
      "llm_load_tensors: offloading 25 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 25/41 layers to GPU\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  800.00 MiB, K (f16):  400.00 MiB, V (f16):  400.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "llama_new_context_with_model: compute buffer total size = 227.20 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 224.01 MiB\n",
      "llama_new_context_with_model: total VRAM used: 3474.50 MiB (model: 3250.49 MiB, context: 224.01 MiB)\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = load_outlines(\n",
    "        model_paths[0],\n",
    "        instance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1876d88e-c271-4fbd-af23-f17dba9136e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm, sampler = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8000090b-bebb-4252-b55d-a406925fab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b01074-cf04-4929-bdd7-732515d0970d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'BaseTask': {'properties': {'status': {'$ref': '#/$defs/CompletionStatus'},\n",
       "    'task_description': {'maxLength': 50,\n",
       "     'title': 'Task Description',\n",
       "     'type': 'string'}},\n",
       "   'required': ['status', 'task_description'],\n",
       "   'title': 'BaseTask',\n",
       "   'type': 'object'},\n",
       "  'CompletionStatus': {'enum': ['todo', 'done', 'inprogress'],\n",
       "   'title': 'CompletionStatus',\n",
       "   'type': 'string'}},\n",
       " 'properties': {'recon': {'items': {'$ref': '#/$defs/BaseTask'},\n",
       "   'title': 'Recon',\n",
       "   'type': 'array'},\n",
       "  'initial_access': {'items': {'$ref': '#/$defs/BaseTask'},\n",
       "   'title': 'Initial Access',\n",
       "   'type': 'array'},\n",
       "  'execution': {'items': {'$ref': '#/$defs/BaseTask'},\n",
       "   'title': 'Execution',\n",
       "   'type': 'array'},\n",
       "  'post_exploitation': {'items': {'$ref': '#/$defs/BaseTask'},\n",
       "   'title': 'Post Exploitation',\n",
       "   'type': 'array'}},\n",
       " 'required': ['recon', 'initial_access', 'execution', 'post_exploitation'],\n",
       " 'title': 'PTT',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PTT.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbc415aa-3939-49a8-a1d3-83f7d655ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"On 142.250.80.46 there is one ssh port open on port 22, a http port on port 80, and a https port on port 443\"\n",
    "ptt = \"\"\"\n",
    "{\n",
    "    \"Reconnaissance\": [\n",
    "        {\"Perform a full port scan\": \"inprogress\"},\n",
    "        {\"Determine the purpose of each open port\": \"todo\"},\n",
    "    ],\n",
    "}\"\"\"\n",
    "schema=PTT.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a69036c-f2c8-4eeb-bf16-3719d0dd6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"SYSTEM:\n",
    "You're an excellent cybersecurity penetration tester assistant. \n",
    "You are given a Python dictionary, namely \"Penetration Testing Tree (PTT)\", and user input.\n",
    "The PTT contains current tasks. Given user input, modify completion status of each task and add new tasks if needed.\n",
    "You MUST answer using the following json schema: {schema}\n",
    "PTT: {ptt}\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b870fba1-abb7-4414-a176-8b09b9b18920",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = outlines.generate.json(llm, PTT, sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f350e957-ce19-4c3b-b1a5-a1e5b00752c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2a2f1ba710>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = Generator(device=\"cpu\")\n",
    "rng.manual_seed(789005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c685150-324c-4bcd-9276-cae89ab9448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  189678.51 ms\n",
      "llama_print_timings:      sample time =     120.82 ms /   299 runs   (    0.40 ms per token,  2474.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18843.62 ms /   471 tokens (   40.01 ms per token,    25.00 tokens per second)\n",
      "llama_print_timings:        eval time =  104102.08 ms /   298 runs   (  349.34 ms per token,     2.86 tokens per second)\n",
      "llama_print_timings:       total time =  124042.59 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PTT(recon=[BaseTask(status=<CompletionStatus.done: 'done'>, task_description='Perform a full port scan'), BaseTask(status=<CompletionStatus.todo: 'todo'>, task_description='Determine the purpose of each open port')], initial_access=[BaseTask(status=<CompletionStatus.inprogress: 'inprogress'>, task_description='Attempt to exploit vulnerable services'), BaseTask(status=<CompletionStatus.todo: 'todo'>, task_description='Gain initial access if no vulnerabilities found')], execution=[BaseTask(status=<CompletionStatus.done: 'done'>, task_description='Identify potential privilege escalation vectors'), BaseTask(status=<CompletionStatus.todo: 'todo'>, task_description='Execute the chosen privilege escalation technique')], post_exploitation=[BaseTask(status=<CompletionStatus.done: 'done'>, task_description='Obtain sensitive data from the system'), BaseTask(status=<CompletionStatus.todo: 'todo'>, task_description='Preserve evidence and maintain access')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt.format(schema=schema, ptt=ptt, prompt=user), rng=rng, max_tokens=instance[\"generate_len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0db6992-1f3e-4fa8-9058-2c17343171ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1762868/1945790164.py:15: PydanticDeprecatedSince20: The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  question_with_schema = f'{question}{AnswerFormat.schema_json()}'\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(message: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    year_of_birth: int\n",
    "    num_seasons_in_nba: int\n",
    "\n",
    "question = 'Please give me information about Michael Jordan. You MUST answer using the following json schema: '\n",
    "question_with_schema = f'{question}{AnswerFormat.schema_json()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eada3f28-81ba-418a-b209-d5014d612760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please give me information about Michael Jordan. You MUST answer using the following json schema: {\"properties\": {\"first_name\": {\"title\": \"First Name\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"type\": \"string\"}, \"year_of_birth\": {\"title\": \"Year Of Birth\", \"type\": \"integer\"}, \"num_seasons_in_nba\": {\"title\": \"Num Seasons In Nba\", \"type\": \"integer\"}}, \"required\": [\"first_name\", \"last_name\", \"year_of_birth\", \"num_seasons_in_nba\"], \"title\": \"AnswerFormat\", \"type\": \"object\"}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_with_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "912cce1f-c179-4a56-a7c9-aa8260f0df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =llamaindex_llamacpp_lm_format_enforcer(llm, JsonSchemaParser(AnswerFormat.model_json_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f1f4af-ea6b-4708-a609-f1c2eb9e232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7088.47 ms\n",
      "llama_print_timings:      sample time =       0.75 ms /     2 runs   (    0.37 ms per token,  2673.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2695.14 ms /    22 tokens (  122.51 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:        eval time =     399.62 ms /     1 runs   (  399.62 ms per token,     2.50 tokens per second)\n",
      "llama_print_timings:       total time =    3100.54 ms /    23 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.complete(question).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171240b-a50c-4c6b-ab93-ae5535eeddab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
