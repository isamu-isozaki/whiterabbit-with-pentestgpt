from pydantic import BaseModel, constr
from enum import Enum

from typing import Optional
from llama_cpp import LogitsProcessorList
from lmformatenforcer import CharacterLevelParser, JsonSchemaParser
from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor
from llama_index.llms.llama_cpp import LlamaCPP
class CompletionStatus(Enum):
    todo = 'todo'
    done = 'done'
    inprogress = "inprogress"

class HighLevelTasks(Enum):
    recon = 'Reconnaissance'
    initial_access = 'Initial Access'
    execution = 'Execution'
    post_exploitation = 'Post Exploitation'

class BaseTask(BaseModel):
    status: CompletionStatus
    task_description: constr(max_length=50)

class TODOList(BaseModel):
    tasks: list[BaseTask]
class PTT(BaseModel):
    tasks: dict[HighLevelTasks, TODOList]


def update_completion_status(llm: LlamaCpp, prompt: str, current_schema: str):
    # below is incomplete as we need the current schema's str for this to work
    llm =llamaindex_llamacpp_lm_format_enforcer(llm, JsonSchemaParser(CompletionStatus.schema()))
    current_schema_list = current_schema.split("todo")
    output = []
    for elem in current_schema_list:
        if "inprogress" not in elem:
            output.append(elem)
        else:
            for elem_part in elem.split("inprogress"):
                output.append(elem_part)
    current_schema_list = output
    for elem in current_schema_list:
        prompt+=elem
        output = llm.complete(prompt)
        prompt+=output
    return prompt

def add_new_items(llm: LlamaCPP, prompt: str, currentTree: PTT):
    # below is incomplete as the json schema input is inconsistent with the output schema
    # after update completion status
    llm =llamaindex_llamacpp_lm_format_enforcer(llm, JsonSchemaParser(TODOList.schema()))

    prompt += "{"
    for key in currentTree.tasks:
        prompt += f'"{key}": ['
        for task in currentTree[key]:
            prompt += f"Task({task.task_description}: {task.status}),"
        output = llm.complete(prompt)
        prompt += output + '],'
    return prompt

def llamaindex_llamacpp_lm_format_enforcer(llm: LlamaCPP, character_level_parser: Optional[CharacterLevelParser]) -> LlamaCPP:
    logits_processors: Optional[LogitsProcessorList] = None
    if character_level_parser:
        logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm._model, character_level_parser)])
    # If changing the character level parser each call, inject it before calling complete. If its the same format
    # each time, you can set it once after creating the LlamaCPP model
    llm.generate_kwargs['logits_processor'] = logits_processors
    return llm