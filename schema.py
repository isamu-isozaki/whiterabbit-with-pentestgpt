from pydantic import BaseModel, constr
from enum import Enum
import outlines
import json
import copy
from outlines.samplers import Sampler, multinomial, greedy
import re
from langchain.tools import tool
from langchain.prompts import PromptTemplate
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import HumanMessage, SystemMessage, AIMessage
import json
from langchain.chat_models import ChatOpenAI

def get_generator(llm: dict[str, str] | outlines.models.LlamaCpp, generate_type: str = "text", choices: list[str] | None =None, regex=None, json_str=None, temperature: float = 1.0, logging: bool = True):
    if isinstance(llm, dict):
        extra_body = {"outlines_type": generate_type}
        if generate_type == "json":
            extra_body["json"] = json_str
        elif generate_type == "regex":
            extra_body["regex"] = regex
        elif generate_type == "choices":
            extra_body["choices"] = choices

        def get_streamed_output(prompt: str, stop_at: str | None = None, max_tokens: int = 2048):
            generator = ChatOpenAI(temperature=temperature,
                openai_api_base=llm["url"], 
                openai_api_key=llm['api_key'],
                streaming=True,
                max_tokens=max_tokens)
            extra_body["stop_out"] = stop_at
            messages = [
                SystemMessage(
                    content="You are a helpful assistant."
                ),
                HumanMessage(
                    content=prompt
                )
            ]
            full_output = ""
            for chunk in generator.stream(messages, extra_body=extra_body):
                if logging:
                    print(chunk.content, end="", flush=True)
                full_output += chunk.content
            return full_output
        return get_streamed_output
    else:
        def get_outlines_output(prompt: str, stop_at: str | None = None, max_tokens: int = 2048):
            sampler = multinomial(top_k=50, top_p=1.0, temperature=temperature)
            if generate_type == "json":
                generator = outlines.generate.json(llm, json=json_str, sampler=sampler)
            elif generate_type == "regex":
                generator = outlines.generate.regex(llm, regex=regex, sampler=sampler)
            elif generate_type == "choices":
                generator = outlines.generate.choices(llm, choices=choices, sampler=sampler)
            output = generator(prompt, max_tokens=max_tokens, stop_at=stop_at)
            if logging:
                print(output)
            return output
        return get_outlines_output


def get_options(prompt, options):
    output = ""
    assert output not in options
    option_str = options[0] + "/"
    for i, option in enumerate(options):
        if i == 0:
            continue
        option_str += option + "/"
    option_str = option_str[:-1]
    prompt += f" Answer with {option_str}"
    while output not in options:
        output = input(prompt)
    return output

def load_outlines(model_path:str, model_config: dict) -> tuple[outlines.models.LlamaCpp, Sampler]:
    llm = outlines.models.llamacpp(
        model_path,
        **{
            "n_gpu_layers": model_config["n_gpu_layers"],
            "n_batch": model_config["n_batch"],
            "n_ctx": model_config["generate_len"]
        },
        device="cpu"
    )
    sampler =  multinomial(top_k=model_config["top_k"], top_p=model_config["top_p"], temperature=model_config["temperature"])
    return llm, sampler

def find_inprogress(ptt_dict: dict[str, list] | list) -> int:
    in_progress_num = 0

    if isinstance(ptt_dict, list):
        for task in ptt_dict:
            if task["status"] == "in progress":
                in_progress_num += 1
        return in_progress_num
    for key in ptt_dict:
        for task in ptt_dict[key]:
            if task["status"] == "in progress":
                in_progress_num += 1
    return in_progress_num

def find_inprogress_or_todo(ptt_dict: dict[str, list]) -> int:
    num = 0
    for key in ptt_dict:
        for task in ptt_dict[key]:
            if task["status"] == "in progress" or task["status"] == "todo":
                num += 1
    return num

def find_inprogress_task(ptt: dict[str, list] | None = None, ptt_list: list | None = None):
    assert ptt or ptt_list
    if ptt is not None:
        for key in ptt:
            for task in ptt[key]:
                if task["status"] == "in progress":
                    return task["task"]
        assert False, f"There must be exactly one task that is inprogress in {ptt}"
    elif ptt_list is not None:
        for task in ptt_list:
            if task["status"] == "in progress":
                return task["task"]
        assert False, f"There must be exactly one task that is inprogress in {ptt_list}"


def str2dict(output: str):
    try:
        output = output.strip().replace("'", '"')
        return json.loads(output)
    except Exception as e:
        print("repr output: "+repr(output))
        raise Exception(e)

def get_current_status(ptt: dict | None = None, ptt_list: list | None = None):
    assert ptt or ptt_list
    if ptt is not None:
        progress_ptt = copy.deepcopy(ptt)
        delete_indices = {}
        for key in progress_ptt:
            delete_indices[key] = []
            for i, task in enumerate(progress_ptt[key]):
                if task["status"] not in ["done", "in progress"]:
                    delete_indices[key].append(i)
        for key in delete_indices:
            for i in delete_indices[key][::-1]:
                progress_ptt[key].pop(i)
        return progress_ptt
    elif ptt_list is not None:
        progress_ptt = copy.deepcopy(ptt_list)
        delete_indices = []
        for i, task in enumerate(progress_ptt):
            if task["status"] not in ["done", "in progress"]:
                delete_indices.append(i)
        for i in delete_indices[::-1]:
            progress_ptt.pop(i)
        return progress_ptt
def default_qa(template: str, question: str, llm, temperature, max_tokens: int = -1):
    generative_prompt = template.format(prompt=question)
    generator = get_generator(llm, generate_type="text", temperature=temperature)
    if max_tokens == -1:
        return generator(generative_prompt)
    else:
        return generator(generative_prompt, max_tokens=max_tokens)

def input_parser(template: str, command_output: str, llm, temperature, max_tokens: int = 200):
    input_parser_prompt = template.format(prompt=command_output)
    generator = get_generator(llm, generate_type="text", temperature=temperature)

    if max_tokens == -1:
        return generator(input_parser_prompt)
    else:
        return generator(input_parser_prompt, max_tokens=max_tokens)

def generative_module(template: str, llm, sampler, ptt: dict | None = None, ptt_list: list | None = None, current_history:str = "", max_tokens: int = -1, only_provide_currrent_status: bool = False, task: str | None = None, temperature: float = 1.0):
    assert ptt or ptt_list
    # TODO Make below logic cleaner or just migrate to ptt_list altogether if it is better
    if ptt is not None:
        if only_provide_currrent_status:
            ptt = get_current_status(ptt)
        if task is None:
            task = find_inprogress_task(ptt)
    else:
        if only_provide_currrent_status:
            ptt = get_current_status(ptt_list=ptt_list)
        if task is None:
            task = find_inprogress_task(ptt_list=ptt_list)
    print(f"Task is {task}")
    ptt = json.dumps(ptt)

    generative_prompt = template.format(ptt=ptt, prompt=task, history=current_history)
    generator = get_generator(llm, generate_type="text", temperature=temperature)

    if max_tokens == -1:
        return generator(generative_prompt)
    else:
        return generator(generative_prompt, max_tokens=max_tokens)

def get_new_tasks(original: dict[str, dict[str, str]], updated: dict[str, dict[str, str]]) -> list[str]:
    output: list[str] = []
    for key in updated:
        for task in updated[key]:
            description = task["task"]
            task_exists = False
            for original_task in original[key]:
                if description in original_task["task"]:
                    task_exists = True
            if not task_exists:
                output.append(description)
    return output

def delete_status(ptt: dict, statuses: list[str]) -> dict:
    output = {}
    for key in ptt:
        output[key] = []
        for task in ptt[key]:
            valid_task = True
            for status in statuses:
                if status in task["status"]:
                    valid_task = False
            if valid_task:
                output[key].append(task)
    return output

def get_tools(ptt_list: list):
    @tool
    def add_task(status: str, task: str):
        """Add a task with a given status"""
        ptt_list.append({"status": status, "task": task})

    @tool
    def remove_task(task: str):
        """Remove a task from the todo list"""
        i = 0
        found = False
        for i in range(len(ptt_list)):
            if ptt_list[i]["task"] == task:
                found = True
                break
        assert found
        ptt_list.pop(i)

    @tool
    def modify_status(status: str, task: str):
        """Modify the status of the task to a new status"""
        for ptt_task in ptt_list:
            if ptt_task["task"] == task:
                ptt_task["status"] = status
                return
        assert False

    # We may not ever use this option
    @tool
    def run_command(command: str) -> str:
        """Run command and get command output"""
        command_output = input(f"Output from {command}")
        return command_output

    # For this, I plan to use with elastic search/rag
    @tool
    def search(search_content: str):
        """Search for relevant documents to search_content over a cyber security dataset"""
        None


    return [add_task, remove_task, modify_status]

def get_status_tasks(ptt_dict: dict | None = None, ptt_list: list | None = None, status: str = "todo"):
    assert ptt_dict or ptt_list
    if ptt_dict is not None:
        output = {}
        for category in ptt_dict:
            output[category] = []
            for task in ptt_dict[category]:
                if task["status"] == status:
                    output[category].append(task["task"])
            if len(output[category]) == 0:
                del output[category]
    else:
        output = []
        for task in ptt_list:
            if task["status"] == status:
                output.append(task["task"])
    return output

def get_flattened_tasks(ptt_list: list):
    output = []
    for task in ptt_list:
        output.append(task["task"])
    return output

def get_tools_info(tools):
    output = ""
    for i, tool in enumerate(tools):
        output += f"{i+1}. {tool.name}\n"
        output += f"     Description: {tool.description}\n\n"
    output += f"{len(tools)+1}. END\n"
    output += f"     Description: Ends turn\n\n"
    return output

def get_tool_names(tools):
    output = ""
    for i, tool in enumerate(tools):
        output += f"{tool.name}, "
    output += f"END"
    return output

def get_generation_including_stop(generator, prompt, stop_at, max_tokens: int = 2048):
    output = generator(prompt, stop_at, max_tokens) + stop_at
    return output

def guide_react_constrained_output(llm, temperature, choice_temperature, prompt, ptt_list, include_none: bool = True, name_arguments: bool = True):
    try:
        ptt_list = copy.deepcopy(ptt_list)
        original_prompt = prompt
        generator = get_generator(llm, generate_type="text", temperature=temperature)
        inprogress_num = 1
        tools = get_tools(ptt_list)
        add_task, remove_task, modify_status = tools
        tool_names =  get_tool_names(tools)
        completed_tasks =get_status_tasks(ptt_list=ptt_list, status="done")
        todo_tasks =get_status_tasks(ptt_list=ptt_list, status="todo")
        inprogress_task =get_status_tasks(ptt_list=ptt_list, status="in progress")[0]
        tool_choices = tool_names.split(", ")
        if include_none:
            tool_choices.append("None")
        print(f"Tool choices: {tool_choices}")
        print("Original prompt: ", prompt)

        # I do this as I noticed for whiterabbitneo, it has the tendancy to set action to None to continue discussion
        initial_discussion = get_generation_including_stop(generator, prompt, stop_at="Action: ")
        prompt += initial_discussion
        print(initial_discussion)
        while True:
            generator = get_generator(llm, generate_type="choices", choices=tool_choices, temperature=choice_temperature)
            task = generator(prompt)
            action = generator(prompt)
            prompt += action
            print("Action chosen ", action)
            generator = get_generator(llm, generate_type="text", temperature=temperature)

            if action == "None":
                # If action is None we skip all the way to Thought
                analysis = get_generation_including_stop(generator, prompt, stop_at="Action: ")
                prompt += analysis
                print(analysis)
                continue
            elif action == "END":
                prompt += action
                return ptt_list, inprogress_num, prompt[len(original_prompt):]
            # mainly just whitespace etc
            prompt += get_generation_including_stop(generator, prompt, stop_at="Action Input: ")
            # if it's add_task all tasks must be todo
            if action == "add_task":
                if name_arguments:
                    prompt += 'status="todo", task="'
                else:
                    prompt += '"todo", "'
                generator = get_generator(llm, generate_type="text", temperature=temperature)

                # sometimes the llm adds the exact same task in add_task
                task_description = get_generation_including_stop(generator, prompt, stop_at='"')[:-1]
                num_retries = 0
                while (task_description in completed_tasks) or (task_description in todo_tasks) or task_description == inprogress_task:
                    task_description = get_generation_including_stop(generator, prompt, stop_at='"')[:-1]
                    num_retries += 1
                    if num_retries == 3:
                        return ptt_list, inprogress_num, prompt[len(original_prompt):]
                print("Adding task with task description", task_description)
                prompt += task_description+'"'
                add_task._run("todo", task_description)
            elif action == "remove_task":
                if name_arguments:
                    prompt += 'task="'
                else:
                    prompt += '"'
                generator = get_generator(llm, generate_type="choices", choices=get_flattened_tasks(ptt_list), temperature=choice_temperature)
                task = generator(prompt)
                task = generator(prompt)
                print("Removing task ", task)
                prompt += task+'"'
                remove_task._run(task)
                if task == inprogress_task:
                    inprogress_num -= 1
            elif action == "modify_status":
                # Put more harsh conditions here if necessary. For example, not being able to modify done tasks
                # or in progress tasks only being able to change to done
                if name_arguments:
                    prompt += 'status="'
                else:
                    prompt += '"'
                generator = get_generator(llm, generate_type="choices", choices=["done", "todo", "in progress"], temperature=choice_temperature)
                status = generator(prompt)
                prompt += status
                if name_arguments:
                    prompt += '", task="'
                else:
                    prompt += '", "'
                generator = get_generator(llm, generate_type="choices", choices=get_flattened_tasks(ptt_list), temperature=choice_temperature)
                task = generator(prompt)
                prompt += task+'"'
                print("Modifying status of ", task, " to ", status)
                modify_status._run(status, task)
                if task == inprogress_task:
                    inprogress_num -= 1
                if status == "in progress":
                    inprogress_num += 1
            print("Updated ptt list ", ptt_list)
            # The model will do Thought and observation but we do not care too much what the model writes here
            generator = get_generator(llm, generate_type="text", temperature=temperature)
            prompt += get_generation_including_stop(generator, prompt, stop_at="Action: ")
    except Exception as e:
        print(e)
        return ptt_list, inprogress_num, prompt[len(original_prompt):]
def reasoning_module_tools(template: str, summary: str, past_history: str, ptt_list: list, llm, temperature, choice_temperature, dataset: list = [], include_none: bool = True, ask_update_temperature: bool = False, name_arguments: bool = False):
    num_inprogress = find_inprogress(ptt_list)
    assert num_inprogress == 1, f"The number of current inprogress tasks must be 1 in {ptt_list}"
    original_ptt_list = copy.deepcopy(ptt_list)
    while True:
        ptt_list = copy.deepcopy(original_ptt_list)
        tools = get_tools(ptt_list)
        tools_text = get_tools_info(tools)
        tool_names =  get_tool_names(tools)
        completed_tasks =get_status_tasks(ptt_list=ptt_list, status="done")
        todo_tasks =get_status_tasks(ptt_list=ptt_list, status="todo")
        inprogress_task =get_status_tasks(ptt_list=ptt_list, status="in progress")[0]
        llm_prompt = template.format(tools=tools_text, history=past_history, completed_tasks=", ".join(completed_tasks), todo_tasks=", ".join(todo_tasks), inprogress_task=inprogress_task, summary=summary, toolNames=tool_names).strip()
        ptt_list, num_inprogress, output = guide_react_constrained_output(llm, temperature, choice_temperature, llm_prompt, ptt_list, include_none=include_none, name_arguments=name_arguments)
        print(output)
        print("Updated ptt_list: ", json.dumps(ptt_list, indent=4))
        if num_inprogress != 1:
            dataset.append({"prompt": llm_prompt, "output": output, "correct": "n"})
            print("In progress not set")
            if ask_update_temperature:
                update_choice_temperature = get_options("Update choice temperature?", ["y", "n"])
                if update_choice_temperature == "y":
                    choice_temperature = float(input("Enter new temperature"))
                update_temperature = get_options("Update temperature?", ["y", "n"])
                if update_temperature == "y":
                    temperature = float(input("Enter new temperature"))

            continue
        return ptt_list
