from pydantic import BaseModel, constr
from enum import Enum
import outlines
import json
import copy
from outlines.samplers import Sampler, multinomial
import re
from langchain.tools import tool

class CompletionStatus(str, Enum):
    todo = 'todo'
    done = 'done'
    inprogress = "in progress"
class CompletionStatusOnlyNew(str, Enum):
    todo = 'todo'
    inprogress = "in progress"
class InProgressCompletionStatus(str, Enum):
    inprogress = "in progress"
    done = 'done'

class OnlyTODO(str, Enum):
    todo = 'todo'

class OnlyInProgress(str, Enum):
    todo = "in progress"

class BaseTask(BaseModel):
    status: CompletionStatus
    task: constr(max_length=150)
class BaseTaskOnlyTODO(BaseModel):
    status: OnlyTODO
    task: constr(max_length=150)
class BaseTaskOnlyNew(BaseModel):
    status: CompletionStatusOnlyNew
    task: constr(max_length=150)

class PTT(BaseModel):
    Reconnaissance: list[BaseTask]
    Enumeration: list[BaseTask]
    Vulnerability_Scanning: list[BaseTask]
    Exploitation: list[BaseTask]
    Privilege_Escalation: list[BaseTask]
    Post_Exploitation: list[BaseTask]

# TODO: Use below instead
PTTGeneral = dict[str, list[BaseTask]]

def get_options(prompt, options):
    output = ""
    assert output not in options
    option_str = options[0] + "/"
    for i, option in enumerate(options):
        if i == 0:
            continue
        option_str += option + "/"
    option_str = option_str[:-1]
    prompt += f" Answer with {option_str}"
    while output not in options:
        output = input(prompt)
    return output

def load_outlines(model_path:str, model_config: dict) -> tuple[outlines.models.LlamaCpp, Sampler]:
    llm = outlines.models.llamacpp(
        model_path,
        **{
            "n_gpu_layers": model_config["n_gpu_layers"],
            "n_batch": model_config["n_batch"],
            "n_ctx": model_config["generate_len"]
        },
        device="cpu"
    )
    sampler =  multinomial(top_k=model_config["top_k"], top_p=model_config["top_p"], temperature=model_config["temperature"])
    return llm, sampler

def find_inprogress(ptt_dict: dict[str, list] | list) -> int:
    in_progress_num = 0

    if isinstance(ptt_dict, list):
        for task in ptt_dict:
            if task["status"] == "in progress":
                in_progress_num += 1
        return in_progress_num
    for key in ptt_dict:
        for task in ptt_dict[key]:
            if task["status"] == "in progress":
                in_progress_num += 1
    return in_progress_num

def find_inprogress_or_todo(ptt_dict: dict[str, list]) -> int:
    num = 0
    for key in ptt_dict:
        for task in ptt_dict[key]:
            if task["status"] == "in progress" or task["status"] == "todo":
                num += 1
    return num

def find_inprogress_task(ptt: dict[str, list] | None = None, ptt_list: list | None = None):
    assert ptt or ptt_list
    if ptt is not None:
        for key in ptt:
            for task in ptt[key]:
                if task["status"] == "in progress":
                    return task["task"]
        assert False, f"There must be exactly one task that is inprogress in {ptt}"
    elif ptt_list is not None:
        for task in ptt_list:
            if task["status"] == "in progress":
                return task["task"]
        assert False, f"There must be exactly one task that is inprogress in {ptt_list}"


def str2dict(output: str):
    try:
        output = output.strip().replace("'", '"')
        return json.loads(output)
    except Exception as e:
        print("repr output: "+repr(output))
        raise Exception(e)

def get_current_status(ptt: dict | None = None, ptt_list: list | None = None):
    assert ptt or ptt_list
    if ptt is not None:
        progress_ptt = copy.deepcopy(ptt)
        delete_indices = {}
        for key in progress_ptt:
            delete_indices[key] = []
            for i, task in enumerate(progress_ptt[key]):
                if task["status"] not in ["done", "in progress"]:
                    delete_indices[key].append(i)
        for key in delete_indices:
            for i in delete_indices[key][::-1]:
                progress_ptt[key].pop(i)
        return progress_ptt
    elif ptt_list is not None:
        progress_ptt = copy.deepcopy(ptt_list)
        delete_indices = []
        for i, task in enumerate(progress_ptt):
            if task["status"] not in ["done", "in progress"]:
                delete_indices.append(i)
        for i in delete_indices[::-1]:
            progress_ptt.pop(i)
        return progress_ptt
def default_qa(template: str, question: str, llm, sampler, max_tokens: int = -1):
    generative_prompt = template.format(prompt=question)
    generator = outlines.generate.text(llm, sampler=sampler)
    if max_tokens == -1:
        return generator(generative_prompt)
    else:
        return generator(generative_prompt, max_tokens=max_tokens)

def input_parser(template: str, command_output: str, llm, sampler, max_tokens: int = 200):
    input_parser_prompt = template.format(prompt=command_output)
    generator = outlines.generate.text(llm, sampler=sampler)
    if max_tokens == -1:
        return generator(input_parser_prompt)
    else:
        return generator(input_parser_prompt, max_tokens=max_tokens)

def generative_module(template: str, llm, sampler, ptt: dict | None = None, ptt_list: list | None = None, current_history:str = "", max_tokens: int = -1, only_provide_currrent_status: bool = False, task: str | None = None):
    assert ptt or ptt_list
    # TODO Make below logic cleaner or just migrate to ptt_list altogether if it is better
    if ptt is not None:
        if only_provide_currrent_status:
            ptt = get_current_status(ptt)
        if task is None:
            task = find_inprogress_task(ptt)
    else:
        if only_provide_currrent_status:
            ptt = get_current_status(ptt_list=ptt_list)
        if task is None:
            task = find_inprogress_task(ptt_list=ptt_list)
    print(f"Task is {task}")
    ptt = json.dumps(ptt)

    generative_prompt = template.format(ptt=ptt, prompt=task, history=current_history)
    generator = outlines.generate.text(llm, sampler=sampler)
    if max_tokens == -1:
        return generator(generative_prompt)
    else:
        return generator(generative_prompt, max_tokens=max_tokens)

def get_new_tasks(original: dict[str, dict[str, str]], updated: dict[str, dict[str, str]]) -> list[str]:
    output: list[str] = []
    for key in updated:
        for task in updated[key]:
            description = task["task"]
            task_exists = False
            for original_task in original[key]:
                if description in original_task["task"]:
                    task_exists = True
            if not task_exists:
                output.append(description)
    return output

def delete_status(ptt: dict, statuses: list[str]) -> dict:
    output = {}
    for key in ptt:
        output[key] = []
        for task in ptt[key]:
            valid_task = True
            for status in statuses:
                if status in task["status"]:
                    valid_task = False
            if valid_task:
                output[key].append(task)
    return output
def reasoning_module(template: str, prompt: str, past_history: str, ptt: dict, llm, sampler, force_add_task={}, update_status=True, todo_tasks:list[str]=["Obtain a secret file with a hash in it"], max_spaces:int=0, delete_done: bool =False, choice_temperature=0.3):
    force_add_task = copy.deepcopy(force_add_task)
    num_inprogress = find_inprogress(ptt)
    assert num_inprogress <= 1, f"The number of current inprogress tasks must be 1 in {ptt}"
    llm_prompt = template.format(ptt=json.dumps(ptt), history=past_history, prompt=prompt).strip()
    while True:
        if update_status:
            original, inprogress_set = update_completion_status_outlines(llm, llm_prompt, ptt, todo_tasks=todo_tasks, inprogress_always_set=False, choice_temperature=choice_temperature)
        else:
            original = ptt
            inprogress_set = True
        print("Updated completion status")
        print(original)
        delete_statuses = []
        if delete_done:
            delete_statuses.append("done")
        new_tasks: list[str] = []
        updated = add_new_items_outlines(llm, llm_prompt, original, sampler, inprogress_set, max_spaces=max_spaces, force_add_task=force_add_task)
        # If new items are added they do not get deleted
        added_tasks = get_new_tasks(original, updated)
        for added_task in added_tasks:
            new_tasks.append(added_task)
        num_inprogress = find_inprogress(updated)
        print("Added new tasks")
        print(new_tasks)
        print(f"Number of inprogress tasks: ", num_inprogress)
        if num_inprogress == 0:
            ptt = updated
            llm_prompt = template.format(ptt=json.dumps(ptt), history=past_history, prompt=prompt).strip()

            if update_status:
                updated, inprogress_set = update_completion_status_outlines(llm, llm_prompt, ptt, todo_tasks=todo_tasks, inprogress_always_set=True, new_tasks=new_tasks, choice_temperature=choice_temperature)
                updated = delete_status(updated, delete_statuses)
                num_inprogress = find_inprogress(updated)
                print("Updated status")
                print(updated)
                if num_inprogress == 1:
                    return updated
                print("In progress not set")
                update_choice_temperature = get_options("Update choice temperature?", ["y", "n"])
                if update_choice_temperature == "y":
                    choice_temperature = float(input("Enter new temperature"))
            else:
                # shouldn't be possible to come here as if update_staus is set to False, then we only add todo tasks to the existing ptt 
                # and do nothing else
                raise Exception("If we are not updating status, there must already be one inprogress task")
        if num_inprogress == 1:
            return updated

def get_tools(ptt_list: list):
    @tool
    def add_task(status: str, task: str):
        """Add a task with a given status"""
        ptt_list.append({"status": status, "task": task})

    @tool
    def remove_task(task: str):
        """Remove a task from the todo list"""
        i = 0
        found = False
        for i in range(len(ptt_list)):
            if ptt_list[i]["task"] == task:
                found = True
                break
        assert found
        ptt_list.pop(i)

    @tool
    def modify_status(status: str, task: str):
        """Modify the status of the task to a new status"""
        for ptt_task in ptt_list:
            if ptt_task["task"] == task:
                ptt_task["status"] = status
                return
        assert False

    # We may not ever use this option
    @tool
    def run_command(command: str) -> str:
        """Run command and get command output"""
        command_output = input(f"Output from {command}")
        return command_output

    # For this, I plan to use with elastic search/rag
    @tool
    def search(search_content: str):
        """Search for relevant documents to search_content over a cyber security dataset"""
        None


    return [add_task, remove_task, modify_status]

def get_status_tasks(ptt_dict: dict | None = None, ptt_list: list | None = None, status: str = "todo"):
    assert ptt_dict or ptt_list
    if ptt_dict is not None:
        output = {}
        for category in ptt_dict:
            output[category] = []
            for task in ptt_dict[category]:
                if task["status"] == status:
                    output[category].append(task["task"])
            if len(output[category]) == 0:
                del output[category]
    else:
        output = []
        for task in ptt_list:
            if task["status"] == status:
                output.append(task["task"])
    return output

def get_flattened_tasks(ptt_list: list):
    output = []
    for task in ptt_list:
        output.append(task["task"])
    return output

def get_tools_info(tools):
    output = ""
    for i, tool in enumerate(tools):
        output += f"{i+1}. {tool.name}\n"
        output += f"     Description: {tool.description}\n\n"
    output += f"{len(tools)+1}. END\n"
    output += f"     Description: Ends turn\n\n"
    return output

def get_tool_names(tools):
    output = ""
    for i, tool in enumerate(tools):
        output += f"{tool.name}, "
    output += f"END"
    return output

def get_generation_including_stop(generator, prompt, stop_at):
    output = generator(prompt, stop_at=stop_at) + stop_at
    return output

def guide_react_constrained_output(llm, sampler, choice_sampler, prompt, ptt_list):
    ptt_list = copy.deepcopy(ptt_list)
    original_prompt = prompt
    generator = outlines.generate.text(llm, sampler=sampler)
    inprogress_num = 1
    tools = get_tools(ptt_list)
    add_task, remove_task, modify_status = tools
    tool_names =  get_tool_names(tools)
    completed_tasks =get_status_tasks(ptt_list=ptt_list, status="done")
    todo_tasks =get_status_tasks(ptt_list=ptt_list, status="todo")
    inprogress_task =get_status_tasks(ptt_list=ptt_list, status="in progress")[0]
    tool_choices = tool_names.split(", ")
    print(f"Tool choices: {tool_choices}")
    # I do this as I noticed for whiterabbitneo, it has the tendancy to set action to None to continue discussion
    initial_discussion = get_generation_including_stop(generator, prompt, stop_at="Thought: ")
    prompt += initial_discussion
    while True:
        thought = get_generation_including_stop(generator, prompt, stop_at="Action: ")
        prompt += thought
        generator = outlines.generate.choice(llm, tool_choices)
        action = generator(prompt)
        prompt += action
        generator = outlines.generate.text(llm, sampler=sampler)
        if action == "None":
            # If action is None we skip all the way to Thought
            analysis = get_generation_including_stop(generator, prompt, stop_at="Thought: ")
            prompt += analysis
            continue
        # mainly just whitespace etc
        prompt += get_generation_including_stop(generator, prompt, stop_at="Action Input: ")
        # if it's add_task all tasks must be todo
        if action == "add_task":
            prompt += 'status="todo", task="'
            generator = outlines.generate.text(llm, sampler=sampler)
            # sometimes the llm adds the exact same task in add_task
            task_description = get_generation_including_stop(generator, prompt, stop_at='"')[:-1]
            while (task_description in completed_tasks) or (task_description in todo_tasks) or task_description == inprogress_task:
                task_description = get_generation_including_stop(generator, prompt, stop_at='"')[:-1]

            prompt += task_description+'"'
            add_task._run("todo", task_description)
        elif action == "remove_task":
            prompt += 'task="'
            generator = outlines.generate.choice(llm, sampler=choice_sampler, choices=get_flattened_tasks(ptt_list))
            task = generator(prompt)
            prompt += task+'"'
            remove_task._run(task)
            if task == inprogress_task:
                inprogress_num -= 1
        elif action == "modify_status":
            # Put more harsh conditions here if necessary. For example, not being able to modify done tasks
            # or in progress tasks only being able to change to done
            prompt += 'status="'
            generator = outlines.generate.choice(llm, sampler=choice_sampler, choices=["done", "todo", "in progress"])
            status = generator(prompt)
            prompt += status
            prompt += '", task="'
            generator = outlines.generate.choice(llm, sampler=choice_sampler, choices=get_flattened_tasks(ptt_list))
            task = generator(prompt)
            prompt += task+'"'
            modify_status._run(status, task)
            if task == inprogress_task:
                inprogress_num -= 1
            if status == "in progress":
                inprogress_num += 1
        elif action == "END":
            prompt += action
            return ptt_list, inprogress_num, prompt[len(original_prompt):]
        # The model will do Thought and observation but we do not care too much what the model writes here
        generator = outlines.generate.text(llm, sampler=sampler)
        prompt += get_generation_including_stop(generator, prompt, stop_at="Thought: ")

def reasoning_module_tools(template: str, summary: str, past_history: str, ptt_list: list, llm, sampler, choice_sampler, dataset: list = []):
    num_inprogress = find_inprogress(ptt_list)
    assert num_inprogress == 1, f"The number of current inprogress tasks must be 1 in {ptt_list}"
    original_ptt_list = copy.deepcopy(ptt_list)
    while True:
        ptt_list = copy.deepcopy(original_ptt_list)
        tools = get_tools(ptt_list)
        tools_text = get_tools_info(tools)
        tool_names =  get_tool_names(tools)
        completed_tasks =get_status_tasks(ptt_list=ptt_list, status="done")
        todo_tasks =get_status_tasks(ptt_list=ptt_list, status="todo")
        inprogress_task =get_status_tasks(ptt_list=ptt_list, status="in progress")[0]
        llm_prompt = template.format(tools=tools_text, history=past_history, completed_tasks=", ".join(completed_tasks), todo_tasks=", ".join(todo_tasks), inprogress_task=inprogress_task, summary=summary, toolNames=tool_names).strip()
        ptt_list, num_inprogress, output = guide_react_constrained_output(llm, sampler, choice_sampler, llm_prompt, ptt_list)
        print("Actions: ", output)
        print("Updated ptt_list: ", json.dumps(ptt_list, indent=4))
        if num_inprogress != 1:
            dataset.append({"prompt": llm_prompt, "output": output, "correct": "n"})
            print("In progress not set")
            update_choice_temperature = get_options("Update choice temperature?", ["y", "n"])
            if update_choice_temperature == "y":
                choice_temperature = float(input("Enter new temperature"))
            choice_sampler = multinomial(top_k=50, top_p=1.0, temperature=choice_temperature)
            update_temperature = get_options("Update temperature?", ["y", "n"])
            if update_temperature == "y":
                temperature = float(input("Enter new temperature"))
            sampler = multinomial(top_k=50, top_p=1.0, temperature=temperature)

            continue
        return ptt_list

def description_first_status_later(ptt: dict) -> dict:
    output = {}
    for key in ptt:
        output[key] = []
        for elem in ptt[key]:
            task = elem["task"]
            status = elem["status"]
            output[key].append({"task": task, "status": status})
    return output
def status_first_description_later(ptt: dict) -> dict:
    output = {}
    for key in ptt:
        output[key] = []
        for elem in ptt[key]:
            task = elem["task"]
            status = elem["status"]
            output[key].append({"status": status, "task": task})
    return output

def update_completion_status_outlines(llm, prompt: str, ptt: dict, todo_tasks:list[str]=["Obtain a secret file with a hash in it"], inprogress_always_set: bool = True, new_tasks: list[str] = [], choice_temperature: float = 0.3):
    """
    For updating status, we want to first see the task description then decide on the status.
    This is because we want our model to first read the task description before choosing a status
    """
    min_number_of_todo_inprogress = len(todo_tasks)+1
    current_number_of_todo_inprogress = find_inprogress_or_todo(ptt)
    assert current_number_of_todo_inprogress >= min_number_of_todo_inprogress
    status_schema = CompletionStatus if current_number_of_todo_inprogress > min_number_of_todo_inprogress else CompletionStatusOnlyNew
    # below is incomplete as we need the current schema's str for this to work
    choice_sampler =  multinomial(top_k=1000, top_p=1, temperature=choice_temperature)
    ptt = description_first_status_later(ptt)
    ptt = json.dumps(ptt)
    original_prompt_len = len(prompt)
    choices: list[str] = []
    for status in status_schema:
        choices += [status.value]
    print(choices)
    generator =outlines.generate.choice(llm, choices, sampler=choice_sampler)
    ptt_list = ptt.split("todo")
    output = []
    for elem in ptt_list:
        if "in progress" not in elem:
            output.append((elem, "todo"))
        else:
            # there is only one inprogress in todo list
            elems = elem.split("in progress")
            output.append((elems[0], "in progress"))
            output.append((elems[1], "todo"))

    ptt_list = output
    inprogress_set = False
    for i, elem in enumerate(ptt_list):
        elem_task = elem[0]
        elem_curr_status = elem[1]
        prompt+=elem_task
        task_set = False
        # If task is a task we want to set as todo
        for todo_task in todo_tasks:
            if todo_task in elem_task:
                output = "todo"
                task_set = True
        if task_set:
            prompt+=output
            continue
        new_task = False
        for new_task in new_tasks:
            if new_task in elem_task:
                new_task = True
        if new_task:
            choices: list[str] = []
            for status in CompletionStatusOnlyNew:
                choices += [status.value]
            generator =outlines.generate.choice(llm, choices, sampler=choice_sampler)
        if i == len(ptt_list)-1:
            output = ""
        elif inprogress_set and new_task:
            output = "todo"
        elif inprogress_set:
            choices = ["todo", "done"]
            generator =outlines.generate.choice(llm, choices, sampler=choice_sampler)
            output = generator(prompt)
        elif current_number_of_todo_inprogress == min_number_of_todo_inprogress and not inprogress_set and inprogress_always_set:
            output = "in progress"
        elif elem_curr_status == "in progress":
            assert not new_task
            inprogress_choices: list[str] = []
            for status in InProgressCompletionStatus:
                inprogress_choices += [status.value]
            generator =outlines.generate.choice(llm, inprogress_choices, sampler=choice_sampler)
            output = generator(prompt)
        else:
            choices: list[str] = []
            for status in status_schema:
                choices += [status.value]
            generator =outlines.generate.choice(llm, choices, sampler=choice_sampler)
            output = generator(prompt)
        if output == "done":
            current_number_of_todo_inprogress -= 1
        choices: list[str] = []
        for status in status_schema:
            choices += [status.value]
        generator =outlines.generate.choice(llm, choices, sampler=choice_sampler)
        if "in progress" in output:
            inprogress_set = True
        prompt+=output
    print("Updating done from ")
    updated_todo_ptt = prompt[original_prompt_len:]
    print(str2dict(updated_todo_ptt))
    prompt = prompt[:original_prompt_len]
    ptt_list = updated_todo_ptt.split("done")
    output = []
    for elem in ptt_list:
        output.append((elem, "done"))

    ptt_list = output
    choices = ["done"]
    generator =outlines.generate.choice(llm, choices, sampler=choice_sampler)
    for i, elem in enumerate(ptt_list):
        elem_task = elem[0]
        elem_curr_status = elem[1]
        prompt+=elem_task
        if i == len(ptt_list)-1:
            output = ""
        else:
            output = generator(prompt)
        prompt+=output
    return str2dict(prompt[original_prompt_len:]), inprogress_set

def add_whitespace(prompt, llm, sampler, max_spaces=4):
    if max_spaces == 0:
        return ""
    whitespace_regex = r"[ \t\r\n]*"
    generator = outlines.generate.regex(
        llm,
        whitespace_regex,
        sampler=sampler
    )
    whitespace = generator(prompt, max_tokens=max_spaces)
    return whitespace
def add_new_items_outlines(llm, prompt: str, ptt: dict, sampler, inprogress_set: bool = False, max_spaces=0, force_add_task: dict[str, int] = {}):
    """
    For this, we want our model to first output a completion status and then a task description. This is because we want it to know the
    status before thinking of what kind of task it is
    """
    """
    schema format:
    {"recon": [
        {"task": "Perform a full port scan", "status": "done"},
        {"task": "Determine the purpose of each open port", "status": "todo"}
    ], 
    "initial_access": [], 
    "execution": [], 
    "post_exploitation": []}
    """
    ptt = status_first_description_later(ptt)
    ptt = json.dumps(ptt)
    original_prompt_len = len(prompt)
    choice_sampler =  multinomial(top_k=50, top_p=1, temperature=1.0)


    if inprogress_set:
        task_schema = BaseTaskOnlyTODO
    else:
        task_schema = BaseTaskOnlyNew


    continue_choices = [",",  "],"]
    tree_dict = json.loads(ptt)
    prompt += add_whitespace(prompt, llm, sampler, max_spaces)
    prompt += "{"
    prompt += add_whitespace(prompt, llm, sampler, max_spaces)
    for i, key in enumerate(tree_dict):
        prompt += f'"{key}": ['
        prompt += add_whitespace(prompt, llm, sampler, max_spaces)
        if len(tree_dict[key]) > 0:
            for task in tree_dict[key]:
                description = task["task"]
                status = task["status"]
                prompt += '{"status":' + f'"{status}", "task": "{description}"' +'},'
            prompt = prompt[:-1]
            # The model always chooses to end the todo list
            if force_add_task.get(key, 0) > 0:
                force_add_task[key] -= 1
                prompt += ","
                prompt += add_whitespace(prompt, llm, sampler, max_spaces)
            else:
                generator =outlines.generate.choice(llm, continue_choices, sampler=choice_sampler)
                output = generator(prompt)
                prompt+=output
                if "]," in output:
                    if len(tree_dict) -1 != i:
                        prompt += add_whitespace(prompt, llm, sampler, max_spaces)
                    continue
                prompt += add_whitespace(prompt, llm, sampler, max_spaces)
        else:
            if force_add_task.get(key, 0) > 0:
                force_add_task[key] -= 1
                prompt += add_whitespace(prompt, llm, sampler, max_spaces)
            else:
                generator =outlines.generate.choice(llm, ['{"',  "],"], sampler=choice_sampler)
                output = generator(prompt)
                if "]," in output:
                    prompt+=output
                    continue
        while True:
            generator = outlines.generate.json(llm, task_schema, sampler=sampler, whitespace_pattern=" \t\n\r")
            output = generator(prompt)
            safe_task = re.escape(output.task.replace("'",'"')).replace('\\','')
            output = json.dumps({"status": output.status.value, "task": f"{safe_task}"})
            if "in progress" in output:
                task_schema =  BaseTaskOnlyTODO
            prompt += output
            if force_add_task.get(key, 0) > 0:
                force_add_task[key] -= 1
                prompt += ","
                prompt += add_whitespace(prompt, llm, sampler, max_spaces)
            else:
                generator =outlines.generate.choice(llm, continue_choices, sampler=sampler)
                output = generator(prompt)
                prompt += output
                if output == "],":
                    if len(tree_dict) -1 != i:
                        prompt += add_whitespace(prompt, llm, sampler, max_spaces)
                    break
    prompt = prompt[:-1] +"}"

    return str2dict(prompt[original_prompt_len:])
