from pydantic import BaseModel, constr
from enum import Enum

from typing import Optional
from llama_cpp import LogitsProcessorList
from lmformatenforcer import CharacterLevelParser, JsonSchemaParser, RegexParser, UnionParser, StringParser
from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor
from llama_index.llms.llama_cpp import LlamaCPP
import copy
import outlines
import json
class CompletionStatus(str, Enum):
    todo = 'todo'
    done = 'done'
    inprogress = "inprogress"
class InProgressCompletionStatus(str, Enum):
    inprogress = "inprogress"
    done = 'done'

class OnlyTODO(str, Enum):
    todo = 'todo'

class BaseTask(BaseModel):
    status: CompletionStatus
    task_description: constr(max_length=50)
class BaseTaskOnlyTODO(BaseModel):
    status: OnlyTODO
    task_description: constr(max_length=50)

class PTT(BaseModel):
    recon: list[BaseTask]
    initial_access: list[BaseTask]
    execution: list[BaseTask]
    post_exploitation: list[BaseTask]



def update_completion_status_outlines(llm, prompt: str, current_schema: str):
    # below is incomplete as we need the current schema's str for this to work
    original_prompt_len = len(prompt)
    choices: list[str] = []
    for status in CompletionStatus:
        choices += [status.value]
    generator =outlines.generate.choice(llm, choices)
    current_schema_list = current_schema.split("todo")
    output = []
    for elem in current_schema_list:
        if "inprogress" not in elem:
            output.append((elem, "todo"))
        else:
            # there is only one in progress in todo list
            elems = elem.split("inprogress")
            output.append((elems[0], "inprogress"))
            output.append((elems[1], "todo"))

    current_schema_list = output
    inprogress_set = False
    for i, elem in enumerate(current_schema_list):
        elem_task = elem[0]
        elem_curr_status = elem[1]
        prompt+=elem_task
        if i == len(current_schema_list)-1:
            output = ""
        elif inprogress_set:
            output = "todo"
        elif elem_curr_status == "inprogress":
            inprogress_choices: list[str] = []
            for status in InProgressCompletionStatus:
                inprogress_choices += [status.value]
            generator =outlines.generate.choice(llm, inprogress_choices)
            output = generator(prompt)
            
        else:
            output = generator(prompt)
        if "inprogress" in output:
            inprogress_set = True
        
        prompt+=output
    return prompt[original_prompt_len:], inprogress_set

def update_completion_status(llm: LlamaCPP, prompt: str, current_schema: str):
    # below is incomplete as we need the current schema's str for this to work
    original_prompt_len = len(prompt)
    choices_parser: list[CharacterLevelParser] = []
    for status in CompletionStatus:
        choices_parser += [StringParser(status.value)]
    llm =llamaindex_llamacpp_lm_format_enforcer(llm, UnionParser(choices_parser))
    current_schema_list = current_schema.split("todo")
    output = []
    for elem in current_schema_list:
        if "inprogress" not in elem:
            output.append((elem, "todo"))
        else:
            # there is only one in progress in todo list
            elems = elem.split("inprogress")
            output.append((elems[0], "inprogress"))
            output.append((elems[1], "todo"))

    current_schema_list = output
    inprogress_set = False
    for i, elem in enumerate(current_schema_list):
        elem_task = elem[0]
        elem_curr_status = elem[1]
        prompt+=elem_task
        if i == len(current_schema_list)-1:
            output = ""
        elif inprogress_set:
            output = "todo"
        elif elem_curr_status == "inprogress":
            choices_parser: list[CharacterLevelParser] = []
            for status in InProgressCompletionStatus:
                choices_parser += [StringParser(status.value)]
            llm =llamaindex_llamacpp_lm_format_enforcer(llm, UnionParser(choices_parser))
            output = llm.complete(prompt).text
            choices_parser: list[CharacterLevelParser] = []
            for status in CompletionStatus:
                choices_parser += [StringParser(status.value)]
            llm =llamaindex_llamacpp_lm_format_enforcer(llm, UnionParser(choices_parser))
        else:
            output = llm.complete(prompt).text
        if "inprogress" in output:
            inprogress_set = True
        
        prompt+=output
    return prompt[original_prompt_len:], inprogress_set

def add_new_items(llm: LlamaCPP, prompt: str, currentTree: str, inprogress_set: bool = False):
    # below is incomplete as the json schema input is inconsistent with the output schema
    # after update completion status
    """
    schema format:
    {"recon": [
        {"task_description": "Perform a full port scan", "status": "done"},
        {"task_description": "Determine the purpose of each open port", "status": "todo"}
    ], 
    "initial_access": [], 
    "execution": [], 
    "post_exploitation": []}
    """
    original_prompt_len = len(prompt)

    if inprogress_set:
        task_parser = JsonSchemaParser(BaseTaskOnlyTODO.schema())
    else:
        task_parser = JsonSchemaParser(BaseTask.schema())

    list_next_parser = UnionParser([StringParser(","), StringParser("],")])
    tree_dict = json.loads(currentTree)
    prompt += "{"
    for key in tree_dict:
        prompt += f'"{key}": ['
        for task in tree_dict[key]:
            task_description = task["task_description"]
            status = task["status"]
            prompt += '{"task_description":' + f'{task_description}, "status": {status}' +'},'
        while True:
            # print(prompt)
            llm = llamaindex_llamacpp_lm_format_enforcer(llm, task_parser)
            output = llm.complete(prompt).text
            if "inprogress" in output:
                task_parser =  JsonSchemaParser(BaseTaskOnlyTODO.schema())
            prompt += output
            print("^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^")
            print(output)
            llm = llamaindex_llamacpp_lm_format_enforcer(llm, list_next_parser)
            output = llm.complete(prompt).text
            prompt += output
            if output == "],":
                break
    prompt += "}"
    return prompt[original_prompt_len:]

def llamaindex_llamacpp_lm_format_enforcer(llm: LlamaCPP, character_level_parser: Optional[CharacterLevelParser], analyze: bool = False) -> LlamaCPP:
    logits_processors: Optional[LogitsProcessorList] = None
    if character_level_parser:
        logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm._model, character_level_parser, analyze=analyze)])
    # If changing the character level parser each call, inject it before calling complete. If its the same format
    # each time, you can set it once after creating the LlamaCPP model
    llm.generate_kwargs['logits_processor'] = logits_processors
    return llm