from pydantic import BaseModel, constr
from enum import Enum

from typing import Optional
from llama_cpp import LogitsProcessorList
from lmformatenforcer import CharacterLevelParser, JsonSchemaParser, RegexParser, UnionParser, StringParser
from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor
from llama_index.llms.llama_cpp import LlamaCPP
import copy
import outlines
import json
from outlines.samplers import Sampler, multinomial

class CompletionStatus(str, Enum):
    todo = 'todo'
    done = 'done'
    inprogress = "inprogress"
class CompletionStatusOnlyNew(str, Enum):
    todo = 'todo'
    inprogress = "inprogress"
class InProgressCompletionStatus(str, Enum):
    inprogress = "inprogress"
    done = 'done'

class OnlyTODO(str, Enum):
    todo = 'todo'

class BaseTask(BaseModel):
    status: CompletionStatus
    task_description: constr(max_length=100)
class BaseTaskOnlyTODO(BaseModel):
    status: OnlyTODO
    task_description: constr(max_length=100)
class BaseTaskOnlyNew(BaseModel):
    status: CompletionStatusOnlyNew
    task_description: constr(max_length=100)

class PTT(BaseModel):
    recon: list[BaseTask]
    initial_access: list[BaseTask]
    execution: list[BaseTask]
    post_exploitation: list[BaseTask]


def update_completion_status(llm: LlamaCPP, prompt: str, current_schema: str):
    # below is incomplete as we need the current schema's str for this to work
    original_prompt_len = len(prompt)
    choices_parser: list[CharacterLevelParser] = []
    for status in CompletionStatus:
        choices_parser += [StringParser(status.value)]
    llm =llamaindex_llamacpp_lm_format_enforcer(llm, UnionParser(choices_parser))
    current_schema_list = current_schema.split("todo")
    output = []
    for elem in current_schema_list:
        if "inprogress" not in elem:
            output.append((elem, "todo"))
        else:
            # there is only one in progress in todo list
            elems = elem.split("inprogress")
            output.append((elems[0], "inprogress"))
            output.append((elems[1], "todo"))

    current_schema_list = output
    inprogress_set = False
    for i, elem in enumerate(current_schema_list):
        elem_task = elem[0]
        elem_curr_status = elem[1]
        prompt+=elem_task
        if i == len(current_schema_list)-1:
            output = ""
        elif inprogress_set:
            output = "todo"
        elif elem_curr_status == "inprogress":
            choices_parser: list[CharacterLevelParser] = []
            for status in InProgressCompletionStatus:
                choices_parser += [StringParser(status.value)]
            llm =llamaindex_llamacpp_lm_format_enforcer(llm, UnionParser(choices_parser))
            output = llm.complete(prompt).text
            choices_parser: list[CharacterLevelParser] = []
            for status in CompletionStatus:
                choices_parser += [StringParser(status.value)]
            llm =llamaindex_llamacpp_lm_format_enforcer(llm, UnionParser(choices_parser))
        else:
            output = llm.complete(prompt).text
        if "inprogress" in output:
            inprogress_set = True

        prompt+=output
    return prompt[original_prompt_len:], inprogress_set


def update_completion_status_outlines(llm, prompt: str, current_schema: str, sampler):
    # below is incomplete as we need the current schema's str for this to work
    original_prompt_len = len(prompt)
    choices: list[str] = []
    for status in CompletionStatus:
        choices += [status.value]
    generator =outlines.generate.choice(llm, choices)
    current_schema_list = current_schema.split("todo")
    output = []
    for elem in current_schema_list:
        if "inprogress" not in elem:
            output.append((elem, "todo"))
        else:
            # there is only one in progress in todo list
            elems = elem.split("inprogress")
            output.append((elems[0], "inprogress"))
            output.append((elems[1], "todo"))

    current_schema_list = output
    inprogress_set = False
    for i, elem in enumerate(current_schema_list):
        elem_task = elem[0]
        elem_curr_status = elem[1]
        prompt+=elem_task
        if i == len(current_schema_list)-1:
            output = ""
        elif inprogress_set:
            output = "todo"
        elif elem_curr_status == "inprogress":
            inprogress_choices: list[str] = []
            for status in InProgressCompletionStatus:
                inprogress_choices += [status.value]
            generator =outlines.generate.choice(llm, inprogress_choices, sampler=sampler)
            output = generator(prompt)
        else:
            output = generator(prompt)
        if "inprogress" in output:
            inprogress_set = True
        prompt+=output
    return prompt[original_prompt_len:], inprogress_set

def add_new_items(llm: LlamaCPP, prompt: str, currentTree: str, inprogress_set: bool = False, recon: bool = False, initial_access: bool = False, execution: bool = False, post_exploitation: bool = False):
    # below is incomplete as the json schema input is inconsistent with the output schema
    # after update completion status
    """
    schema format:
    {"recon": [
        {"task_description": "Perform a full port scan", "status": "done"},
        {"task_description": "Determine the purpose of each open port", "status": "todo"}
    ], 
    "initial_access": [], 
    "execution": [], 
    "post_exploitation": []}
    """
    original_prompt_len = len(prompt)

    if inprogress_set:
        task_parser = JsonSchemaParser(BaseTaskOnlyTODO.schema())
    else:
        task_parser = JsonSchemaParser(BaseTaskOnlyNew.schema())


    list_next_parser = UnionParser([StringParser(","), StringParser("],")])
    tree_dict = json.loads(currentTree)
    prompt += "{"
    for key in tree_dict:
        prompt += f'"{key}": [\n'
        for task in tree_dict[key]:
            task_description = task["task_description"]
            status = task["status"]
            prompt += '{"task_description":' + f'"{task_description}", "status": "{status}"' +'},\n'
        while True:
            print(prompt)
            llm = llamaindex_llamacpp_lm_format_enforcer(llm, task_parser)
            output = llm.complete(prompt).text
            if "inprogress" in output:
                task_parser =  JsonSchemaParser(BaseTaskOnlyTODO.schema())
            prompt += output
            print(output)
            llm = llamaindex_llamacpp_lm_format_enforcer(llm, list_next_parser)
            output = llm.complete(prompt).text
            prompt += output +"\n"
            if output == "],":
                break
    prompt += "}"
    return prompt[original_prompt_len:]
def add_whitespace(prompt, llm, sampler, max_spaces=4):
    if max_spaces == 0:
        return ""
    whitespace_regex = r"[ \t\r\n]+"
    generator = outlines.generate.regex(
        llm,
        whitespace_regex,
        sampler=sampler
    )
    whitespace = generator(prompt, max_tokens=max_spaces)
    return whitespace
def add_new_items_outlines(llm: LlamaCPP, prompt: str, currentTree: str, sampler, inprogress_set: bool = False, max_spaces=0, force_add_task: dict[str, bool] = {}):
    # below is incomplete as the json schema input is inconsistent with the output schema
    # after update completion status
    """
    schema format:
    {"recon": [
        {"task_description": "Perform a full port scan", "status": "done"},
        {"task_description": "Determine the purpose of each open port", "status": "todo"}
    ], 
    "initial_access": [], 
    "execution": [], 
    "post_exploitation": []}
    """
    original_prompt_len = len(prompt)
    choice_sampler =  multinomial(top_k=50, top_p=1, temperature=1.0)


    if inprogress_set:
        task_schema = str(BaseTaskOnlyTODO.schema()).replace("'", '"')
    else:
        task_schema = str(BaseTaskOnlyNew.schema()).replace("'", '"')


    continue_choices = [",",  "],"]
    tree_dict = json.loads(currentTree)
    prompt += add_whitespace(prompt, llm, sampler, max_spaces)
    prompt += "{"
    prompt += add_whitespace(prompt, llm, sampler, max_spaces)
    for i, key in enumerate(tree_dict):
        prompt += f'"{key}": ['
        prompt += add_whitespace(prompt, llm, sampler, max_spaces)
        if len(tree_dict[key]) > 0:
            for task in tree_dict[key]:
                task_description = task["task_description"]
                status = task["status"]
                prompt += '{"task_description":' + f'"{task_description}", "status": "{status}"' +'},'
            prompt = prompt[:-1]
            # The model always chooses to end the todo list
            if force_add_task.get(key, False):
                prompt += ","
                prompt += add_whitespace(prompt, llm, sampler, max_spaces)
            else:
                generator =outlines.generate.choice(llm, continue_choices, sampler=choice_sampler)
                output = generator(prompt)
                prompt+=output
                prompt += add_whitespace(prompt, llm, sampler, max_spaces)


                print(f"{key} adding tasks:", "]," not in output)
                if "]," in output:
                    continue
        else:
            if force_add_task.get(key, False):
                prompt += add_whitespace(prompt, llm, sampler, max_spaces)
            else:
                generator =outlines.generate.choice(llm, ['{"',  "],"], sampler=choice_sampler)
                if "]," in output:
                    prompt+=output
                    continue
                    
        while True:
            print(task_schema)
            generator = outlines.generate.json(llm, task_schema, sampler=sampler, whitespace_pattern=" \t\n\r")
            output = generator(prompt)
            if "inprogress" in output:
                task_schema =  str(BaseTaskOnlyTODO.schema()).replace("'", '"')
            prompt += str(output)
            generator =outlines.generate.choice(llm, continue_choices, sampler=sampler)
            output = generator(prompt)
            prompt += output
            if output == "],":
                if len(tree_dict) -1 != i:
                    prompt += add_whitespace(prompt, llm, sampler, max_spaces)
                break
            prompt += add_whitespace(prompt, llm, sampler, max_spaces)

    prompt = prompt[:-1] +"}"

    return prompt[original_prompt_len:]

def llamaindex_llamacpp_lm_format_enforcer(llm: LlamaCPP, character_level_parser: Optional[CharacterLevelParser], analyze: bool = False) -> LlamaCPP:
    logits_processors: Optional[LogitsProcessorList] = None
    if character_level_parser:
        logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm._model, character_level_parser, analyze=analyze)])
    # If changing the character level parser each call, inject it before calling complete. If its the same format
    # each time, you can set it once after creating the LlamaCPP model
    llm.generate_kwargs['logits_processor'] = logits_processors
    return llm