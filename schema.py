from pydantic import BaseModel, constr
from enum import Enum

from typing import Optional
from llama_cpp import LogitsProcessorList
from lmformatenforcer import CharacterLevelParser, JsonSchemaParser, RegexParser, UnionParser, StringParser
from lmformatenforcer.integrations.llamacpp import build_llamacpp_logits_processor
from llama_index.llms.llama_cpp import LlamaCPP
import copy
import json
class CompletionStatus(str, Enum):
    todo = 'todo'
    done = 'done'
    inprogress = "inprogress"
class OnlyTODO(str, Enum):
    todo = 'todo'

class BaseTask(BaseModel):
    status: CompletionStatus
    task_description: constr(max_length=50)
class BaseTaskOnlyTODO(BaseModel):
    status: OnlyTODO
    task_description: constr(max_length=50)

class PTT(BaseModel):
    recon: list[BaseTask]
    initial_access: list[BaseTask]
    execution: list[BaseTask]
    post_exploitation: list[BaseTask]



def update_completion_status(llm: LlamaCPP, prompt: str, current_schema: str):
    # below is incomplete as we need the current schema's str for this to work
    original_prompt_len = len(prompt)
    choices_parser: list[CharacterLevelParser] = []
    for status in CompletionStatus:
        choices_parser += [StringParser(status.value)]
    llm =llamaindex_llamacpp_lm_format_enforcer(llm, UnionParser(choices_parser))
    current_schema_list = current_schema.split("todo")
    output = []
    for elem in current_schema_list:
        if "inprogress" not in elem:
            output.append(elem)
        else:
            for elem_part in elem.split("inprogress"):
                output.append(elem_part)
    current_schema_list = output
    inprogress_set = False
    for elem in current_schema_list:
        prompt+=elem
        if inprogress_set:
            output = "todo"
        else:
            output = llm.complete(prompt)
        if "inprogress" in output:
            inprogress_set = True
        prompt+=output
    return prompt[original_prompt_len:], inprogress_set

def add_new_items(llm: LlamaCPP, prompt: str, currentTree: str, inprogress_set: bool = False):
    # below is incomplete as the json schema input is inconsistent with the output schema
    # after update completion status
    """
    schema format:
    {"recon": [
        {"task_description": "Perform a full port scan", "status": "done"},
        {"task_description": "Determine the purpose of each open port", "status": "todo"}
    ], 
    "initial_access": [], 
    "execution": [], 
    "post_exploitation": []}
    """
    original_prompt_len = len(prompt)

    if inprogress_set:
        llm =llamaindex_llamacpp_lm_format_enforcer(llm, UnionParser([JsonSchemaParser(BaseTaskOnlyTODO.schema()), StringParser("]")]))
    else:
        llm =llamaindex_llamacpp_lm_format_enforcer(llm, UnionParser([JsonSchemaParser(BaseTask.schema()), StringParser("]")]))
    tree_dict = json.load(currentTree)
    prompt += "{"
    for key in tree_dict:
        prompt += f'"{key}": ['
        for task in tree_dict[key]:
            task_description = task["task_description"]
            status = task["status"]
            prompt += f'"task_description": {task_description}, "status": {status}),'
        while True:
            output = llm.complete(prompt)
            prompt += output
            if output == "]":
                break
    prompt += "}"
    return prompt[original_prompt_len:]

def llamaindex_llamacpp_lm_format_enforcer(llm: LlamaCPP, character_level_parser: Optional[CharacterLevelParser], analyze: bool = False) -> LlamaCPP:
    logits_processors: Optional[LogitsProcessorList] = None
    if character_level_parser:
        logits_processors = LogitsProcessorList([build_llamacpp_logits_processor(llm._model, character_level_parser, analyze=analyze)])
    # If changing the character level parser each call, inject it before calling complete. If its the same format
    # each time, you can set it once after creating the LlamaCPP model
    llm.generate_kwargs['logits_processor'] = logits_processors
    return llm